[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "Data science involves applying a set of strategies to transform a recorded set of values into something from which we can glean knowledge and insight. This course will introduce you to concepts and methods from the field of data science, along with how to apply them in R. You will learn how to acquire, clean, wrangle, and visualize data. You will also learn best practices in data science workflows, such as code documentation and version control. Issues in data ethics will be addressed throughout the course.\nClasses will be held on Mondays from 1:40 PM to 2:55 PM and on Wednesday and Thursdays from 1:20 PM to 2:35 PM.\nSDS 100: Reproducible Scientific Computing with Data is a co-requisite for this course and designed to help support you in coding in R. Please note that I walk into this course with the assumption that most students have never coded before. Coding for the first time can be intimidating, but I intend do everything in my power to support you through the learning curve and to make things both fun and relevant in the process. I personally picked up most of my data science skills through a lot of trial-and-error, practice, and curiosity. My hope is that, in this course, you will learn through experimentation, along with independent and collaborative problem-solving. Honing these competencies will serve you as you move on to other courses in the SDS program and/or at Smith.\n\n\n\n\n\n\n\n\nLindsay Poirier, she/her/hers.\n\n\n\n\n\nI am a cultural anthropologist that studies how civic data gets produced, how communities think about and interface with data, and how data infrastructure can be designed more equitably. My Ph.D. is in an interdisciplinary discipline called Science and Technology Studies - a field that studies the intricate ways science, technology, culture, and politics all co-constitute each other. I work on a number of collaborative research projects that leverage public data to deepen understanding of social and environmental inequities in the US, while also qualitatively studying the politics behind data gaps and inconsistencies. As an instructor, I prioritize active learning and often structure courses as flipped classrooms. You can expect in-class time to involve lectures, activities, and labs.\n\n\n\nSlackMeeting with Me\n\n\nI can best support students in this course when I can readily keep tabs on our course-related communication. Because of this, I ask that you please don’t email me regarding course-related questions or issues. The best way to get in touch with me is via our course Slack. If you have course-related questions, I encourage you to ask them in the #sds-192-questions channel. When discretion is needed, feel free to DM. Please reserve more formal concerns like grades or accommodation requests for an in-person (or in-person virtual) conversation.\nDuring the week, I will try my best to answer all Slack messages within 24 hours of receiving them. Please note that to maintain my own work-life balance, I don’t answer Slack messages late in the evenings or on the weekends. It’s important that you plan when you start your assignments accordingly.\n\n\nMeeting with me outside of class is a great opportunity for us to chat about what you’re learning in the course, clarify expectations on assignments, and review work in progress. I also love when students drop in to office hours to request book recommendations, discuss career or research paths, or just to say hi!\nThere are two ways to meet with me. If you would like to have a one-on-one private conversation, I ask that you schedule an appointment with me via the booking form on Moodle. For support on class topics, you may drop-in during my regularly scheduled office hours.\n\nWednesday, 2:45 PM - 3:45 PM, McConnell 214\nFriday, 11:00 AM - 12:00 PM, McConnell 214\n\n\n\n\n\n\n\n\nI will make all course readings available on Perusall, which can be accessed through our course Moodle page.\n\n\n\n\nCourse Syllabus Quiz: 3%\nCourse Infrastructure Set-up: 2%\nReading Quizzes: 5%\nLabs (10): 30%\nProjects (3): 30%\nMid-term Exam: 15%\nFinal Exam: 15%\n\nAlso note that your grade may be impacted if you have more than 3 unexcused absences. See the course Attendance Policy below.\n\n\n\n\n\n\nSpinelli Center\n\n\n\nSmith’s Spinelli Center offers a number of resources to support SDS students. Spinelli Center Data Assistants will visit our classroom regularly to support you through lab work. The Center also offers drop-in tutoring hours Sunday through Thursday 7-9 PM. Finally, you can drop-in to Seelye 207D or schedule an appointment with the Data Research and Statistics Counselor (Kenneth Jeong). To schedule an appointment, email qlctutor@smith.edu.\n\n\n\n\n\n\nPreparationAttendanceExtensionsAcademic HonestyGenerative AI\n\n\nThis is a 4-credit course with 4.5 hours per week of in-classroom instructions. Smith expects students to devote 7.5 out-of-class hours per week to 4-credit classes. I have designed the course assignments and selected the course readings with this target in mind.\n\n\nAttending class is not only important for your learning but also an act of community. Attendance will be taken each class period. That said, we all have reasons we can’t be available from time-to-time. You may miss three classes with no penalty. You do not need to inform me that you will be absent in these cases. After the third unexcused absence, your grade may drop by a modifier for each class missed. I understand that you may need to be absent beyond these three sessions. Additional absences may be excused due to family/personal difficulties, sickness, or school or career-related activities; however, I will require some form of documentation for these absences. Please speak with your class dean or the Accessibility Resource Center so that we can get documentation of your need.\nI also ask that you make every effort to arrive to class on time. This is a large course, and when students arrive late, it can be distracting for me as the instructor, and it can be distracting to other students in the course. It also makes it difficult for me to plan group activities. Students arriving more than 10 minutes late for class without having informed me ahead of time will be marked as absent.\nIf you must miss a class entirely, you should contact a peer to discuss what was missed. Please note that the SDS Program has adopted a shared policy regarding in-person attendance:\n\nIn keeping with Smith’s core identity and mission as an in-person, residential college, SDS affirms College policy (as per the Provost and Dean of the College) that students will attend class in person. SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Accessibility Resource Center will be the only exceptions to this policy. As with any other kind of ADA accommodations, please notify your instructor during the first week of classes to discuss how we can meet your accommodations.\n\n\n\nThere is an automatic 24-hour grace period on all lab and project assignments. There will be no penalties for submitting the project within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any project or lab assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted without an accommodation from a class dean or from the ARC.\nNote that this policy does not apply to reading assignments/Perusall annotations. Reading assignments/Perusall annotations need to be completed by the due date for credit.\n\n\n\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. Students and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information, and honest and respectful use of college resources. Any cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\n\n\nSubmitting work completed by another student as your own.\nCopying and pasting words from sources without quoting and citing the author.\nParaphrasing material from another source without citing the author.\nFailing to cite your sources correctly.\nFalsifying or misrepresenting information in submitted work.\nPaying another student or service to complete assignments for you.\nSubmitting work generated by artificially intelligent tools such as chatGPT\n\n\n\nIn this course, you are learning, not only how to produce code, but also how to think like a data scientist - i.e. how to develop logical solutions to problems, how to discern a good plot from a bad one, and how to spot errors in reasoning that can lead to misleading results. If you are using generative AI to write your code, then you are not developing these foundational skills. Further, it’s important to keep in mind that, as generative AI is increasingly replacing entry level data science labor, some of the most important jobs in data science will involve being able to explain how an AI came to its results and to audit AI for errors and bias. If you don’t develop the skills to understand how the underlying code is composed/works, then you will not be prepared for this kind of work.\nWith this in mind, any use of generative AI to complete assignments or produce content for this course is prohibited. Prohibited forms of usage include but are not limited to: summarizing course readings, drafting responses to written prompts, drafting comments for Slack, composing code, formatting code, answering lab, quiz, or exam questions, or proofreading text. Any use of generative artificial intelligence in this course will be considered a case of academic dishonesty/plagiarism and will be reported to the Academic Honor Board.\n\n\n\n\n\n\n\nCode of ConductPrinciples of CommunityPronouns\n\n\nAs the instructor for this course, I am committed to making participation in this course a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned to this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0, available here.\n\n\nI hope that we can foster a collaborative and caring environment in this classroom: one that celebrates successes, respects individual strengths and weaknesses, demonstrates compassion for each other’s struggles, and affirms diverse identities. Here are some ideas that I have for creating this environment in our course:\n\nCheck-in with colleagues before starting collaborative work. “What three words describe how you’re feeling?” “Name one challenge and one success from this week.” “What are you doing for self-care right now?” Thank each other for sharing where they’re at.\nConsider when to step up and when to step back in class discussions, creating space for others to contribute. Listening is just as important to community-building as speaking.\nAcknowledge that there is much we don’t know about how our colleagues experience the world. …but don’t ask colleagues to speak on behalf of a social group you perceive them to be a part of.\nCheer on colleagues as they give presentations or try something out for the first time.\nAsk questions often in our #sds-192-questions channel. Help each other out by answering questions when you can.\nMistakes happen. I will certainly make mistakes in class. Admit mistakes, and then move on.\n\n\n\nUsing the proper pronouns for our students is foundational to a safe, respectful classroom environment that creates a culture of trust. For information on pronouns and usage, please see the Office of Equity and Inclusion link here: Pronouns\n\n\n\n\n\n\n\nAccommodationsStudent Well-beingTrigger Warnings\n\n\nIt is my goal for everyone to succeed in this course. If you have personal circumstances that may impact your experience of our classroom, I encourage you to contact Accessibility Resource Center in College Hall 104 or at arc@smith.edu. The Center will generate a letter that indicates to me what kind of support you need and how I can make your classroom experience more accommodating. Once you have this letter, you are welcome to visit my office hours or email me to discuss ideas about how we can tailor the course accordingly. While you can request accommodations at any time, the sooner we start this conversation, the better. If you have concerns about the course that are not addressed through ARC, please contact me. At no point will I ask you to divulge details about your personal circumstances to me.\n\n\nCollege life is stressful, and life outside of college can be overwhelming. It is my position that attending to your physical and mental health and well-being should be a top priority. I will remind you of this often throughout the semester. I encourage you to schedule a time to talk with me if you are struggling with this course. If you, or anyone you know, is experiencing distress, there are numerous campus resources that can provide support via the Schacht Center. I can point you to these resources at any time throughout the semester.\n\n\nA trigger is a topic or image that can precipitate an intense emotional response. When common triggering topics are to be covered in this course, I will do my best to provide a trigger warning in advance of the discussion. However, I can’t always anticipate triggers. With this in mind I’ve set up an anonymous form, available on Moodle, where you can indicate topics for which you would like me to provide a warning.\n\n\n\n\n\n\n\nMoodlePerusallGitHubSlack\n\n\nGrades, forms, and handouts will be available on the course Moodle.\n\n\nAll course readings and recorded lectures will be available on Perusall. You can access Perusall via our course Moodle page.\n\n\nI will be using GitHub Classroom to distribute several course assignments, including labs and projects. You will submit assignments by pushing changes to template documents to a private GitHub repository. I will provide guidance on how to do this early in the semester.\n\n\nThis class will use the R statistical software package. If you haven’t already, you will install and configure R and RStudio in SDS 100. You should let me know in the first week of the course if you are using a Chromebook or tablet.\n\n\n\nOutside of class almost all of our communication will happen via Slack. You can use the following channels\n\n#general: Course announcements (only I can post)\n#sds-192-discussions: Share news articles and relevant opportunities\n#sds-192-questions: Ask and answer questions about our course\nYou can also create private Slack channels with your project group members."
  },
  {
    "objectID": "learning_dimensions.html",
    "href": "learning_dimensions.html",
    "title": "Learning Dimensions",
    "section": "",
    "text": "This course is designed to assess your work along five different learning dimensions. Class activities and assignments will reference the learning dimensions engaged, so that you have a clear understanding of what you should take-away from them. Additionally, you may be asked to reference aspects of these learning dimensions in written assignments. The five learning dimensions are as follows:\n\nKnowledge and Understanding\nThis dimension refers to the development of your ability to recall, define, explain, apply, and synthesize course concepts and ideas. Primary course concepts include:\nEPISTEMOLOGY DISCOURSE BINARY OPPOSITION INFRASTRUCTURES INVISIBLE LABOR BOUNDARY WORK RITUALS INCENTIVES MOBILIZATION MEANING-MAKING IGNORANCE\n\n\nData Ethnography Methods\nThis dimension refers to the development of your ability to identify, implement, and critique the research methods taught in this course. Principal methods taught in this course include:\nDISCOURSE ANALYSIS INTERVIEWING CULTURAL ANALYSIS OF INFRASTRUCTURE PARTICIPANT OBSERVATION\n\n\nSkills and Competencies\nThis dimension refers to the development of your ability to execute and evaluate further opportunities for growth in the skills and competencies engaged in this course. Skills practiced in this course include:\nTIME MANAGEMENT PLANNING FIELDWORK THICK DESCRIPTION DATA DOCUMENTATION REVISION WRITING IN GENRE\n\n\nCritical Thinking\nThis dimension refers to the development of your ability to perceive the world in new ways, interpret cultural meaning and import, situate your own perspectives, communicate complexity, and grapple with issues that don’t have easy solutions. Critical modes of thinking engaged in this course include:\nBECOMING OBSERVANT INTERPRETING CULTURAL MEANING ANALYZING SOCIAL FORCES AND SYSTEMS SITUATING KNOWLEDGE COMMUNICATING (IN) CONTEXT EVALUATING ETHICAL DILEMMAS\n\n\nCollective Thinking and Collaborative Engagement\nThis dimension refers to the development of your ability to recognize a diversity of perspectives, learn from others, and contribute to the functioning of a reflective, collaborative learning environment. Opportunities for cooperative engagement include:\n\nConsistently contributing to Slack discussions\nSubmitting thoughtful, critical reflections in Perusall\nFostering discussion in the classroom\nEnsuring equitable divisions of labor and support among final project group members"
  },
  {
    "objectID": "slides/Day19-Mobilization.html",
    "href": "slides/Day19-Mobilization.html",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "",
    "text": "On April 10, 2023 at 9AM Northampton’s reported AQI was 54."
  },
  {
    "objectID": "slides/Day22-Credibility.html",
    "href": "slides/Day22-Credibility.html",
    "title": "Day Twenty-Two: Credibility",
    "section": "",
    "text": "class: center, middle # Turn to a neighbor and discuss: What makes science separate from other features on the map? Can you think of stakeholders who might disagree with your map of science? Why?\n\n\nBoundary Work\n\nThomas Gieryn, Cultural Cartographies of Science\nAcknowledges the lack of stable criteria for demarcating science from non-science.\nDifferent people justify what makes something scientific vs unscientific differently.\nAs credibility contests emerge, people make claims based on their own maps of science.\nCulturally chart the boundaries around what we consider scientific.\n\n\nclass: center, middle # Science is a symptom of the legitimate power to decide reality - its edges and contents disputed, moved all over the place, settled here and there as decisions about truthful and reliable claims are acted upon … (Gieryn, Cultural Boundaries of Science)\n\nclass: center, middle # How do we draw the boundaries of credibility in data work? Who gets seen as a legitimate claims-maker in data science work, and whose voices are often denied a seat at the table?\n\n\n\nExample: Lois Gibbs and Love Canal\n\n\n\n\n\nDeficit Model\n\nBelief that the reason people distrust science is because they lack knowledge\nOften casts scientific illiteracy as both a technical problem and a moral problem\nPresumed solution to increasing public trust in science is to teach more science\n\n\nSTS rejects the deficit model…\n\n\nIgnores public’s situated knowledge\nSuggests knowledge is context-free\nDoesn’t get at the root of distrust\n\n\nclass: center, middle # How do we decide who is considered to have expertise?\n\n\n\nEnactments of Expertise\n.pull-left[\n\nExpertise as a signal of authority\nEnacted through credentialing, use of certain vocabularies, creation of exclusive social circles, clothing, norms of behavior\nEnactments also serve a gate-keeping role in determining who gets to contribute to science/data science ]\n\n.pull-right[\nIn her history of the medical activism by the Black Panthers, Alondra Nelson discusses how one way the Black Panthers enacted their expertise was in their choice to wear white lab coats.\n > Sickle cell anemia testing. Oakland, Calif. 1972. credit NY Times\n]\n\n\n\nCitizen Science\n\nExclusionary science as a democratic problem\nExamples of citizen engagement:\n\nCounting declining bird populations\nAir and water quality monitoring\nWriting health books for underrepresented populations\n\nMust learn how to enact expertise to earn a seat at the table\n\n\n\n\nSocial Movement Theory\n\nSociological research that studies how social movements form, operate, sustain, and fizzle out\nPolitical Process Theory: suggests that certain political formations enable social movements to form and operate\nResource Mobilization Theory: suggests that access to certain resources enable social movements to mobilize\nFraming: Suggests that collective ideologies and values bring social movements into fruition"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html",
    "href": "slides/Day4-BinaryOppositions.html",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "",
    "text": "Please merge your branches into main!\nBe sure not to close the Feedback pull request.\nBe sure to fill out CATME survey for next week."
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html",
    "href": "slides/Day9-DataDocumentation.html",
    "title": "Day Nine: Data Documentation",
    "section": "",
    "text": "class: center, middle # How do we define metadata?\n\n\n5 W’s of Metadata\n\n\nclass: center, middle # Why is metadata important?\n\n\n\nExample: Library Catalog\n.pull-left[ ]\n.pull-right[ ]\n\n\n\nMetadata Schemas\n\nA standardized labeling system for cataloging or describing data\nEnables search engines to index data by certain criteria\nExamples:\n\nSort by “date created”\nRetrieve all results from a specific “author/creator”\nFilter results to a specific “subject”\nExclude results from a specific “publisher”\n\n\n\n\n\nExample: Citation Manager\n.pull-left[ ]\n.pull-right[ ]\n\n\n\nExample: Citation Manager\n\n\nclass: center, middle # What’s the difference between administrative and descriptive metadata?\n\n\n\n\n\nData Dictionaries\n\nDocuments for holding descriptive metadata\nDefine the variables in a dataset and the values that may fill in those variables\nAre not always as descriptive as we’d like them to be\n\n\nclass: center, middle # Example: NYC Metadata for All\n\n\n\n\nFor Monday\n\nQuestions to consider:\n\nWhat does Biruk mean when they refer to “translation” in this chapter?\nWhere do we see looping effects in this chapter? What gets “lost in translation”?"
  },
  {
    "objectID": "slides/Day23-Ignorance.html",
    "href": "slides/Day23-Ignorance.html",
    "title": "Day Twenty-Three: Ignorance",
    "section": "",
    "text": "PollEv.com/lindsaysmith\n\n\n\n\n\nSelect at least eight words from the cloud to write on sticky notes.\nOrganize these terms in a meaningful way on your orange paper. Feel free to draw lines between interconnected terms.\nIdentify three categories of ignorance represented on your map. Be sure to write them somewhere on the map.\nTake a photo the finished map, and post it in the #random channel on Slack."
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html",
    "href": "slides/Day8-ResearchEthics.html",
    "title": "Day Eight: Research Ethics",
    "section": "",
    "text": "class: center, middle # Quick note about reading for this course.\n\n\nI encourage you to come to office hours if…\n\nYou’d like to discuss strategies for managing the reading/workload.\nYou’d like clarifications on any of the assignment expectations/course infrastructure.\nYou’d like to review certain concepts discussed in lecture or the readings.\nYou’d like to chat about opportunities for exploring more about data ethnography.\nYou’re stressed and need some positive affirmation.\n\n\nclass: center, middle # What were some takeaways from Monday’s class?\n\n\n\nCommon Rule\n\nIn the US, research organizations receiving federal funding are subject to the Common Rule\n\nLaws and regulations regarding how Institutional Review Boards are to operate\nIRBs are organizational bodies that review the ethics of human subjects research in order to protect human welfare, rights, and privacy before a study gets carried out\n\nUsually composed of representatives from an institution with a diverse background\n\n\n\n\nTuskegee Study\n\nConducted from 1932 to 1972 by US Public Health Services and Center for Disease Control, in collaboration with Tuskegee University in Alabama\nInvolved 400 African Americans with syphilis\nStudy of leaving the disease untreated even though it was treatable\n\nWere told they would receive free medical care\nNever informed of their diagnosis\nProvided with placebos and ineffective methods\n\n\n\n\n\nFacebook’s Emotional Contagion Study\n\nJanuary 2012, Facebook data scientists manipulated what 700,000 users saw on feeds to examine emotional contagion\n\nSome shown happy, positive content\nOthers shown sad, negative content\n\nLegal?\nEthical?\n\n\n\n\nEthics of ethnographic research\n\nEthnography different than many other forms of human subjects research in that it takes place in natural settings vs in clinical settings\nQuestion of how to balance benefits of the research with the potential harms posed to the participants\nPotential harms:\n\nReputation\nDisclosure of personal information\nDisruption of relationships\nLoss of claims\n\n\n\nclass: center, middle # What about informed consent in virtual spaces?\n\nclass: center, middle # What about informed consent in virtual spaces?\n\nclass: center, middle # Dataset Review in Project Groups\n\nclass: center, middle # What were three takeaways from today?\n\n\n\nFor Monday\n\nGet approval for dataset\nReadings:\n\nWhy does good data documentation matter?"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html",
    "href": "slides/Day6-Reflexivity.html",
    "title": "Day Six: Reflexivity",
    "section": "",
    "text": "Turn to your neighbor and discuss:\n\nWhy would it be important to consider these aspects of your cultural identity as you engaged in ethnographic fieldwork?\n\n\nclass: center, middle # What are Biruk’s research questions?\n\n\nHow do raw units of information - numbers written onto a questionnaire by data collectors - acquire value as statistics that inform national AIDS policy and interventions?\n\n\nHow do on-the-ground dynamics and practices of survey research cultures mediate the production of numbers?\n\n\nFinally, how are quantitative health data and their social worlds co-produced and with what consequences for local economies, formulations of expertise, and lived experience?\n\n\nclass: center, middle # What does Biruk make known about her assumptions/beliefs/values as she begins to interpret data?\n\n\n\nCritiques and Criticisms of Ethnographic Research\n\nEthnographic research (like many other forms of research) has been historically complicit in imperialism and colonialism\nMethods emerged as part of European colonial efforts to document folks “Native” to “other” lands - often to enable control and exploitation of those cultures\nMany binary oppositions indicate how power operated here:\n\nStudier/studied\nResearcher/researched\n\nConcerns over who gets to “write culture”\n\n\n\n\nReflexive Turn\n\nEmerged in the 1970s as a result of feminist and post-colonial critiques\n\nFeminist: Where do we stand? Can we ever observe from an objective or neutral place?\nPost-colonial: What do we consider “Other,” and how do we portray what is “Other”?\n\nCalled on ethnographers to engage in self-reflection (standpoints, assumptions, etc.)\nCan we write another culture “objectively” when our own biases and epistemologies, and social capital are inevitably involved in the research?\n\n\n\n\n…aka the Literary Turn\n\nEthnography was often understood to be about writing (documenting observations and interpreting them)\nBegan to pay attention to power in the language used to “write culture”\n\nBinary oppositions\nGenre and representations of “fact”\n\nCalled for making ethnographic writing more polyphonous (or represent a plurality of voices)\nSometimes invited “poetics” and “experimentalism”\n\n\nIn other words, there are political reasons as to why the fieldnotes that you read for Monday felt different than other scientific genres!\n\n\n\n\nReflexivity for Each of our Methods\n\nParticipant Observation\n\nObserving interaction and behaviors “in the field”\n\nInterviewing\n\nEngaging in semi-structured conversations with informants\n\nArchival Research\n\nCurating and interpreting historical documents and artifacts\n\nDiscourse analysis\n\nInterpreting the cultural meaning interwoven in texts and speech\n\n\n\nIn your groups, be sure to introduce yourselves to each other (pronounds, majors, etc.)\n\n\n\n\nInfrastructure Overview\n\nGroup project\nFirst fieldnote\nReview of labor log\n\n\nclass: center, middle # What were three takeaways from today?\n\n\n\nFor Monday\n\nComplete group contract\nQuestions to consider:\n\nWhat are looping effects, how do they emerge, and why should we pay attention to them?\nWhat are data assemblages, and why is “assemblage” a powerful concept for understanding data?"
  },
  {
    "objectID": "slides/Day7-Looping.html",
    "href": "slides/Day7-Looping.html",
    "title": "Day Seven: Data Looping Effects",
    "section": "",
    "text": "Office hours for help with labor log.\nGroup contract and first field note due next Tuesday."
  },
  {
    "objectID": "slides/Day20-EthnographicArguments.html",
    "href": "slides/Day20-EthnographicArguments.html",
    "title": "Day Twenty: Ethnographic Arguments",
    "section": "",
    "text": "Announcements\n\nStay tuned for updates about Monday’s class\n\n\n\n\nWhat is an ethnographic argument?\n\nCentral theme derived from an ethnographic study\nIdentifies a certain cultural pattern or phenomena\nExamples:\n\nHow data is talked about\nThe cultural assumptions underpinning a data infrastructure\nHow labor is recognized in a data practice\n\n\n\n\n\nWriting an Ethnographic Argument\n\nParagraph 1: Introduction and thesis\nParagraph 2-x: Sub-arguments supporting thesis\n\nPresents evidence from data collection to support sub-argument\n\nParagraph Final: Summarize thesis and wrap-up\n\n\n\n\nWhat counts as evidence/data in ethnographic arguments?\n\nDiscourse Analysis/Interview\n\nDirect quotes and their context\n\nParticipant Observation\n\nVignettes/thick description\n\nCultural Analysis of Infrastructure\n\nMaterial from secondary sources\nFacts about the infrastructure’s historical development\nDescriptions of the organization of the infrastructure\n\n\n\n\n\nRevision Expectations\n\nRevision = re - vision = seeing again\nInvolves more than spelling and grammar fixes and word changes; editing is only one component of revision\nThings to consider:\n\nRecognize and re-articulate the purpose\nRefine the focus (audience and presentation)\nClarify the argument\nStrengthen and hone evidence\n\n\n\nhttps://writingcenter.unc.edu/tips-and-tools/revising-drafts/"
  },
  {
    "objectID": "slides/Day1-Intro.html#what-is-ethnography",
    "href": "slides/Day1-Intro.html#what-is-ethnography",
    "title": "Day One: Introductions",
    "section": "What is ethnography?",
    "text": "What is ethnography?\n\nstudy of human culture and social relations\ninvolves interactions and observations, recording, and analysis\ndata collection methods are predominantly qualitative\nanalysis is predominantly inductive and interpretive"
  },
  {
    "objectID": "slides/Day1-Intro.html#what-fields-of-research-inform-data-ethnography",
    "href": "slides/Day1-Intro.html#what-fields-of-research-inform-data-ethnography",
    "title": "Day One: Introductions",
    "section": "What fields of research inform data ethnography?",
    "text": "What fields of research inform data ethnography?\n\nScience and Technology Studies (STS)\n\nan interdisciplinary field that examines how science, technology, politics, and culture all co-produce each other\nSTS disciplines include anthropology, sociology, literary studies, political science, economics, and more\nWhat might be some examples?\n\nCritical Data Studies\n\nan interdisciplinary field examining the epistemological, political, social, and ethical aspects of data artifacts, practices, and infrastructures\nWhat are some political dimensions of data? Examples?\nWhat are some ethical dimensions of data? Examples?\nIs there a difference?"
  },
  {
    "objectID": "slides/Day1-Intro.html#who-is-the-professor-why-is-an-anthropologist-teaching-data-science",
    "href": "slides/Day1-Intro.html#who-is-the-professor-why-is-an-anthropologist-teaching-data-science",
    "title": "Day One: Introductions",
    "section": "Who is the professor? Why is an anthropologist teaching data science?",
    "text": "Who is the professor? Why is an anthropologist teaching data science?\n\nPlease call me Lindsay (preferred), Professor Poirier, or Dr. Poirier\nPreviously Assistant Professor of Science and Technology Studies at UC Davis\nLab Manager at BetaNYC\nM.S./Ph.D. in Science and Technology Studies from Rensselaer Polytechnic Institute\nB.S. in Information Technology and Web Science from Rensselaer Polytechnic Institute\nDancing, crafting, cooking, re-watching the same TV series over and over again.\nI have a very spunky dog Madison.\nI am first generation."
  },
  {
    "objectID": "slides/Day1-Intro.html#exercise",
    "href": "slides/Day1-Intro.html#exercise",
    "title": "Day One: Introductions",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\n\nEthnographers often collect more data than they know what to do with\nWrite as much as possible about:\n\nwhat people do/why they do it\nbeliefs/values/expertise\nsocial structures\nquestions you are left with"
  },
  {
    "objectID": "slides/Day1-Intro.html#syllabus-review",
    "href": "slides/Day1-Intro.html#syllabus-review",
    "title": "Day One: Introductions",
    "section": "Syllabus Review",
    "text": "Syllabus Review\n\nPolicies\nGrading Contract\nCourse Website\nPerusall\nSlack"
  },
  {
    "objectID": "slides/Day1-Intro.html#reading-tuesday",
    "href": "slides/Day1-Intro.html#reading-tuesday",
    "title": "Day One: Introductions",
    "section": "Reading Tuesday",
    "text": "Reading Tuesday\n\nIn what social contexts and research cultures did the terms Big Data and AI emerge?\nWhat are the consequences of perceiving the work and technologies in these domains as “magic”?\nHow does the actual work of AI and Big Data differ from public hype?\nHow do Elish and boyd recommend engaging ethnography in these fields?\nWhat is methodological reflexivity, and how might it benefit research into Big Data and AI?"
  },
  {
    "objectID": "slides/Day10-Translation.html",
    "href": "slides/Day10-Translation.html",
    "title": "Day Ten: Translation",
    "section": "",
    "text": "class: center, middle # What were some takeaways from Thursday’s class?"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day2-Epstemologies.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhat does it mean that you know this to be true? What counts as knowing?\nHow/through what means do you know this to be true?\nWhat are the limits of your knowledge on this?"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#epistemology",
    "href": "slides/Day2-Epstemologies.html#epistemology",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "Epistemology",
    "text": "Epistemology\n\nGreek words\n\nEpisteme”: knowledge; understanding\n“Logos”: reason; argument\n\nPhilosophical study of the nature and limits of knowledge\n\nWhat conditions must be met for us to say that we “know” something to be true?\nHow do a group of people come to acquire knowledge?"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#what-counts-as-knowledge",
    "href": "slides/Day2-Epstemologies.html#what-counts-as-knowledge",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "What counts as knowledge?",
    "text": "What counts as knowledge?\n\nHistorically knowledge defined as “justified true belief”\nDoes knowledge exist independently of a knowing mind?\n\nPositivists claim yes, there is objective truth independent of a knower\nInterpretivists and constructivists claim no, truths are subjective or tied to a knowing mind"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#how-do-we-acquire-knowledge",
    "href": "slides/Day2-Epstemologies.html#how-do-we-acquire-knowledge",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "How do we acquire knowledge?",
    "text": "How do we acquire knowledge?\n\nEmpiricists claim that knowledge emerges from direct observation\nRationalists claim that knowledge emerges from logic and reason\n…and then there’s testimony, memory, intuition, feeling"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#reading-discussion",
    "href": "slides/Day2-Epstemologies.html#reading-discussion",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "Reading Discussion",
    "text": "Reading Discussion\n\nWhat are the consequences of perceiving the work and technologies in big data and AI as “magic”?\n\n\nAssign someone to take notes and someone else to facilitate.\nTo start, everyone will take a turn to offer their brief initial perspective.\nOpen for broader discussion. The facilitator should keep time and ensure the conversation stays on topic.\nPrepare two bullet points that summarize the key takeaways from your discussion to report out."
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#feminist-epistemologies",
    "href": "slides/Day2-Epstemologies.html#feminist-epistemologies",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "Feminist Epistemologies",
    "text": "Feminist Epistemologies\n\nAll knowledge is embodied\n\nContrast with disembodied knowledge - i.e. not tied to a specific body\n\nBodies are situated in certain social positions and have a finite point of view (Haraway 1991)\n\nCritique of the “unmarked body,” the “God trick,” or the “view from nowhere”\n\nKnowledge is tied to particular standpoints\n\nOur experiences, what we’ve read, our education, our social positions, and what our bodies enable us to do, see, hear, taste, touch, and smell\nFactors are innumerable and unique to every person"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html",
    "href": "slides/Day3-BigDataDiscourse.html",
    "title": "Day Three: Big Data Discourse",
    "section": "",
    "text": "What assumptions are built into this metaphor?\nWhat are the some social consequences of associating data with this idea?"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html",
    "href": "slides/Day5-ThickDescription.html",
    "title": "Day Five: Thick Data for Big Data",
    "section": "",
    "text": "Discliplinary Conventions\n\nWhat is considered a relevant research question?\nWhat kind of arguments are made?\nWhat kind of data is collected/produced/presented?\nHow is collaboration valued?\nWhen/how is data shared?\n\n\n\n\nDisciplinary Genres\n\nIs text written in active or passive voice? Personal pronouns?\nAre subheadings chosen based on academic conventions?\nWhat kind of evidence is presented to persuade the reader? Statistics? Anecdotes?\nIn what ways are research methods explicated?\nHow and with what sort of detail are objects of study described?\nWhat role do poetics and metaphor play in the text? Can you identify emotion?\nWhat reflexive moves (self-reflection) does the author make? How do they elaborate their standpoint in/through the text?\n\n\n\n\nWhat is ethnography?\n\nStudy of human cultures\n\nCulture often defined as “semiotic”\nEthnographers aim to interpret the underlying meanings of surface actions and expressions\nMake the “familiar strange”\n\nOften characterized as interpretivist and qualitative\nTypically involves extended, immersive fieldwork\nHas historically been a solo discipline\nQualitative data rarely shared\n\n\n\n\nMethods of Ethnography\n\nParticipant Observation\n\nObserving interaction and behaviors “in the field”\n\nInterviewing\n\nEngaging in semi-structured conversations with informants\n\nArchival Research\n\nCurating and interpreting historical documents and artifacts\n\nDiscourse analysis\n\nInterpreting the cultural meaning interwoven in texts and speech\n\n\n\n\n\nTools of Ethnography\n\nField Notebooks\n\nDocument thick descriptions of observations\n\nRecording Devices\n\nAudio and video documentation of interviews\n\nCoding Software\n\nDraw out consistent themes of notes and interviews\n\n\n\n\n\nWhat is thick description?\n.pull-left[ \n]\n.pull-right[\n\nDescribe in the “thinnest” terms possible what is happening in this image.\nThick description moves towards interpreting its cultural meaning.\nExample drawn from a famous 1973 chapter by Clifford Geertz: “Thick Description: Towards an Interpretive Theory of Culture.”\n\n]\n\nclass: center, middle # How might we interpret the role of hex stickers in the R community?  Image source: Beiers, Sophie. 2020. “Tips & Tricks from the Newbies of Rstudio::Conf2020.” ACLU Tech & Analytics (blog). March 13, 2020. https://medium.com/aclu-tech-analytics/tips-tricks-from-the-newbies-of-rstudio-conf2020-5ccc780ba0e7.\n\nclass: center, middle # What were three takeaways from today?\n\n\n\nFor Wednesday\n\nLet me know if you’d like to lead a class discussion\nBe on the lookout for project group posting\nReading for Wednesday\n\nWhat are Biruk’s research questions?\nWhat does Biruk make known about her assumptions/beliefs/values as she begins to interpret data?"
  },
  {
    "objectID": "slides/Day13-Interviewing.html",
    "href": "slides/Day13-Interviewing.html",
    "title": "Day Thirteen: Labor and Interviewing",
    "section": "",
    "text": "How would the response to this question deepen understanding of the cultural underpinnings of a data infrastructure?"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html",
    "href": "slides/Day11-Infrastructure.html",
    "title": "Day Eleven: Ethnographies of Infrastructure",
    "section": "",
    "text": "Pick one supporting infrastructure:\n\nHow often do you think about this infrastructure? Did you notice it pre-breakdown?\nWho has a say in the design of this infrastructure, and who is excluded?"
  },
  {
    "objectID": "grading_contract.html",
    "href": "grading_contract.html",
    "title": "Grading Contract",
    "section": "",
    "text": "Note\n\n\n\nGrading contracts have been theorized and implemented in the research of Dr. Peter Elbow and Dr. Asao Inoue. This grading contract is adapted from their work, along with the contracts of Dr. Kate Navickas and Dr. Kati Ahern.\nThis course will be using a grading contract. This means that the grade you ultimately receive for this course is primarily based on the labor that you perform rather than a subjective evaluation of the quality of your work and writing in relation to your colleagues. Your grade will be determined by the extent of your engagement in class, your timely completion of assignments, and how you support the course community. You will still attend to and work to improve the quality of your writing and thinking in this course. You will receive extensive feedback on all of your submissions (from me and from your colleagues), and you will have opportunities to revise. However, you will not receive points or an A, B, C, D, etc. on assignments.\nI’ve experimented with a few alternative grading systems, and I’m particularly excited to continue engaging this system this semester as I believe it aligns with my overarching goals for the course. In this course I am less concerned with your ability to come to a specific answer, to master certain procedures, or to produce standardized deliverables. I’m more concerned with the extent to which this course enables you become more perceptive to data environments, to deepen your critical thinking about data’s cultural meanings, to experiment with your thinking and writing, and to grapple with ethical dilemmas. There’s always room for strengthening these thinking modes further; in fact, every time I teach this course, I find that my own ability to think critically about data deepens through classroom discussions and my reading of student work.\nThere are many benefits to contract grading. Here are there reasons why I’ve opted for a grading contract in this course:"
  },
  {
    "objectID": "grading_contract.html#grade-breakdown",
    "href": "grading_contract.html#grade-breakdown",
    "title": "Grading Contract",
    "section": "Grade Breakdown",
    "text": "Grade Breakdown\nEach row indicates what labor you need to complete in the course to earn the grade indicated in the first column of that row. Note that to earn a particular grade all minimum labor criteria in the corresponding row must be met.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nJournal Entries (out of 5)\nMini-projects\nFinal Project Checkpoints\nReading Annotations\nCommunity Labor Points\nCourse Absences/Late Arrivals\nAdvanced Assignments\n\n\n\n\nA\n4 or more\n2 + 1 substantive revision\n1 group contract + 1 draft + 1 final project\n13 or more\n8 or more\n&lt;=3\n3\n\n\nB\n4 or more\n2 + 1 substantive revision\n1 group contract + 1 draft + 1 final project\n13 or more\n8 or more\n&lt;=6\n0\n\n\nC\n3 or more\n1 + 1 substantive revision\n1 group contract + 1 draft + 1 final project\n10 or more\n6 or more\n&lt;=9\n0\n\n\nD\n2 or more\n1, no revision\n1 group contract + 1 draft + 1 final project\n7 or more\n4 or more\n&lt;=12\n0\n\n\nE\n1\n0\n0\n7 or fewer\n3 or fewer\n&lt;=15\n0\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAt the start of the course, I will assign a syllabus quiz. If you receive an 80% or better on that quiz, it can replace one of your reading annotation assignments. If you receive a 100% on that quiz, it can replace two of your reading annotation assignments.\n\n\n\nDeadlines\nAssignments are due before class (2:45PM EST) on the designated due date. If you take the 24-hour grace period for written assignments (note that this is not an option for reading assignments), the assignment is due before (2:45PM EST) on the day following the designated due date. Any granted extensions are similarly due before 2:45PM EST on the date the deadline is extended to.\n\n\nEarning an A\nYou’ll notice that, in most categories, there’s no difference in the quantity of labor you need to complete to earn an A vs. a B. Earning a B in this course demonstrates that you have completed enough work to meet the course’s learning goals. An A grade in this course indicates that you have exceeded expectations of the course’s learning goals. Please note that this does not mean that you need to do more labor (i.e. more assignments) to earn A. Instead, you need to consistently demonstrate levels of critical thinking on existing course assignments that surpass the minimum requirements.\nTo earn an A in this course, you will need to complete 3 course assignments at an advanced level. You will select the assignments that you want to complete at this level from the labor log, giving you an opportunity to choose where you want to engage deeper. The following chart helps breakdown some of the differences between meeting expectations and exceeding expectations:\n\n\n\n\n\n\n\nMeeting Expectations\nExceeding Expectations\n\n\n\n\nDefining and applying course concepts\nDrawing connections between course concepts, critiquing them, or extending them (i.e. offering new ways to think about them)\n\n\nCiting arguments from a course reading to support interpretation and ethnographic arguments\nEngaging more deeply with a course reading - unpacking how its arguments are supported and evaluating its relevance when applied to other ethnographic data\n\n\nImplementing an ethnographic research method\nDefending the selection of a research method and evaluating its strengths/limitations\n\n\nWriting up research findings to convey an argument\nExperimenting with the prose and form of argumentative writing with a clear analytic purpose\n\n\nReporting ethnographic findings in writing\nCritically situating the standpoints of your research findings throughout the writing\n\n\nAnalyzing how a social force shapes the constitution of data\nAnalyzing how multiple, at times competing, social forces shape the constitution of data\n\n\nClearly communicating the contexts that produced data in a format appropriate for a given audience\nCreatively playing with the style and medium of communication in order to better reach and engage an audience\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you may complete a reading annotation assignment at an advanced level by signing up to lead a 15-minute classroom discussion of that reading. I will remind you of this option throughout the first few weeks of the course. This is the only option for completing reading annotation assignments at an advanced level.\n\n\nThe labor log designates which assignments you may attempt to complete at an advanced level. You will be asked to justify why you consider your work on the assignment as advanced in the labor log. Please note that, when justifying why I should consider the work advanced in the labor log, you should clearly explain how the cognitive effort on the assignment surpasses minimum expectations. While you may end up writing more or spending more time on these assignments, exceeding the minimum word count or spending x hours on the assignment would not be considered adequate justification for advanced level work. Being able to justify why your work is advanced demonstrates to me that you have gained a deep understanding of the course’s learning objectives and that you have the ability to communicate what you’ve learned. While it is ultimately up to you to determine what constitutes advanced level work, I am happy to brainstorm ideas with you in office hours.\nPlease note that I reserve the right to deny advanced level designations after reviewing your work, and I also reserve the right to deem an assignment as having been completed at an advanced level, even when you haven’t designated it as such. If ultimately I deny a designation, you may attempt to earn the advanced level designation again. Note that this is a good reason not to wait until the end of the semester to attempt advanced level work.\nAttendance/late arrivals also separate A grades from B grades. Keep in mind that the final grade will be reduced by one grade modifier for each unexcused absence/late arrival beyond the number allowed for your contracted grade. This means that if you completed the work for an A, and you have 4 absences, the final grade will be an A-, and with 5 absences, it would be a B+. I chose to use grade modifiers here so that if you miss just one class beyond the first 3 allotted (i.e. 4 absences), it doesn’t drop your grade so drastically from an A to a B."
  },
  {
    "objectID": "grading_contract.html#faq",
    "href": "grading_contract.html#faq",
    "title": "Grading Contract",
    "section": "FAQ",
    "text": "FAQ\n\nWhat kind of feedback will I receive regarding my work?\nYou will receive extensive and timely feedback on all of your submissions (from me and from your colleagues), and you will have opportunities to revise. I will also provide rubrics for most assignments to indicate how I will evaluate your work. However, you will not receive points or an A, B, C, D, etc. on individual assignments.\n\n\nThere appear to be a significant number of assignments and deadlines in this course. Why is this the case?\nI’ve been very deliberate in pacing your assignments for this course for a number of reasons. Developing the critical thinking skills required of ethnography demands building and consistently practicing observational habits. I want you to leave this class being more mindful of the cultural forces shaping the numbers you see everyday. You are being asked to complete several short reflective writing assignments for this course so that you can start to build those habits. I am aware that this will mean that, sometimes, the writing you produce for this course will not be the very best quality you can produce. Especially for the field note assignments, I have prioritized consistency over depth of reflection because the primary goal there is to encourage you to become more habitually observant. This is also why you have the opportunity to select assignments you want to invest more time on when aiming for an A. If you are struggling with this course’s workload, I recommend coming to talk with me in office hours.\n\n\nHow can I best keep track of my labor in this course?\nThroughout the semester, you will be asked to keep track of your labor via a log that you will share with me. I will reference this log to calculate certain aspects of your final grade. Instructions for using the labor log will be provided in the second week of the course.\n\n\nCan I earn +/- grades in this course?\n+/- will be assigned to final grades at my discretion in cases where a student’s work consistently exceeds the expectations (+) of their contracted grade or is in some way insufficient (-). Students can track their progress towards a grade modifier in feedback that I provide throughout the semester. Your grade may also be reduced by a grade modifier for each missed class beyond the number associated with your contracted grade.\n\n\nWhat if I need an extension on an assignment?\nThere is a 24-hour grace period on all written assignments, except for reading annotations. There will be no penalties for submitting the written assignment within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any written assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted.\n\n\nCan I ask for extensions or use the grace period for reading annotations?\nReading assignments/Perusall annotations prepare you to participate in class discussions. For this reason, they need to be completed by the due date for credit. I’ve provided considerable leeway to miss a reading annotation assignment from time-to-time in order to accommodate flexibility in this regard.\n\n\nWhat if I don’t know how to complete an assignment at an advanced level?\nIf you are not sure how to complete assignments at an advanced level, you should plan to come talk with me in office hours. I’m more than happy to help you brainstorm ideas!\n\n\nAn assignment deadline is approaching, and I’m unhappy with the quality of my work. What should I do?\nWhile I always encourage students to strive to submit the best work they can, this course’s grading contract, coupled with the course’s revision assignments, permits you to submit work that you know you want to continue to improve upon without penalty. Not submitting assignments at all has the potentially to significantly lower your grade in this course, whereas submitting an assignment that could benefit from more revision will not be detrimental to your grade as long as the minimum submission requirements are met."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Acknowledgements\n\n\n\nStyling and infrastructure for this page inspired by related syllabi produced by Ben Baumer and R. Jordan Crouser."
  },
  {
    "objectID": "schedule.html#january-26-2022",
    "href": "schedule.html#january-26-2022",
    "title": "Schedule",
    "section": "January 26, 2022",
    "text": "January 26, 2022\n\nIntroductions\nBECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Fill out the First Day of Class Questionnaire\n\n\n Course slides are here."
  },
  {
    "objectID": "schedule.html#january-31-2022",
    "href": "schedule.html#january-31-2022",
    "title": "Schedule",
    "section": "January 31, 2022",
    "text": "January 31, 2022\n\nHegemonic Backdrops of Big Data\nEPISTEMOLOGY DISCOURSE ANALYSIS BECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) boyd, danah and Kate Crawford (2012). “Critical Questions for Big Data”. In: Information, Communication & Society 15.5, pp. 662-679. (Visited on Jan. 19, 2018).\n Fill out the Trigger Warnings Questionnaire in Moodle.\n Install Desktop version of Slack and configure notifications for our course.\n\n\n Course slides are here\n (Read in Perusall) Kitchin, Rob (2014). “Big Data, new epistemologies and paradigm shifts”. En. In: Big Data & Society 1.1, p. 2053951714528481. (Visited on Jul. 16, 2019).\n (Read in Perusall) Leonelli, S. (2014). “What difference does quantity make? On the epistemology of Big Data in biology:”. En. In: Big Data & Society. Publisher: SAGE PublicationsSage UK: London, England. (Visited on Mar. 28, 2020).\n (Read in Perusall) Onuoha, Mimi (2016). The Point of Collection. En. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#february-02-2022",
    "href": "schedule.html#february-02-2022",
    "title": "Schedule",
    "section": "February 02, 2022",
    "text": "February 02, 2022\n\nMetaphors of Big Data\nDISCOURSE ANALYSIS DISCOURSE\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Levy Karen, Tim Hwang (2015). ‘The Cloud’ and Other Dangerous Metaphors. En. Section: Technology. (Visited on Aug. 29, 2021).\n (Read in Perusall) Puschmann, Cornelius and Jean Burgess (2014). “Metaphors of Big Data”. En. In: International Journal of Communication 8.0, p. 20. (Visited on May. 02, 2016).\n Create a GitHub account if you don’t have one\n Click on the Student Portfolio GitHub Repo in Moodle to create your portfolio\n Acknowledge that you’ve read and understand the grading contract by completing the Grading Contract Acknowledgement in Moodle\n Direct message Rose in Slack if you would like to lead a class discussion enrichment assignment.\n\n\n Here is the article we will engage in today’s activity.\n Discourse Analysis in Nine Steps is here.\n (Read in Perusall) Watson, Sarah M. (2021). Metaphors of Big Data. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#february-07-2022",
    "href": "schedule.html#february-07-2022",
    "title": "Schedule",
    "section": "February 07, 2022",
    "text": "February 07, 2022\n\nBinary Oppositions in Big Data Discourse\nDISCOURSE ANALYSIS INTERPRETING CULTURAL MEANING\n\nDue TodayFurther Resources\n\n\n Complete course infrastructure set-up by following instructions in the setting-up-r-environment directory in your GitHub portfolio\n DM Rose if you’d like to lead a class discussion for enrichment"
  },
  {
    "objectID": "schedule.html#february-09-2022",
    "href": "schedule.html#february-09-2022",
    "title": "Schedule",
    "section": "February 09, 2022",
    "text": "February 09, 2022\n\nThick Data for Big Data\nTHICK DESCRIPTION BECOMING OBSERVANT COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Fiore-Silfvast, Brittany (2014). Hacked Ethnographic Fieldnotes. En. (Visited on Feb. 18, 2021).\n (Read in Perusall) Burrell, Jenna (2012). The Ethnographer’s Complete Guide to Big Data: Small Data People in a Big Data World. (Visited on Aug. 20, 2021).\n Fill out CATME Survey (link sent to your email)\n DM Rose if you’d like to lead a class discussion for enrichment\n\n\n (Read in Perusall) Wang, Tricia (2013). Big Data Needs Thick Data. (Visited on Sep. 10, 2019)."
  },
  {
    "objectID": "schedule.html#february-14-2022",
    "href": "schedule.html#february-14-2022",
    "title": "Schedule",
    "section": "February 14, 2022",
    "text": "February 14, 2022\n\nEthnography in Data Land\nTHICK DESCRIPTION BECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Introduction , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1."
  },
  {
    "objectID": "schedule.html#february-16-2022",
    "href": "schedule.html#february-16-2022",
    "title": "Schedule",
    "section": "February 16, 2022",
    "text": "February 16, 2022\n\nData Looping Effects\nANALYZING SOCIAL FORCES AND SYSTEMS EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Kitchin, Rob and Tracey P. Lauriault (2014). Towards Critical Data Studies: Charting and Unpacking Data Assemblages and Their Work. SSRN Scholarly Paper ID 2474112. Rochester, NY: Social Science Research Network. (Visited on Nov. 07, 2017).\n\n\n (Read in Perusall) Hacking, Ian (2006). “Making Up People”. In: London Review of Books 28.\n (Read in Perusall) Urla, Jacqueline (1993). “Cultural Politics in an Age of Statistics: Numbers, Nations, and the Making of Basque Identity”. In: American Ethnologist 20.4. Publisher: [Wiley, American Anthropological Association], pp. 818-843. (Visited on Aug. 30, 2021).\n (Read in Perusall) Kristensen, Dorthe Brogård and Minna Ruckenstein (2018). “Co-evolving with self-tracking technologies”. En. In: New Media & Society 20.10. Publisher: SAGE Publications, pp. 3624-3640. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#february-21-2022",
    "href": "schedule.html#february-21-2022",
    "title": "Schedule",
    "section": "February 21, 2022",
    "text": "February 21, 2022\n\nEthics of Qualitative Research\nPOWER\n\nDue TodayFurther Resources\n\n\n Team Contract\n Fieldnote 1"
  },
  {
    "objectID": "schedule.html#february-23-2022",
    "href": "schedule.html#february-23-2022",
    "title": "Schedule",
    "section": "February 23, 2022",
    "text": "February 23, 2022\n\nDocumenting Datasets\nDATA DOCUMENTATION COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, et al. (2020). “Datasheets for Datasets”. In: arXiv:1803.09010 [cs]. arXiv: 1803.09010. (Visited on Jan. 24, 2021).\n Get approval for dataset\n\n\n Potential final project datasets are here.\n (Read in Perusall) Bender, Emily M. and Batya Friedman (2018). “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science”. In: Transactions of the Association for Computational Linguistics 6, pp. 587-604. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#february-28-2022",
    "href": "schedule.html#february-28-2022",
    "title": "Schedule",
    "section": "February 28, 2022",
    "text": "February 28, 2022\n\nMaking Measures Commensurate: Translation and Reductionism\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING TRANSLATION INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Chapter 1 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n\n\n (Read in Perusall) Espeland, Wendy Nelson and Mitchell L. Stevens (1998). “Commensuration as a Social Process”. In: Annual Review of Sociology 24.1. Publisher: Annual Reviews, pp. 313-343. (Visited on Aug. 30, 2021).\n (Read in Perusall) Merry, Sally Engle (2016). The Seductions of Quantification: Measuring Human Rights, Gender Violence, and Sex Trafficking. En. Google-Books-ID: 0FcqDAAAQBAJ. University of Chicago Press. ISBN: 978-0-226-26131-7."
  },
  {
    "objectID": "schedule.html#march-02-2022",
    "href": "schedule.html#march-02-2022",
    "title": "Schedule",
    "section": "March 02, 2022",
    "text": "March 02, 2022\n\nEthnographies of Infrastructure\nCULTURAL ANALYSIS OF INFRASTRUCTURE BECOMING OBSERVANT INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Star, Susan Leigh (1999). “The Ethnography of Infrastructure”. En. In: American Behavioral Scientist 43.3, pp. 377-391. (Visited on Feb. 18, 2016).\n Work on semiotic analysis\n\n\n (Read in Perusall) Lampland, Martha and Susan Leigh Star, ed. (2008). Standards and Their Stories: How Quantifying, Classifying, and Formalizing Practices Shape Everyday Life. 1 edition. Ithaca: Cornell University Press. ISBN: 978-0-8014-7461-3.\n (Read in Perusall) Ottinger, Gwen (2010). “Buckets of Resistance: Standards and the Effectiveness of Citizen Science”. En. In: Science, Technology, & Human Values 35.2, pp. 244-270. (Visited on Oct. 05, 2019).\n (Read in Perusall) Timmermans, Stefan and Steven Epstein (2010). “A World of Standards but not a Standard World: Toward a Sociology of Standards and Standardization*“. In: Annual Review of Sociology 36.1, pp. 69-89. (Visited on Oct. 16, 2014)."
  },
  {
    "objectID": "schedule.html#march-07-2022",
    "href": "schedule.html#march-07-2022",
    "title": "Schedule",
    "section": "March 07, 2022",
    "text": "March 07, 2022\n\nSorting Things Out: Cultural Analyses of Categories\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Bowker, Geoffrey C. (1998). “The Kindness of Strangers: Kinds and Politics in Classification Systems”. En. In: Library Trends 47.2, pp. 255-292. (Visited on Oct. 14, 2019).\n Fieldnote 2\n Be sure to get approval for the TED Talks you plan to view for Mini-Project 1.\n\n\n (Read in Perusall) Bowker, Geoffrey C. and Susan Leigh Star (1999). Sorting Things Out: Classification and Its Consequences. En. Cambridge, MA: MIT Press. ISBN: 978-0-262-52295-3.\n (Read in Perusall) Waterton, Claire (2002). “From Field to Fantasy: Classifying Nature, Constructing Europe”. En. In: Social Studies of Science 32.2, pp. 177-204. (Visited on May. 15, 2019).\n (Read in Perusall) Kirksey, Eben (2015). “Species: a praxiographic study”. Fr. In: Journal of the Royal Anthropological Institute 21.4, pp. 758-780. (Visited on Oct. 05, 2019)."
  },
  {
    "objectID": "schedule.html#march-09-2022",
    "href": "schedule.html#march-09-2022",
    "title": "Schedule",
    "section": "March 09, 2022",
    "text": "March 09, 2022\n\nInfrastructure Field Day\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#march-21-2022",
    "href": "schedule.html#march-21-2022",
    "title": "Schedule",
    "section": "March 21, 2022",
    "text": "March 21, 2022\n\nData Ghost Work\nINTERVIEWING ANALYZING SOCIAL FORCES AND SYSTEMS LABOR\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Chapter 1 , Gray, Mary L. and Siddharth Suri (2019). Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass. Illustrated edition. Boston: Mariner Books. ISBN: 978-1-328-56624-9.\n Work on stakholder analysis\n\n\n (Read in Perusall) Irani, Lilly (2015). Justice for “Data Janitors”. En-US. (Visited on Dec. 13, 2018).\n (Read in Perusall) Plantin, Jean-Christophe (2019). “Data Cleaners for Pristine Datasets: Visibility and Invisibility of Data Processors in Social Science”. En. In: Science, Technology, & Human Values 44.1. Publisher: SAGE Publications Inc, pp. 52-73. (Visited on Aug. 20, 2021).\n (Read in Perusall) Forsythe, Diana E. (1993). “The Construction of Work in Artificial Intelligence”. En. In: Science, Technology, & Human Values 18.4. Publisher: SAGE Publications Inc, pp. 460-479. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#march-23-2022",
    "href": "schedule.html#march-23-2022",
    "title": "Schedule",
    "section": "March 23, 2022",
    "text": "March 23, 2022\n\nSocial Constructions of Expertise in Data Work\nINTERVIEWING INTERPRETING CULTURAL MEANING LABOR\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Chapter 2 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n Mini-Project 1\n\n\n Here is a link to the Final Project guide that Rose developed.\n (Read in Perusall) Gieryn, Thomas F. (1999). Cultural Boundaries of Science: Credibility on the Line. En. University of Chicago Press. ISBN: 978-0-226-29261-8."
  },
  {
    "objectID": "schedule.html#march-28-2022",
    "href": "schedule.html#march-28-2022",
    "title": "Schedule",
    "section": "March 28, 2022",
    "text": "March 28, 2022\n\nHow Data Domesticates Us: Rituals for Data Cleaning\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Ribes, David and Steven J Jackson (2013). “Data bite man: The work of sustaining a long-term study”. In: Raw data” is an oxymoron. Ed. by Lisa Gitelman. Cambridge, MA: MIT Press, pp. 147-166.\n Work on ritual analysis\n\n\n (Read in Perusall) Bowker, Geoffrey C. (2000). “Biodiversity Datadiversity”. En. In: Social Studies of Science 30.5, pp. 643-683. (Visited on May. 14, 2014).\n (Read in Perusall) Walford, Antonia (2017). “Raw Data: Making Relations Matter”. En_US. In: Social Analysis 61.2. Publisher: Berghahn Journals Section: Social Analysis, pp. 65-80. (Visited on Aug. 20, 2021).\n (Read in Perusall) Pink, Sarah, Shanti Sumartojo, Deborah Lupton, et al. (2017). “Mundane data: The routines, contingencies and accomplishments of digital living”. En. In: Big Data & Society 4.1. Publisher: SAGE Publications Ltd, p. 2053951717700924. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#march-30-2022",
    "href": "schedule.html#march-30-2022",
    "title": "Schedule",
    "section": "March 30, 2022",
    "text": "March 30, 2022\n\nCooking Data\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Fieldnote 3"
  },
  {
    "objectID": "schedule.html#april-04-2022",
    "href": "schedule.html#april-04-2022",
    "title": "Schedule",
    "section": "April 04, 2022",
    "text": "April 04, 2022\n\nInstitutional Incentives in Data Reporting\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Work on institutional analysis"
  },
  {
    "objectID": "schedule.html#april-06-2022",
    "href": "schedule.html#april-06-2022",
    "title": "Schedule",
    "section": "April 06, 2022",
    "text": "April 06, 2022\n\nEconomies of Data Production: Gifts and Transactions\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Chapter 4 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n\n\n (Read in Perusall) Gerlitz, Carolin and Anne Helmond (2013). “The like economy: Social buttons and the data-intensive web”. En. In: New Media & Society 15.8. Publisher: SAGE Publications, pp. 1348-1365. (Visited on Aug. 30, 2021).\n (Read in Perusall) Beer, David (2015). “Productive measures: Culture and measurement in the context of everyday neoliberalism”. En. In: Big Data & Society 2.1. Publisher: SAGE Publications Ltd, p. 2053951715578951. (Visited on Aug. 29, 2021)."
  },
  {
    "objectID": "schedule.html#april-11-2022",
    "href": "schedule.html#april-11-2022",
    "title": "Schedule",
    "section": "April 11, 2022",
    "text": "April 11, 2022\n\nMobilizing Data: Making Numbers Actionable\nDISCOURSE ANALYSIS SITUATING KNOWLEDGE MOBILIZATION\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Ottinger, Gwen and Rachel Zurer (2011). New Voices, New Approaches: Drowning in Data. En-US. (Visited on Dec. 13, 2018).\n Work on discourse analysis\n Mini-Project 2\n\n\n (Read in Perusall) Pine, Kathleen H. and Max Liboiron (2015). “The Politics of Measurement and Action”. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery, pp. 3147-3156. ISBN: 978-1-4503-3145-6. (Visited on Aug. 30, 2021).\n (Read in Perusall) Dourish, Paul and Edgar Gómez Cruz (2018). “Datafication and data fiction: Narrating data and narrating with data”. En. In: Big Data & Society 5.2. Publisher: SAGE Publications Ltd, p. 2053951718784083. (Visited on Apr. 05, 2021)."
  },
  {
    "objectID": "schedule.html#april-13-2022",
    "href": "schedule.html#april-13-2022",
    "title": "Schedule",
    "section": "April 13, 2022",
    "text": "April 13, 2022\n\nData Circulation\nDISCOURSE ANALYSIS COMMUNICATING (IN) CONTEXT MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Fieldnote 4\n\n\n (Read in Perusall) Bates, Jo, Yu-Wei Lin, and Paula Goodale (2016). “Data journeys: Capturing the socio-material constitution of data objects and flows”. En. In: Big Data & Society 3.2. Publisher: SAGE Publications Ltd, p. 2053951716654502. (Visited on Mar. 28, 2020).\n (Read in Perusall) Leonelli, Sabina (2010). “Packaging Small Fact for Re-use: Databases in Model Organism Biology”. En. In: How Well Do Facts Travel?: The Dissemination of Reliable Knowledge. Ed. by Peter Howlett and Mary S. Morgan. Cambridge, MA: Cambridge University Press, pp. 325-348. ISBN: 978-1-139-49239-3."
  },
  {
    "objectID": "schedule.html#april-18-2022",
    "href": "schedule.html#april-18-2022",
    "title": "Schedule",
    "section": "April 18, 2022",
    "text": "April 18, 2022\n\nMobilizing Data Otherwise: Citizen Science and Sensing\nDISCOURSE SITUATING KNOWLEDGE MOBILIZATION CREDIBILITY\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Gabrys, Jennifer, Helen Pritchard, and Benjamin Barratt (2016). “Just good enough data: Figuring data citizenships through air pollution sensing and data stories”. En. In: Big Data & Society 3.2. Publisher: SAGE Publications Ltd, p. 2053951716679677. (Visited on Mar. 28, 2020).\n First draft due\n\n\n (Read in Perusall) Calvillo, Nerea (2018). “Political airs: From monitoring to attuned sensing air pollution”. En. In: Social Studies of Science 48.3, pp. 372-388. (Visited on Sep. 24, 2019).\n (Read in Perusall) Jalbert, Kirk and Abby J. Kinchy (2016). “Sense and Influence: Environmental Monitoring Tools and the Power of Citizen Science”. In: Journal of Environmental Policy & Planning 18.3. Publisher: Routledge _ eprint: https://doi.org/10.1080/1523908X.2015.1100985, pp. 379-397. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#april-20-2022",
    "href": "schedule.html#april-20-2022",
    "title": "Schedule",
    "section": "April 20, 2022",
    "text": "April 20, 2022\n\nData Activism and Advocacy\nSITUATING KNOWLEDGE MOBILIZATION CREDIBILITY\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Liboiron, Max (2015). “Disaster Data, Data Activism : Grassroots Responses to Representing Superstorm Sandy”. En. In: Extreme Weather and Global Media. Ed. by Julia Leyda and Diane Negra. Taylor & Francis Group. (Visited on Aug. 27, 2019).\n\n\n (Read in Perusall) Bruno, Isabelle, Emmanuel Didier, and Tommaso Vitale (2014). Statactivism: Forms of Action between Disclosure and Affirmation. En. SSRN Scholarly Paper ID 2466882. Rochester, NY: Social Science Research Network. (Visited on Dec. 18, 2018).\n (Read in Perusall) Milan, Stefania and Lonneke van der Velden (2016). “The Alternative Epistemologies of Data Activism”. In: Digital Culture & Society 2.2, pp. 57-74. (Visited on Jul. 16, 2019).\n (Read in Perusall) Currie, Morgan, Britt S Paris, Irene Pasquetto, et al. (2016). “The conundrum of police officer-involved homicides: Counter-data in Los Angeles County”. En. In: Big Data & Society 3.2, p. 2053951716663566. (Visited on Aug. 08, 2018)."
  },
  {
    "objectID": "schedule.html#april-25-2022",
    "href": "schedule.html#april-25-2022",
    "title": "Schedule",
    "section": "April 25, 2022",
    "text": "April 25, 2022\n\nData Agnotology: Ignorance and Knowledge Gaps\nEPISTEMOLOGY IGNORANCE SITUATING KNOWLEDGE\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) mimimimimi (2021). On Missing Data Sets. original-date: 2016-02-03T16:30:28Z. (Visited on Aug. 20, 2021).\n (Read in Perusall) Milan, Stefania and Emiliano Treré (2020). “The Rise of the Data Poor: The COVID-19 Pandemic Seen From the Margins”. En. In: Social Media + Society 6.3. Publisher: SAGE Publications Ltd, p. 2056305120948233. (Visited on Aug. 31, 2021).\n\n\n (Read in Perusall) D’Ignazio, Catherine and Lauren F. Klein (2020). Data Feminism. Cambridge, Massachusetts: The MIT Press. ISBN: 978-0-262-04400-4."
  },
  {
    "objectID": "schedule.html#april-27-2022",
    "href": "schedule.html#april-27-2022",
    "title": "Schedule",
    "section": "April 27, 2022",
    "text": "April 27, 2022\n\nData and Algorithmic Power\nPOWER SITUATING KNOWLEDGE EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n (Read in Perusall) Eubanks, Virginia (2018). “A Child Abuse Prediction Model Fails Poor Families”. In: Wired. (Visited on Mar. 28, 2019).\n Fieldnote 5\n\n\n (Read in Perusall) Brayne, Sarah (2017). “Big Data Surveillance: The Case of Policing”. In: American Sociological Review 82.5. Publisher: SAGE Publications Inc, pp. 977-1008. (Visited on Aug. 18, 2021).\n (Read in Perusall) Christin, Angèle (2020). “The ethnographer and the algorithm: beyond the black box”. En. In: Theory and Society 49.5, pp. 897-918. (Visited on Aug. 31, 2021).\n (Read in Perusall) Seaver, Nick (2017). “Algorithms as culture: Some tactics for the ethnography of algorithmic systems”. En. In: Big Data & Society 4.2. Publisher: SAGE Publications Ltd, p. 2053951717738104. (Visited on Jan. 22, 2021)."
  },
  {
    "objectID": "schedule.html#may-02-2022",
    "href": "schedule.html#may-02-2022",
    "title": "Schedule",
    "section": "May 02, 2022",
    "text": "May 02, 2022\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Mini-Project Revisions"
  },
  {
    "objectID": "schedule.html#may-04-2022",
    "href": "schedule.html#may-04-2022",
    "title": "Schedule",
    "section": "May 04, 2022",
    "text": "May 04, 2022\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Final project due\n Enrichment"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day3-BigDataDiscourse.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Three: Big Data Discourse",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhat data discourses are the words you wrote on the left side embedded within?\nCan you identify any terms that might fit in between these opposites?"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#discourse",
    "href": "slides/Day3-BigDataDiscourse.html#discourse",
    "title": "Day Three: Big Data Discourse",
    "section": "Discourse",
    "text": "Discourse\n\nHow we communicate or converse about topics, people, and things\nDominant discourse characterizes the discourses that emerge as predominant throughout society\n\nShapes our values, identities, behaviors, and interactions with each other\nAlso shapes, disseminates, and is prodded by our ideologies, or worldviews\n\nCultural hegemony describes when our ideologies reflect those with power over us"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#technology-discourse",
    "href": "slides/Day3-BigDataDiscourse.html#technology-discourse",
    "title": "Day Three: Big Data Discourse",
    "section": "Technology Discourse",
    "text": "Technology Discourse\n\nTechnocratic: Technology will fix social problems.\n\nComputers will save the world!\nOther examples?\n\nDystopian: Technology is frightening or debilitating.\n\nRobots will take over all jobs.\nOther examples?\n\nDeterminist: Technology determines how society operates.\n\nMobile phones are making us anti-social.\n\nOther examples?"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#discourse-analysis-in-nine-steps",
    "href": "slides/Day3-BigDataDiscourse.html#discourse-analysis-in-nine-steps",
    "title": "Day Three: Big Data Discourse",
    "section": "Discourse Analysis in Nine Steps",
    "text": "Discourse Analysis in Nine Steps\n\nEstablish the context\nConsider the medium\nDiscern the intended audience\nAssess assumptions\nIdentify cultural cues and references\nEvaluate rhetorical strategies and methods of delivery\nConsider the social structures the discourse operates within\nAssess how the discourse disseminates\nReflect on what is not said or who is not included"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#activity",
    "href": "slides/Day3-BigDataDiscourse.html#activity",
    "title": "Day Three: Big Data Discourse",
    "section": "Activity",
    "text": "Activity\n\nRead article linked under today’s announcements\nHighlight statements that indicate Anderson’s worldview as you read\nReferencing the Discourse Analysis in 9 Steps, discuss components of Anderson’s discourse in small groups. Be sure to discuss Steps 4, 5, and 6 particularly."
  },
  {
    "objectID": "schedule.html#january-26-2023",
    "href": "schedule.html#january-26-2023",
    "title": "Schedule",
    "section": "January 26, 2023",
    "text": "January 26, 2023\n\nIntroductions\nBECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Fill out the First Day of Class Questionnaire\n\n\n Course slides are here."
  },
  {
    "objectID": "schedule.html#january-31-2023",
    "href": "schedule.html#january-31-2023",
    "title": "Schedule",
    "section": "January 31, 2023",
    "text": "January 31, 2023\n\nHegemonic Backdrops of Big Data\nEPISTEMOLOGY DISCOURSE ANALYSIS BECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n boyd, danah and Kate Crawford (2012). “Critical Questions for Big Data”. In: Information, Communication & Society 15.5, pp. 662-679. (Visited on Jan. 19, 2018).\n Fill out the Trigger Warnings Questionnaire in Moodle.\n Install Desktop version of Slack and configure notifications for our course.\n\n\n Course slides are here\n Kitchin, Rob (2014). “Big Data, new epistemologies and paradigm shifts”. En. In: Big Data & Society 1.1, p. 2053951714528481. (Visited on Jul. 16, 2019).\n Leonelli, S. (2014). “What difference does quantity make? On the epistemology of Big Data in biology:”. En. In: Big Data & Society. Publisher: SAGE PublicationsSage UK: London, England. (Visited on Mar. 28, 2020).\n Onuoha, Mimi (2016). The Point of Collection. En. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#february-02-2023",
    "href": "schedule.html#february-02-2023",
    "title": "Schedule",
    "section": "February 02, 2023",
    "text": "February 02, 2023\n\nMetaphors of Big Data\nDISCOURSE ANALYSIS DISCOURSE\n\nDue TodayFurther Resources\n\n\n Levy Karen, Tim Hwang (2015). ‘The Cloud’ and Other Dangerous Metaphors. En. Section: Technology. (Visited on Aug. 29, 2021).\n Puschmann, Cornelius and Jean Burgess (2014). “Metaphors of Big Data”. En. In: International Journal of Communication 8.0, p. 20. (Visited on May. 02, 2016).\n Create a GitHub account if you don’t have one\n Click on the Student Portfolio GitHub Repo in Moodle to create your portfolio\n Acknowledge that you’ve read and understand the grading contract by completing the Grading Contract Acknowledgement in Moodle\n Direct message Rose in Slack if you would like to lead a class discussion enrichment assignment.\n\n\n Course slides are here\n Here is the article we will engage in today’s activity.\n Discourse Analysis in Nine Steps is here.\n Watson, Sarah M. (2021). Metaphors of Big Data. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#february-07-2023",
    "href": "schedule.html#february-07-2023",
    "title": "Schedule",
    "section": "February 07, 2023",
    "text": "February 07, 2023\n\nBinary Oppositions in Big Data Discourse\nDISCOURSE ANALYSIS INTERPRETING CULTURAL MEANING\n\nDue TodayFurther Resources\n\n\n Complete course infrastructure set-up by following instructions in the setting-up-r-environment directory in your GitHub portfolio\n DM Rose if you’d like to lead a class discussion for enrichment\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#february-09-2023",
    "href": "schedule.html#february-09-2023",
    "title": "Schedule",
    "section": "February 09, 2023",
    "text": "February 09, 2023\n\nThick Data for Big Data\nTHICK DESCRIPTION BECOMING OBSERVANT COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Fiore-Silfvast, Brittany (2014). Hacked Ethnographic Fieldnotes. En. (Visited on Feb. 18, 2021).\n Burrell, Jenna (2012). The Ethnographer’s Complete Guide to Big Data: Small Data People in a Big Data World. (Visited on Aug. 20, 2021).\n Fill out CATME Survey (link sent to your email)\n DM Rose if you’d like to lead a class discussion for enrichment\n\n\n Course slides are here\n Here’s an example of some very short “thick description” write-ups of two data environments from my own research.\n Wang, Tricia (2013). Big Data Needs Thick Data. (Visited on Sep. 10, 2019)."
  },
  {
    "objectID": "schedule.html#february-14-2023",
    "href": "schedule.html#february-14-2023",
    "title": "Schedule",
    "section": "February 14, 2023",
    "text": "February 14, 2023\n\nEthnography in Data Land\nTHICK DESCRIPTION BECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Introduction , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#february-16-2023",
    "href": "schedule.html#february-16-2023",
    "title": "Schedule",
    "section": "February 16, 2023",
    "text": "February 16, 2023\n\nData Looping Effects\nANALYZING SOCIAL FORCES AND SYSTEMS EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n Kitchin, Rob and Tracey P. Lauriault (2014). Towards Critical Data Studies: Charting and Unpacking Data Assemblages and Their Work. SSRN Scholarly Paper ID 2474112. Rochester, NY: Social Science Research Network. (Visited on Nov. 07, 2017).\n\n\n Course slides are here.\n Hacking, Ian (2006). “Making Up People”. In: London Review of Books 28.\n Urla, Jacqueline (1993). “Cultural Politics in an Age of Statistics: Numbers, Nations, and the Making of Basque Identity”. In: American Ethnologist 20.4. Publisher: [Wiley, American Anthropological Association], pp. 818-843. (Visited on Aug. 30, 2021).\n Kristensen, Dorthe Brogård and Minna Ruckenstein (2018). “Co-evolving with self-tracking technologies”. En. In: New Media & Society 20.10. Publisher: SAGE Publications, pp. 3624-3640. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#february-21-2023",
    "href": "schedule.html#february-21-2023",
    "title": "Schedule",
    "section": "February 21, 2023",
    "text": "February 21, 2023\n\nEthics of Qualitative Research\nPOWER\n\nDue TodayFurther Resources\n\n\n Team Contract\n Fieldnote 1\n\n\n Course slides are here."
  },
  {
    "objectID": "schedule.html#february-23-2023",
    "href": "schedule.html#february-23-2023",
    "title": "Schedule",
    "section": "February 23, 2023",
    "text": "February 23, 2023\n\nDocumenting Datasets\nDATA DOCUMENTATION COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, et al. (2020). “Datasheets for Datasets”. In: arXiv:1803.09010 [cs]. arXiv: 1803.09010. (Visited on Jan. 24, 2021).\n Get approval for dataset\n\n\n Course slides are here\n Potential final project datasets are here.\n Bender, Emily M. and Batya Friedman (2018). “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science”. In: Transactions of the Association for Computational Linguistics 6, pp. 587-604. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#february-28-2023",
    "href": "schedule.html#february-28-2023",
    "title": "Schedule",
    "section": "February 28, 2023",
    "text": "February 28, 2023\n\nMaking Measures Commensurate: Translation and Reductionism\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING TRANSLATION INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Chapter 1 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n\n\n Course slides are here\n Activity link is here and here and here\n Espeland, Wendy Nelson and Mitchell L. Stevens (1998). “Commensuration as a Social Process”. In: Annual Review of Sociology 24.1. Publisher: Annual Reviews, pp. 313-343. (Visited on Aug. 30, 2021).\n Merry, Sally Engle (2016). The Seductions of Quantification: Measuring Human Rights, Gender Violence, and Sex Trafficking. En. Google-Books-ID: 0FcqDAAAQBAJ. University of Chicago Press. ISBN: 978-0-226-26131-7."
  },
  {
    "objectID": "schedule.html#march-02-2023",
    "href": "schedule.html#march-02-2023",
    "title": "Schedule",
    "section": "March 02, 2023",
    "text": "March 02, 2023\n\nEthnographies of Infrastructure\nCULTURAL ANALYSIS OF INFRASTRUCTURE BECOMING OBSERVANT INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Star, Susan Leigh (1999). “The Ethnography of Infrastructure”. En. In: American Behavioral Scientist 43.3, pp. 377-391. (Visited on Feb. 18, 2016).\n Work on semiotic analysis\n\n\n Course slides are here\n Lampland, Martha and Susan Leigh Star, ed. (2008). Standards and Their Stories: How Quantifying, Classifying, and Formalizing Practices Shape Everyday Life. 1 edition. Ithaca: Cornell University Press. ISBN: 978-0-8014-7461-3.\n Ottinger, Gwen (2010). “Buckets of Resistance: Standards and the Effectiveness of Citizen Science”. En. In: Science, Technology, & Human Values 35.2, pp. 244-270. (Visited on Oct. 05, 2019).\n Timmermans, Stefan and Steven Epstein (2010). “A World of Standards but not a Standard World: Toward a Sociology of Standards and Standardization*“. In: Annual Review of Sociology 36.1, pp. 69-89. (Visited on Oct. 16, 2014)."
  },
  {
    "objectID": "schedule.html#march-07-2023",
    "href": "schedule.html#march-07-2023",
    "title": "Schedule",
    "section": "March 07, 2023",
    "text": "March 07, 2023\n\nSorting Things Out: Cultural Analyses of Categories\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Bowker, Geoffrey C. (1998). “The Kindness of Strangers: Kinds and Politics in Classification Systems”. En. In: Library Trends 47.2, pp. 255-292. (Visited on Oct. 14, 2019).\n Be sure to get approval for the TED Talks you plan to view for Mini-Project 1.\n\n\n Bowker, Geoffrey C. and Susan Leigh Star (1999). Sorting Things Out: Classification and Its Consequences. En. Cambridge, MA: MIT Press. ISBN: 978-0-262-52295-3.\n Waterton, Claire (2002). “From Field to Fantasy: Classifying Nature, Constructing Europe”. En. In: Social Studies of Science 32.2, pp. 177-204. (Visited on May. 15, 2019).\n Kirksey, Eben (2015). “Species: a praxiographic study”. Fr. In: Journal of the Royal Anthropological Institute 21.4, pp. 758-780. (Visited on Oct. 05, 2019)."
  },
  {
    "objectID": "schedule.html#march-09-2023",
    "href": "schedule.html#march-09-2023",
    "title": "Schedule",
    "section": "March 09, 2023",
    "text": "March 09, 2023\n\nInfrastructure Field Day\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Fieldnote 2"
  },
  {
    "objectID": "schedule.html#march-21-2023",
    "href": "schedule.html#march-21-2023",
    "title": "Schedule",
    "section": "March 21, 2023",
    "text": "March 21, 2023\n\nData Ghost Work\nINTERVIEWING ANALYZING SOCIAL FORCES AND SYSTEMS LABOR\n\nDue TodayFurther Resources\n\n\n Chapter 1 , Gray, Mary L. and Siddharth Suri (2019). Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass. Illustrated edition. Boston: Mariner Books. ISBN: 978-1-328-56624-9.\n Work on stakholder analysis\n\n\n Course slides are here\n Irani, Lilly (2015). Justice for “Data Janitors”. En-US. (Visited on Dec. 13, 2018).\n Plantin, Jean-Christophe (2019). “Data Cleaners for Pristine Datasets: Visibility and Invisibility of Data Processors in Social Science”. En. In: Science, Technology, & Human Values 44.1. Publisher: SAGE Publications Inc, pp. 52-73. (Visited on Aug. 20, 2021).\n Forsythe, Diana E. (1993). “The Construction of Work in Artificial Intelligence”. En. In: Science, Technology, & Human Values 18.4. Publisher: SAGE Publications Inc, pp. 460-479. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#march-23-2023",
    "href": "schedule.html#march-23-2023",
    "title": "Schedule",
    "section": "March 23, 2023",
    "text": "March 23, 2023\n\nSocial Constructions of Expertise in Data Work\nINTERVIEWING INTERPRETING CULTURAL MEANING LABOR\n\nDue TodayFurther Resources\n\n\n Chapter 2 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n Mini-Project 1\n\n\n Here is a link to the Final Project guide that Rose developed.\n Gieryn, Thomas F. (1999). Cultural Boundaries of Science: Credibility on the Line. En. University of Chicago Press. ISBN: 978-0-226-29261-8."
  },
  {
    "objectID": "schedule.html#march-28-2023",
    "href": "schedule.html#march-28-2023",
    "title": "Schedule",
    "section": "March 28, 2023",
    "text": "March 28, 2023\n\nHow Data Domesticates Us: Rituals for Data Cleaning\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Ribes, David and Steven J Jackson (2013). “Data bite man: The work of sustaining a long-term study”. In: Raw data” is an oxymoron. Ed. by Lisa Gitelman. Cambridge, MA: MIT Press, pp. 147-166.\n Work on ritual analysis\n\n\n Course slides are here\n Bowker, Geoffrey C. (2000). “Biodiversity Datadiversity”. En. In: Social Studies of Science 30.5, pp. 643-683. (Visited on May. 14, 2014).\n Walford, Antonia (2017). “Raw Data: Making Relations Matter”. En_US. In: Social Analysis 61.2. Publisher: Berghahn Journals Section: Social Analysis, pp. 65-80. (Visited on Aug. 20, 2021).\n Pink, Sarah, Shanti Sumartojo, Deborah Lupton, et al. (2017). “Mundane data: The routines, contingencies and accomplishments of digital living”. En. In: Big Data & Society 4.1. Publisher: SAGE Publications Ltd, p. 2053951717700924. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#march-30-2023",
    "href": "schedule.html#march-30-2023",
    "title": "Schedule",
    "section": "March 30, 2023",
    "text": "March 30, 2023\n\nCooking Data\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Fieldnote 3\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#april-04-2023",
    "href": "schedule.html#april-04-2023",
    "title": "Schedule",
    "section": "April 04, 2023",
    "text": "April 04, 2023\n\nInstitutional Incentives in Data Reporting\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Work on institutional analysis\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#april-06-2023",
    "href": "schedule.html#april-06-2023",
    "title": "Schedule",
    "section": "April 06, 2023",
    "text": "April 06, 2023\n\nEconomies of Data Production: Gifts and Transactions\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Chapter 4 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n\n\n Gerlitz, Carolin and Anne Helmond (2013). “The like economy: Social buttons and the data-intensive web”. En. In: New Media & Society 15.8. Publisher: SAGE Publications, pp. 1348-1365. (Visited on Aug. 30, 2021).\n Beer, David (2015). “Productive measures: Culture and measurement in the context of everyday neoliberalism”. En. In: Big Data & Society 2.1. Publisher: SAGE Publications Ltd, p. 2053951715578951. (Visited on Aug. 29, 2021)."
  },
  {
    "objectID": "schedule.html#april-11-2023",
    "href": "schedule.html#april-11-2023",
    "title": "Schedule",
    "section": "April 11, 2023",
    "text": "April 11, 2023\n\nMobilizing Data: Making Numbers Actionable\nDISCOURSE ANALYSIS SITUATING KNOWLEDGE MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Ottinger, Gwen and Rachel Zurer (2011). New Voices, New Approaches: Drowning in Data. En-US. (Visited on Dec. 13, 2018).\n Work on discourse analysis\n Mini-Project 2\n\n\n Course slides are here.\n Pine, Kathleen H. and Max Liboiron (2015). “The Politics of Measurement and Action”. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery, pp. 3147-3156. ISBN: 978-1-4503-3145-6. (Visited on Aug. 30, 2021).\n Dourish, Paul and Edgar Gómez Cruz (2018). “Datafication and data fiction: Narrating data and narrating with data”. En. In: Big Data & Society 5.2. Publisher: SAGE Publications Ltd, p. 2053951718784083. (Visited on Apr. 05, 2021)."
  },
  {
    "objectID": "schedule.html#april-13-2023",
    "href": "schedule.html#april-13-2023",
    "title": "Schedule",
    "section": "April 13, 2023",
    "text": "April 13, 2023\n\nData Circulation\nDISCOURSE ANALYSIS COMMUNICATING (IN) CONTEXT MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Fieldnote 4\n\n\n Bates, Jo, Yu-Wei Lin, and Paula Goodale (2016). “Data journeys: Capturing the socio-material constitution of data objects and flows”. En. In: Big Data & Society 3.2. Publisher: SAGE Publications Ltd, p. 2053951716654502. (Visited on Mar. 28, 2020).\n Leonelli, Sabina (2010). “Packaging Small Fact for Re-use: Databases in Model Organism Biology”. En. In: How Well Do Facts Travel?: The Dissemination of Reliable Knowledge. Ed. by Peter Howlett and Mary S. Morgan. Cambridge, MA: Cambridge University Press, pp. 325-348. ISBN: 978-1-139-49239-3."
  },
  {
    "objectID": "schedule.html#april-18-2023",
    "href": "schedule.html#april-18-2023",
    "title": "Schedule",
    "section": "April 18, 2023",
    "text": "April 18, 2023\n\nMobilizing Data Otherwise: Citizen Science and Sensing\nDISCOURSE SITUATING KNOWLEDGE MOBILIZATION CREDIBILITY\n\nDue TodayFurther Resources\n\n\n Gabrys, Jennifer, Helen Pritchard, and Benjamin Barratt (2016). “Just good enough data: Figuring data citizenships through air pollution sensing and data stories”. En. In: Big Data & Society 3.2. Publisher: SAGE Publications Ltd, p. 2053951716679677. (Visited on Mar. 28, 2020).\n\n\n Course slides are here.\n Calvillo, Nerea (2018). “Political airs: From monitoring to attuned sensing air pollution”. En. In: Social Studies of Science 48.3, pp. 372-388. (Visited on Sep. 24, 2019).\n Jalbert, Kirk and Abby J. Kinchy (2016). “Sense and Influence: Environmental Monitoring Tools and the Power of Citizen Science”. In: Journal of Environmental Policy & Planning 18.3. Publisher: Routledge _ eprint: https://doi.org/10.1080/1523908X.2015.1100985, pp. 379-397. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#april-20-2023",
    "href": "schedule.html#april-20-2023",
    "title": "Schedule",
    "section": "April 20, 2023",
    "text": "April 20, 2023\n\nData Activism and Advocacy\nSITUATING KNOWLEDGE MOBILIZATION CREDIBILITY\n\nDue TodayFurther Resources\n\n\n Liboiron, Max (2015). “Disaster Data, Data Activism : Grassroots Responses to Representing Superstorm Sandy”. En. In: Extreme Weather and Global Media. Ed. by Julia Leyda and Diane Negra. Taylor & Francis Group. (Visited on Aug. 27, 2019).\n\n\n Bruno, Isabelle, Emmanuel Didier, and Tommaso Vitale (2014). Statactivism: Forms of Action between Disclosure and Affirmation. En. SSRN Scholarly Paper ID 2466882. Rochester, NY: Social Science Research Network. (Visited on Dec. 18, 2018).\n Milan, Stefania and Lonneke van der Velden (2016). “The Alternative Epistemologies of Data Activism”. In: Digital Culture & Society 2.2, pp. 57-74. (Visited on Jul. 16, 2019).\n Currie, Morgan, Britt S Paris, Irene Pasquetto, et al. (2016). “The conundrum of police officer-involved homicides: Counter-data in Los Angeles County”. En. In: Big Data & Society 3.2, p. 2053951716663566. (Visited on Aug. 08, 2018)."
  },
  {
    "objectID": "schedule.html#april-25-2023",
    "href": "schedule.html#april-25-2023",
    "title": "Schedule",
    "section": "April 25, 2023",
    "text": "April 25, 2023\n\nData Agnotology: Ignorance and Knowledge Gaps\nEPISTEMOLOGY IGNORANCE SITUATING KNOWLEDGE\n\nDue TodayFurther Resources\n\n\n mimimimimi (2021). On Missing Data Sets. original-date: 2016-02-03T16:30:28Z. (Visited on Aug. 20, 2021).\n Milan, Stefania and Emiliano Treré (2020). “The Rise of the Data Poor: The COVID-19 Pandemic Seen From the Margins”. En. In: Social Media + Society 6.3. Publisher: SAGE Publications Ltd, p. 2056305120948233. (Visited on Aug. 31, 2021).\n First draft due\n\n\n Course slides are here\n D’Ignazio, Catherine and Lauren F. Klein (2020). Data Feminism. Cambridge, Massachusetts: The MIT Press. ISBN: 978-0-262-04400-4."
  },
  {
    "objectID": "schedule.html#april-27-2023",
    "href": "schedule.html#april-27-2023",
    "title": "Schedule",
    "section": "April 27, 2023",
    "text": "April 27, 2023\n\nData and Algorithmic Power\nPOWER SITUATING KNOWLEDGE EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n Eubanks, Virginia (2018). “A Child Abuse Prediction Model Fails Poor Families”. In: Wired. (Visited on Mar. 28, 2019).\n Fieldnote 5\n\n\n Brayne, Sarah (2017). “Big Data Surveillance: The Case of Policing”. In: American Sociological Review 82.5. Publisher: SAGE Publications Inc, pp. 977-1008. (Visited on Aug. 18, 2021).\n Christin, Angèle (2020). “The ethnographer and the algorithm: beyond the black box”. En. In: Theory and Society 49.5, pp. 897-918. (Visited on Aug. 31, 2021).\n Seaver, Nick (2017). “Algorithms as culture: Some tactics for the ethnography of algorithmic systems”. En. In: Big Data & Society 4.2. Publisher: SAGE Publications Ltd, p. 2053951717738104. (Visited on Jan. 22, 2021)."
  },
  {
    "objectID": "schedule.html#may-02-2023",
    "href": "schedule.html#may-02-2023",
    "title": "Schedule",
    "section": "May 02, 2023",
    "text": "May 02, 2023\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Mini-Project Revisions"
  },
  {
    "objectID": "schedule.html#may-04-2023",
    "href": "schedule.html#may-04-2023",
    "title": "Schedule",
    "section": "May 04, 2023",
    "text": "May 04, 2023\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Final project due\n Enrichment"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#feminist-epistemologies",
    "href": "slides/Day3-BigDataDiscourse.html#feminist-epistemologies",
    "title": "Day Three: Big Data Discourse",
    "section": "Feminist Epistemologies",
    "text": "Feminist Epistemologies\n\nAll knowledge is embodied\n\nContrast with disembodied knowledge - i.e. not tied to a specific body\n\nBodies are situated in certain social positions and have a finite point of view (Haraway 1991)\n\nCritique of the “unmarked body,” the “God trick,” or the “view from nowhere”\n\nKnowledge is tied to particular standpoints\n\nOur experiences, what we’ve read, our education, our social positions, and what our bodies enable us to do, see, hear, taste, touch, and smell\nFactors are innumerable and unique to every person"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#announcements",
    "href": "slides/Day4-BinaryOppositions.html#announcements",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "",
    "text": "Please merge your branches into main!\nBe sure not to close the Feedback pull request.\nBe sure to fill out CATME survey for next week."
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day4-BinaryOppositions.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhat data discourses are the words you wrote on the left side embedded within?\nCan you identify any terms that might fit in between these opposites?"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#binary-oppositions",
    "href": "slides/Day4-BinaryOppositions.html#binary-oppositions",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "Binary Oppositions",
    "text": "Binary Oppositions\n\nLooking at the world through pairs of terms that we consider to have the opposite meaning\nExamples include:\n\nReal/fake\nObjective/subjective\nNature/culture\n\nBinary oppositions are reductionist, or oversimplify complexity\nBinary oppositions are rooted in ideologies and disseminated through discourse"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#hierarchies-in-binary-oppositions",
    "href": "slides/Day4-BinaryOppositions.html#hierarchies-in-binary-oppositions",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "Hierarchies in Binary Oppositions",
    "text": "Hierarchies in Binary Oppositions\n\nIn dominant discourse, one half of a binary opposition tends to be positioned as superior than the other\nOne half tends to get treated as normal or pure, and other as a deviation from the normal, or tainted\nBinary oppositions can reinforce privilege\nWhat are some examples of some hiearchical binary oppositions?"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#natureculture",
    "href": "slides/Day4-BinaryOppositions.html#natureculture",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "Nature/Culture",
    "text": "Nature/Culture\n\nCountless domains (disciplines, newspaper headings, etc.) organized around the divisions between nature and culture\nNature is often associated with purity, innateness, biology, or rawness.\nCulture is seen as ‘Other’ to what is natural\n\ne.g. human judgments bias science and decision-making\ne.g. human cultures destroy the Earth’s purity\n\nFeminist critiques:\n\nShows how purity is political\nArgues that we can’t tell where nature stops and culture starts\nShows how the divisions justify treating certain social groups as superior and others as inferior\nRefers to natureculture: hybrids reverse the logic of binary oppositions"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#activity",
    "href": "slides/Day4-BinaryOppositions.html#activity",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#pull-out-a-piece-of-paper-and-draw-a-line-down-the-center.-on-the-left-side-list-adjectives-that-people-use-to-describe-good-data.-on-the-right-side-write-the-opposite-of-each-word-you-wrote-on-the-left-side.",
    "href": "slides/Day4-BinaryOppositions.html#pull-out-a-piece-of-paper-and-draw-a-line-down-the-center.-on-the-left-side-list-adjectives-that-people-use-to-describe-good-data.-on-the-right-side-write-the-opposite-of-each-word-you-wrote-on-the-left-side.",
    "title": "Day Four: Binary Oppositions of Big Data",
    "section": "Pull out a piece of paper, and draw a line down the center. On the left side, list adjectives that people use to describe “good” data. On the right side, write the opposite of each word you wrote on the left side.",
    "text": "Pull out a piece of paper, and draw a line down the center. On the left side, list adjectives that people use to describe “good” data. On the right side, write the opposite of each word you wrote on the left side."
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#announcements",
    "href": "slides/Day5-ThickDescription.html#announcements",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Announcements",
    "text": "Announcements\n\nYou will be assigned project groups by Tuesday.\nYour first group project - the team contract - will be due the following week.\nDM Rose if you’d like to lead class discussion."
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day5-ThickDescription.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nHow would these research projects look different?\nWhat kind of data would you collect?\nHow would you interpret the data differently?"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#discliplinary-conventions",
    "href": "slides/Day5-ThickDescription.html#discliplinary-conventions",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Discliplinary Conventions",
    "text": "Discliplinary Conventions\n\nWhat is considered a relevant research question?\nWhat kind of arguments are made?\nWhat kind of data is collected/produced/presented?\nHow is collaboration valued?\nWhen/how is data shared?"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#disciplinary-genres",
    "href": "slides/Day5-ThickDescription.html#disciplinary-genres",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Disciplinary Genres",
    "text": "Disciplinary Genres\n\nIs text written in active or passive voice? Personal pronouns?\nAre subheadings chosen based on academic conventions?\nWhat kind of evidence is presented to persuade the reader? Statistics? Anecdotes?\nIn what ways are research methods explicated?\nHow and with what sort of detail are objects of study described?\nWhat role do poetics and metaphor play in the text? Can you identify emotion?\nWhat reflexive moves (self-reflection) does the author make? How do they elaborate their standpoint in/through the text?"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#what-is-ethnography",
    "href": "slides/Day5-ThickDescription.html#what-is-ethnography",
    "title": "Day Five: Thick Data for Big Data",
    "section": "What is ethnography?",
    "text": "What is ethnography?\n\nStudy of human cultures\n\nCulture often defined as “semiotic”\nEthnographers aim to interpret the underlying meanings of surface actions and expressions\nMake the “familiar strange”\n\nOften characterized as interpretivist and qualitative\nTypically involves extended, immersive fieldwork\nHas historically been a solo discipline\nQualitative data rarely shared"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#methods-of-ethnography",
    "href": "slides/Day5-ThickDescription.html#methods-of-ethnography",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Methods of Ethnography",
    "text": "Methods of Ethnography\n\nParticipant Observation\n\nObserving interaction and behaviors “in the field”\n\nInterviewing\n\nEngaging in semi-structured conversations with informants\n\nArchival Research\n\nCurating and interpreting historical documents and artifacts\n\nDiscourse analysis\n\nInterpreting the cultural meaning interwoven in texts and speech"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#tools-of-ethnography",
    "href": "slides/Day5-ThickDescription.html#tools-of-ethnography",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Tools of Ethnography",
    "text": "Tools of Ethnography\n\nField Notebooks\n\nDocument thick descriptions of observations\n\nRecording Devices\n\nAudio and video documentation of interviews\n\nCoding Software\n\nDraw out consistent themes of notes and interviews"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#what-is-thick-description",
    "href": "slides/Day5-ThickDescription.html#what-is-thick-description",
    "title": "Day Five: Thick Data for Big Data",
    "section": "What is thick description?",
    "text": "What is thick description?\n\n\n\n\n\nDescribe in the “thinnest” terms possible what is happening in this image.\nThick description moves towards interpreting its cultural meaning.\nExample drawn from a famous 1973 chapter by Clifford Geertz: “Thick Description: Towards an Interpretive Theory of Culture.”"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#practicing-thick-description",
    "href": "slides/Day5-ThickDescription.html#practicing-thick-description",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Practicing Thick Description",
    "text": "Practicing Thick Description\n Image source: XKCD, https://xkcd.com/1838/\nCreative Commons Attribution-NonCommercial 2.5 License."
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#how-might-we-interpret-the-role-of-hex-stickers-in-the-r-community",
    "href": "slides/Day5-ThickDescription.html#how-might-we-interpret-the-role-of-hex-stickers-in-the-r-community",
    "title": "Day Five: Thick Data for Big Data",
    "section": "How might we interpret the role of hex stickers in the R community?",
    "text": "How might we interpret the role of hex stickers in the R community?\n Image source: Beiers, Sophie. 2020. “Tips & Tricks from the Newbies of Rstudio::Conf2020.” ACLU Tech & Analytics (blog). March 13, 2020. https://medium.com/aclu-tech-analytics/tips-tricks-from-the-newbies-of-rstudio-conf2020-5ccc780ba0e7."
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#fieldnotes-assignment",
    "href": "slides/Day5-ThickDescription.html#fieldnotes-assignment",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Fieldnotes Assignment",
    "text": "Fieldnotes Assignment\n\nSubmitted in your Google Drive for this class in any format you wsih\nMust be &gt;300 words\nMust respond to a prompt on the course website\nMust be about a data environment encountered in the past two weeks\nMust not only describe, but also interpret a data environment"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#mondays-reading",
    "href": "slides/Day5-ThickDescription.html#mondays-reading",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Monday’s Reading",
    "text": "Monday’s Reading\n\nWhat are Biruk’s research questions?\nWhat does Biruk make known about her assumptions/beliefs/values as she begins to interpret data?"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#recommendations",
    "href": "slides/Day5-ThickDescription.html#recommendations",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Recommendations",
    "text": "Recommendations\n\nDiscern something very small - the smaller the better - for your first field note.\nExamples include one headline, a poster, one line of code, etc.\nStick with prompts and concepts that we’ve already learned (e.g. discourse, epistemology).\nRead through the directions on the course website, as well as past examples"
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#imagine-you-were-an-ethnographer-attending-astro-hack-week.-write-a-quantitative-research-question-for-a-study-you-might-conduct.-now-write-a-qualitative-research-question.",
    "href": "slides/Day5-ThickDescription.html#imagine-you-were-an-ethnographer-attending-astro-hack-week.-write-a-quantitative-research-question-for-a-study-you-might-conduct.-now-write-a-qualitative-research-question.",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Imagine you were an ethnographer attending Astro Hack Week. Write a quantitative research question for a study you might conduct. Now write a qualitative research question.",
    "text": "Imagine you were an ethnographer attending Astro Hack Week. Write a quantitative research question for a study you might conduct. Now write a qualitative research question."
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#tuesdays-reading",
    "href": "slides/Day5-ThickDescription.html#tuesdays-reading",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Tuesday’s Reading",
    "text": "Tuesday’s Reading\n\nWhat are Biruk’s research questions?\nWhat does Biruk make known about her assumptions/beliefs/values as she begins to interpret data?"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#imagine-that-you-are-an-ethnographer-attending-astro-hack-week.-pull-out-a-sheet-of-paper-and-jot-your-name-in-the-center.-around-your-name-list-aspects-of-your-cultural-identity-that-would-be-important-to-acknowledge-as-you-studied-this-space.",
    "href": "slides/Day6-Reflexivity.html#imagine-that-you-are-an-ethnographer-attending-astro-hack-week.-pull-out-a-sheet-of-paper-and-jot-your-name-in-the-center.-around-your-name-list-aspects-of-your-cultural-identity-that-would-be-important-to-acknowledge-as-you-studied-this-space.",
    "title": "Day Six: Reflexivity",
    "section": "Imagine that you are an ethnographer attending Astro Hack Week. Pull out a sheet of paper and jot your name in the center. Around your name, list aspects of your cultural identity that would be important to acknowledge as you studied this space.",
    "text": "Imagine that you are an ethnographer attending Astro Hack Week. Pull out a sheet of paper and jot your name in the center. Around your name, list aspects of your cultural identity that would be important to acknowledge as you studied this space."
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day6-Reflexivity.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Six: Reflexivity",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhy would it be important to consider these aspects of your cultural identity as you engaged in ethnographic fieldwork?"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#critiques-and-criticisms-of-ethnographic-research",
    "href": "slides/Day6-Reflexivity.html#critiques-and-criticisms-of-ethnographic-research",
    "title": "Day Six: Reflexivity",
    "section": "Critiques and Criticisms of Ethnographic Research",
    "text": "Critiques and Criticisms of Ethnographic Research\n\nEthnographic research (like many other forms of research) has been historically complicit in imperialism and colonialism\nMethods emerged as part of European colonial efforts to document folks “Native” to “other” lands - often to enable control and exploitation of those cultures\nMany binary oppositions indicate how power operated here:\n\nStudier/studied\nResearcher/researched\n\nConcerns over who gets to “write culture”"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#reflexive-turn",
    "href": "slides/Day6-Reflexivity.html#reflexive-turn",
    "title": "Day Six: Reflexivity",
    "section": "Reflexive Turn",
    "text": "Reflexive Turn\n\nEmerged in the 1970s as a result of feminist and post-colonial critiques\n\nFeminist: Where do we stand? Can we ever observe from an objective or neutral place?\nPost-colonial: What do we consider “Other,” and how do we portray what is “Other”?\n\nCalled on ethnographers to engage in self-reflection (standpoints, assumptions, etc.)\nCan we write another culture “objectively” when our own biases and epistemologies, and social capital are inevitably involved in the research?"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#aka-the-literary-turn",
    "href": "slides/Day6-Reflexivity.html#aka-the-literary-turn",
    "title": "Day Six: Reflexivity",
    "section": "…aka the Literary Turn",
    "text": "…aka the Literary Turn\n\nEthnography was often understood to be about writing (documenting observations and interpreting them)\nBegan to pay attention to power in the language used to “write culture”\n\nBinary oppositions\nGenre and representations of “fact”\n\nCalled for making ethnographic writing more polyphonous (or represent a plurality of voices)\nSometimes invited “poetics” and “experimentalism”\n\n\nIn other words, there are political reasons as to why the fieldnotes that you read felt different than other scientific genres!"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#reflexivity-for-each-of-our-methods",
    "href": "slides/Day6-Reflexivity.html#reflexivity-for-each-of-our-methods",
    "title": "Day Six: Reflexivity",
    "section": "Reflexivity for Each of our Methods",
    "text": "Reflexivity for Each of our Methods\n\nParticipant Observation\n\nObserving interaction and behaviors “in the field”\n\nInterviewing\n\nEngaging in semi-structured conversations with informants\n\nArchival Research\n\nCurating and interpreting historical documents and artifacts\n\nDiscourse analysis\n\nInterpreting the cultural meaning interwoven in texts and speech"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#infrastructure-overview",
    "href": "slides/Day6-Reflexivity.html#infrastructure-overview",
    "title": "Day Six: Reflexivity",
    "section": "Infrastructure Overview",
    "text": "Infrastructure Overview\n\nGroup project\nFirst fieldnote\nReview of labor log"
  },
  {
    "objectID": "slides/Day7-Looping.html#announcements",
    "href": "slides/Day7-Looping.html#announcements",
    "title": "Day Seven: Data Looping Effects",
    "section": "",
    "text": "Office hours for help with labor log.\nGroup contract and first field note due next Tuesday."
  },
  {
    "objectID": "slides/Day7-Looping.html#think-about-a-category-or-label-that-institutions-assign-to-a-group-of-people.-free-write-for-about-2-minutes-on-the-following-prompt-how-does-the-existence-of-this-category-or-label-impact-certain-social-groups",
    "href": "slides/Day7-Looping.html#think-about-a-category-or-label-that-institutions-assign-to-a-group-of-people.-free-write-for-about-2-minutes-on-the-following-prompt-how-does-the-existence-of-this-category-or-label-impact-certain-social-groups",
    "title": "Day Seven: Data Looping Effects",
    "section": "Think about a category or label that institutions assign to a group of people. Free-write for about 2 minutes on the following prompt: How does the existence of this category or label impact certain social groups?",
    "text": "Think about a category or label that institutions assign to a group of people. Free-write for about 2 minutes on the following prompt: How does the existence of this category or label impact certain social groups?"
  },
  {
    "objectID": "slides/Day7-Looping.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day7-Looping.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Seven: Data Looping Effects",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhy does this category or label exist?\nIn what ways has this category or label been normalized in society?\nWhat happens when folks don’t agree with the assignment? Can they contest it?"
  },
  {
    "objectID": "slides/Day7-Looping.html#assemblages",
    "href": "slides/Day7-Looping.html#assemblages",
    "title": "Day Seven: Data Looping Effects",
    "section": "Assemblages",
    "text": "Assemblages\n\nContests the idea that data are hard, shiny objects that can be held, acquired, etc.\nWhen we talk about data, we are actually referring to many interrelated things:\n\nMaterialities, practices, subjectivities, places, governmentalities, etc.\nSee table in this week’s reading\n\n“Data” emerges in the wake of these interrelated assemblages"
  },
  {
    "objectID": "slides/Day7-Looping.html#hackings-looping-effect",
    "href": "slides/Day7-Looping.html#hackings-looping-effect",
    "title": "Day Seven: Data Looping Effects",
    "section": "Hacking’s Looping Effect",
    "text": "Hacking’s Looping Effect\n &gt; Tekin, Serife. 2014. “The Missing Self in Hacking’s Looping Effect.” In Classifying Psychopathology: Mental Kinds and Natural Kinds, edited by Harold Kincaid and Jacqueline A. Sullivan. MIT Press."
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#ws-of-metadata",
    "href": "slides/Day9-DataDocumentation.html#ws-of-metadata",
    "title": "Day Nine: Data Documentation",
    "section": "5 W’s of Metadata",
    "text": "5 W’s of Metadata"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#example-library-catalog",
    "href": "slides/Day9-DataDocumentation.html#example-library-catalog",
    "title": "Day Nine: Data Documentation",
    "section": "Example: Library Catalog",
    "text": "Example: Library Catalog"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#metadata-schemas",
    "href": "slides/Day9-DataDocumentation.html#metadata-schemas",
    "title": "Day Nine: Data Documentation",
    "section": "Metadata Schemas",
    "text": "Metadata Schemas\n\nA standardized labeling system for cataloging or describing data\nEnables search engines to index data by certain criteria\nExamples:\n\nSort by “date created”\nRetrieve all results from a specific “author/creator”\nFilter results to a specific “subject”\nExclude results from a specific “publisher”"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#example-citation-manager",
    "href": "slides/Day9-DataDocumentation.html#example-citation-manager",
    "title": "Day Nine: Data Documentation",
    "section": "Example: Citation Manager",
    "text": "Example: Citation Manager"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#whats-the-difference-between-administrative-and-descriptive-metadata",
    "href": "slides/Day9-DataDocumentation.html#whats-the-difference-between-administrative-and-descriptive-metadata",
    "title": "Day Nine: Data Documentation",
    "section": "What’s the difference between administrative and descriptive metadata?",
    "text": "What’s the difference between administrative and descriptive metadata?"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#data-dictionaries",
    "href": "slides/Day9-DataDocumentation.html#data-dictionaries",
    "title": "Day Nine: Data Documentation",
    "section": "Data Dictionaries",
    "text": "Data Dictionaries\n\nDocuments for holding descriptive metadata\nDefine the variables in a dataset and the values that may fill in those variables\nAre not always as descriptive as we’d like them to be"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#example-nyc-metadata-for-all",
    "href": "slides/Day9-DataDocumentation.html#example-nyc-metadata-for-all",
    "title": "Day Nine: Data Documentation",
    "section": "Example: NYC Metadata for All",
    "text": "Example: NYC Metadata for All"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#for-monday",
    "href": "slides/Day9-DataDocumentation.html#for-monday",
    "title": "Day Nine: Data Documentation",
    "section": "For Monday",
    "text": "For Monday\n\nQuestions to consider:\n\nWhat does Biruk mean when they refer to “translation” in this chapter?\nWhere do we see looping effects in this chapter? What gets “lost in translation”?"
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html#free-write-for-the-next-2-minutes-youve-submitted-your-first-assignment---both-individual-and-group.-what-have-you-learned-so-far-thats-sticking-with-you-the-most-what-questions-do-you-have-about-course-infrastructure-what-challenges-have-you-run-into",
    "href": "slides/Day8-ResearchEthics.html#free-write-for-the-next-2-minutes-youve-submitted-your-first-assignment---both-individual-and-group.-what-have-you-learned-so-far-thats-sticking-with-you-the-most-what-questions-do-you-have-about-course-infrastructure-what-challenges-have-you-run-into",
    "title": "Day Eight: Research Ethics",
    "section": "Free-write for the next 2 minutes: You’ve submitted your first assignment - both individual and group. What have you learned so far that’s sticking with you the most? What questions do you have about course infrastructure? What challenges have you run into?",
    "text": "Free-write for the next 2 minutes: You’ve submitted your first assignment - both individual and group. What have you learned so far that’s sticking with you the most? What questions do you have about course infrastructure? What challenges have you run into?"
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html#i-encourage-you-to-come-to-office-hours-if",
    "href": "slides/Day8-ResearchEthics.html#i-encourage-you-to-come-to-office-hours-if",
    "title": "Day Eight: Research Ethics",
    "section": "I encourage you to come to office hours if…",
    "text": "I encourage you to come to office hours if…\n\nYou’d like to discuss strategies for managing the reading/workload.\nYou’d like clarifications on any of the assignment expectations/course infrastructure.\nYou’d like to review certain concepts discussed in lecture or the readings.\nYou’d like to chat about opportunities for exploring more about data ethnography.\nYou’re stressed and need some positive affirmation."
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html#common-rule",
    "href": "slides/Day8-ResearchEthics.html#common-rule",
    "title": "Day Eight: Research Ethics",
    "section": "Common Rule",
    "text": "Common Rule\n\nIn the US, research organizations receiving federal funding are subject to the Common Rule\n\nLaws and regulations regarding how Institutional Review Boards are to operate\nIRBs are organizational bodies that review the ethics of human subjects research in order to protect human welfare, rights, and privacy before a study gets carried out\n\nUsually composed of representatives from an institution with a diverse background"
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html#tuskegee-study",
    "href": "slides/Day8-ResearchEthics.html#tuskegee-study",
    "title": "Day Eight: Research Ethics",
    "section": "Tuskegee Study",
    "text": "Tuskegee Study\n\nConducted from 1932 to 1972 by US Public Health Services and Center for Disease Control, in collaboration with Tuskegee University in Alabama\nInvolved 400 African Americans with syphilis\nStudy of leaving the disease untreated even though it was treatable\n\nWere told they would receive free medical care\nNever informed of their diagnosis\nProvided with placebos and ineffective methods"
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html#facebooks-emotional-contagion-study",
    "href": "slides/Day8-ResearchEthics.html#facebooks-emotional-contagion-study",
    "title": "Day Eight: Research Ethics",
    "section": "Facebook’s Emotional Contagion Study",
    "text": "Facebook’s Emotional Contagion Study\n\nJanuary 2012, Facebook data scientists manipulated what 700,000 users saw on feeds to examine emotional contagion\n\nSome shown happy, positive content\nOthers shown sad, negative content\n\nLegal?\nEthical?"
  },
  {
    "objectID": "slides/Day8-ResearchEthics.html#ethics-of-ethnographic-research",
    "href": "slides/Day8-ResearchEthics.html#ethics-of-ethnographic-research",
    "title": "Day Eight: Research Ethics",
    "section": "Ethics of ethnographic research",
    "text": "Ethics of ethnographic research\n\nEthnography different than many other forms of human subjects research in that it takes place in natural settings vs in clinical settings\nQuestion of how to balance benefits of the research with the potential harms posed to the participants\nPotential harms:\n\nReputation\nDisclosure of personal information\nDisruption of relationships\nLoss of claims"
  },
  {
    "objectID": "slides/Day10-Translation.html#take-out-a-sheet-of-paper-and-write-happiness-in-the-middle.-around-that-word-list-ways-that-we-might-quantitatively-measure-happiness-so-that-we-might-rank-countries-by-their-happiness.-be-sure-your-ideas-would-produce-a-number.",
    "href": "slides/Day10-Translation.html#take-out-a-sheet-of-paper-and-write-happiness-in-the-middle.-around-that-word-list-ways-that-we-might-quantitatively-measure-happiness-so-that-we-might-rank-countries-by-their-happiness.-be-sure-your-ideas-would-produce-a-number.",
    "title": "Day Ten: Translation",
    "section": "Take out a sheet of paper and write happiness in the middle. Around that word, list ways that we might quantitatively measure happiness so that we might rank countries by their happiness. Be sure your ideas would produce a number.",
    "text": "Take out a sheet of paper and write happiness in the middle. Around that word, list ways that we might quantitatively measure happiness so that we might rank countries by their happiness. Be sure your ideas would produce a number."
  },
  {
    "objectID": "slides/Day10-Translation.html#cantril-ladder",
    "href": "slides/Day10-Translation.html#cantril-ladder",
    "title": "Day Ten: Translation",
    "section": "Cantril Ladder",
    "text": "Cantril Ladder\n\nThink of a ladder, with the best possible life for them being a 10, and the worst possible life being a 0.\nRate current lives on that 0 to 10 scale.\nSub-bars: levels of GDP, life expectancy, generosity, social support, freedom, and corruption\n\nNo impact on final score, only explanatory"
  },
  {
    "objectID": "slides/Day10-Translation.html#example",
    "href": "slides/Day10-Translation.html#example",
    "title": "Day Ten: Translation",
    "section": "Example",
    "text": "Example\n\nResearch question: How do on-the-ground dynamics and practices of survey research cultures mediate the production of numbers?\nPg. 48 “Are you comfortable walking to the market alone?”\nWhat were the responses?\nHow would you interpret what Biruk is seeing here? What’s the significance of this example?"
  },
  {
    "objectID": "slides/Day10-Translation.html#example-1",
    "href": "slides/Day10-Translation.html#example-1",
    "title": "Day Ten: Translation",
    "section": "Example",
    "text": "Example\n\nResearch question: How do on-the-ground dynamics and practices of survey research cultures mediate the production of numbers?\nPg. 61: Question D13\nWhat does Biruk describe as the “fetish for codes?”\nHow would you interpret what Biruk is seeing here?\nWhat’s the significance of this example?"
  },
  {
    "objectID": "slides/Day10-Translation.html#example-2",
    "href": "slides/Day10-Translation.html#example-2",
    "title": "Day Ten: Translation",
    "section": "Example",
    "text": "Example\n\nResearch question: How do on-the-ground dynamics and practices of survey research cultures mediate the production of numbers?\nWhy does Biruk emphasize how stakeholders perceive “the field”? How would you interpret what Biruk is seeing here? What’s the significance of this?"
  },
  {
    "objectID": "slides/Day10-Translation.html#global-multidimensional-poverty-index",
    "href": "slides/Day10-Translation.html#global-multidimensional-poverty-index",
    "title": "Day Ten: Translation",
    "section": "Global Multidimensional Poverty Index",
    "text": "Global Multidimensional Poverty Index\n\n\n\n\n\nUNDP Human Development Report\n\n\n\n\nIndex that measures degrees of poverty in developing countries\nComplements monetary poverty measures by capturing other dimensions of poverty"
  },
  {
    "objectID": "slides/Day10-Translation.html#activity",
    "href": "slides/Day10-Translation.html#activity",
    "title": "Day Ten: Translation",
    "section": "Activity",
    "text": "Activity\n\nWhat is the purpose of this measure?\nWhat is the definition of poverty according to this standard?\nThrough what means did this definition get established?\nWhat assumptions are built into this definition?\nWhat might get lost in translation?"
  },
  {
    "objectID": "slides/Day10-Translation.html#discussion-questions",
    "href": "slides/Day10-Translation.html#discussion-questions",
    "title": "Day Ten: Translation",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhy is pain difficult to quantify?\nWhat is the definition of pain in this standard?\nHow did the folks devising this standard attempt to make the measures commensurate?\nWhat might get lost in translation?"
  },
  {
    "objectID": "slides/Day10-Translation.html#analyzing-poverty-definitions-activity-if-time",
    "href": "slides/Day10-Translation.html#analyzing-poverty-definitions-activity-if-time",
    "title": "Day Ten: Translation",
    "section": "Analyzing Poverty Definitions Activity (if time)",
    "text": "Analyzing Poverty Definitions Activity (if time)\n\nWhat is the purpose of this measure?\nWhat is the definition of poverty according to this standard?\nThrough what means did this definition get established?\nWhat assumptions are built into this definition?\nWhat might get lost in translation?"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day11-Infrastructure.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Eight: Ethnographies of Infrastructure",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\nPick one supporting infrastructure:\n\nHow often do you think about this infrastructure?\nIs it always an enabling infrastructure? When might it not be?"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#stars-definition-of-infrastructure",
    "href": "slides/Day11-Infrastructure.html#stars-definition-of-infrastructure",
    "title": "Day Eight: Ethnographies of Infrastructure",
    "section": "Star’s Definition of Infrastructure",
    "text": "Star’s Definition of Infrastructure\n\nEmbeddedness: Sunk into other socio-technical structures\nTransparency: Does not need to continuously be reassembled\nReach or Scope: Connects more than one site\nLearned as part of membership: Use naturalized through communities of practice\nEmbodiment of standards: Component parts can connect through standardized conventions\nBuilt on an installed base: Wrestles with the logics of its underlying infrastructures\nVisible upon breakdown: Often taken for granted until breaking"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#identify-something-that-meets-stars-definition-of-an-infrastructure.-jot-down-some-ideas-about-howwhen-this-infrastructure-has-broken-down.",
    "href": "slides/Day11-Infrastructure.html#identify-something-that-meets-stars-definition-of-an-infrastructure.-jot-down-some-ideas-about-howwhen-this-infrastructure-has-broken-down.",
    "title": "Day Eleven: Ethnographies of Infrastructure",
    "section": "Identify something that meets Star’s definition of an infrastructure. Jot down some ideas about how/when this infrastructure has broken down.",
    "text": "Identify something that meets Star’s definition of an infrastructure. Jot down some ideas about how/when this infrastructure has broken down."
  },
  {
    "objectID": "slides/Day12-Classification.html#identify-a-classification-system-that-structures-some-of-your-work.",
    "href": "slides/Day12-Classification.html#identify-a-classification-system-that-structures-some-of-your-work.",
    "title": "Day Twelve: Classification",
    "section": "Identify a classification system that structures some of your work.",
    "text": "Identify a classification system that structures some of your work."
  },
  {
    "objectID": "slides/Day12-Classification.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day12-Classification.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Nine: Classification",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhat is the master narrative running through this classification system?"
  },
  {
    "objectID": "slides/Day12-Classification.html#identify-a-classification-system-that-structures-some-aspect-of-your-work.",
    "href": "slides/Day12-Classification.html#identify-a-classification-system-that-structures-some-aspect-of-your-work.",
    "title": "Day Twelve: Classification",
    "section": "Identify a classification system that structures some aspect of your “work”.",
    "text": "Identify a classification system that structures some aspect of your “work”."
  },
  {
    "objectID": "slides/Day12-Classification.html#identify-a-classification-system-that-structures-some-aspect-of-your-work.-how-are-concepts-organized-in-this-classification-system",
    "href": "slides/Day12-Classification.html#identify-a-classification-system-that-structures-some-aspect-of-your-work.-how-are-concepts-organized-in-this-classification-system",
    "title": "Day Nine: Classification",
    "section": "Identify a classification system that structures some aspect of your “work”. How are concepts organized in this classification system?",
    "text": "Identify a classification system that structures some aspect of your “work”. How are concepts organized in this classification system?"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#reminders",
    "href": "slides/Day13-Interviewing.html#reminders",
    "title": "Day Thirteen: Labor and Interviewing",
    "section": "Reminders",
    "text": "Reminders\n\nFirst MP due Thursday\n\nToday is the last day to request an extension\nVideo instructions in Perusall\nExemplary MPs in Slack\n\nReading for Thursday is optional (We won’t have time to discuss it in class)"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#when-do-ethnographers-interview",
    "href": "slides/Day13-Interviewing.html#when-do-ethnographers-interview",
    "title": "Day Ten: Labor and Interviewing",
    "section": "When do ethnographers interview…",
    "text": "When do ethnographers interview…\n\nTo gather historical narratives based on insider knowledge\nTo gather detail about how people narrate their activities/work\nTo ascertain individual assumptions and commitments\nTo deepen understanding of how people see themselves fitting into their social worlds and how they communicate that"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#things-to-keep-in-mind",
    "href": "slides/Day13-Interviewing.html#things-to-keep-in-mind",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Things to keep in mind…",
    "text": "Things to keep in mind…\n\nInterviews are co-constructed by the interviewer and the interviewee\n\n\nWhat does that mean? Why does it matter?\n\n\nInterviews are not aiming to get at the “truth” of events but to capture how people narrate them\n\n\nWhat does that mean? Why does it matter?"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#qualitative-interviews",
    "href": "slides/Day13-Interviewing.html#qualitative-interviews",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Qualitative Interviews",
    "text": "Qualitative Interviews\n\nInvolve open-ended questions seeking in-depth explanations and articulations\nAre often semi-structured\n\nEnables researchers to follow-up, asking the hows and the whys\nAllows the interviewee the flexibility to communicate from their perspective"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#preparing-for-an-interview",
    "href": "slides/Day13-Interviewing.html#preparing-for-an-interview",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Preparing for an interview",
    "text": "Preparing for an interview\n\nIdentify an individual that can provide unique perspective\nReach out to that individual requesting an interview\nCoordinate a date, time, and location for the interview. If conducting through Zoom, create a shared meeting link. Note how location matters for the tone of the interview.\nAsk interviewee to sign informed consent form\nConduct background research on the individual being interviewed and the institutions they are a part of.\nCreate an interview guide - with framing questions and transitions."
  },
  {
    "objectID": "slides/Day13-Interviewing.html#interview-guides",
    "href": "slides/Day13-Interviewing.html#interview-guides",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Interview Guides",
    "text": "Interview Guides\n\nGeneral flow:\n\nStart with more general question and move to more specific questions\nStart with more matter-of-fact questions and move to more intimate questions as you build trust\nClose with a few lighter questions that prepare for friendly conclusion\n\nPreparing the guide:\n\nWrite more interview questions than you will have time to get to.\nGroup questions according to similar topics and themes.\nPrepare transitions for moving between topics."
  },
  {
    "objectID": "slides/Day13-Interviewing.html#framing-questions",
    "href": "slides/Day13-Interviewing.html#framing-questions",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Framing Questions",
    "text": "Framing Questions\n\nAsk questions that encourage elaboration and description.\nConsider how to frame questions so that they make sense to the interviewee:\n\nMy research question: What values inform your data work?\nMy interview questions:\n\nWhat motivated you get involved in this work?\nWho were your primary inspirations in this area and why?\nWhat theorists/books were you reading as you got involved in this work? Do you find that relevant to your work today?"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#during-the-interview",
    "href": "slides/Day13-Interviewing.html#during-the-interview",
    "title": "Day Ten: Labor and Interviewing",
    "section": "During the Interview",
    "text": "During the Interview\n\nThank the individual for joining, introduce yourself, and remind them of the purpose of the interview\nStart the recording, and state your name, the date/time, the location, and the person being interviewed\nConfirm on the recording that the interviewee agrees to be interviewed\nDraw questions from the interview guide that maintain the overall flow of the interview. Regularly ask interviewees to elaborate by asking “how?” and “why?” questions.\nTakes notes on things the recording can’t pick up on (e.g. Body language, Tone, Things you are reminded of as they’re speaking)"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#engaging-during-the-interview",
    "href": "slides/Day13-Interviewing.html#engaging-during-the-interview",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Engaging during the Interview",
    "text": "Engaging during the Interview\n\nEngage in active listening (direct eye contact, nodding)\nDon’t rush the interviewee to move onto a new question.\nIf the conversation veers allow it momentarily and then politely redirect it with a statement like, “I’d like to return to what you were saying about… Can you describe…?”\nPreface difficult conversations, and offer breaks if appropriate.\nDon’t try and finish an interviewee’s sentences.\nAvoid explicitly sharing personal opinions on what they’ve said.\nContemplate ways to reframe questions that an interviewee evades, exaggerates, or provides limited information on."
  },
  {
    "objectID": "slides/Day13-Interviewing.html#questions-to-consider",
    "href": "slides/Day13-Interviewing.html#questions-to-consider",
    "title": "Day Thirteen: Labor and Interviewing",
    "section": "Questions to consider:",
    "text": "Questions to consider:\n\nWhy does Biruk refer to the fieldworkers as “knowledge workers”? What kind of ethnographic move is this?\nBiruk continuously refers to “boundary work” in this chapter? Can you extrapolate what she means by this? What are the effects of enacting boundary work in fieldwork?\nWhat is “local knowledge”? How do credibility contests around “local knowledge” play out in this chapter?"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#reminders",
    "href": "slides/Day15-ParticipantObservation.html#reminders",
    "title": "Day Fifteen: Participant Observation",
    "section": "Reminders",
    "text": "Reminders\n\nFieldnote 3 due Thursday\nMP 2 due April 11\nNow is the time to start thinking about an interview"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#what-is-participant-observation",
    "href": "slides/Day15-ParticipantObservation.html#what-is-participant-observation",
    "title": "Day Twelve: Participant Observation",
    "section": "What is participant observation?",
    "text": "What is participant observation?\n\nAn ethnographic method that involves embedding oneself in a community’s day-to-day activities\nEthnographer observes how people behave and interact through participation\nEthnographer takes written notes on what they observe which are later elaborated"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#when-do-ethnographers-do-participant-observation",
    "href": "slides/Day15-ParticipantObservation.html#when-do-ethnographers-do-participant-observation",
    "title": "Day Twelve: Participant Observation",
    "section": "When do ethnographers do participant observation?",
    "text": "When do ethnographers do participant observation?\n\nTo figure out what kinds of interview questions to ask\nTo place checks on what people say during interviews\nTo gather details about the physical and social environments in which individuals engage\nTo deepen the ethnographer’s understanding of social contexts"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#what-gets-recorded-in-participant-observation",
    "href": "slides/Day15-ParticipantObservation.html#what-gets-recorded-in-participant-observation",
    "title": "Day Twelve: Participant Observation",
    "section": "What gets recorded in participant observation?",
    "text": "What gets recorded in participant observation?\n\nDetails about:\n\nThe physical setting\nThe quantity and demographics of participants\nTheir physical appearance (such as attire)\nTheir traffic\nThe tools they use and/or exchange\nTheir physical movements\nThe way they communicate with one another\n…and more."
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#ethical-guidelines",
    "href": "slides/Day15-ParticipantObservation.html#ethical-guidelines",
    "title": "Day Twelve: Participant Observation",
    "section": "Ethical Guidelines",
    "text": "Ethical Guidelines\n\nWhen should you announce yourself?\nDo you need informed consent?\nWhen is it advisable to identify participants?\nHow should I behave in the space?"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#tips-for-taking-field-notes",
    "href": "slides/Day15-ParticipantObservation.html#tips-for-taking-field-notes",
    "title": "Day Twelve: Participant Observation",
    "section": "Tips for Taking Field Notes",
    "text": "Tips for Taking Field Notes\n\nRecord the date, time, and place of data collection\nRecord as much detail as you can about what you observe\nLeave space in notes to expand later\nUse abbreviations and shorthand when possible\nCreate maps and graphics to help you remember details"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#expanding-field-notes",
    "href": "slides/Day15-ParticipantObservation.html#expanding-field-notes",
    "title": "Day Twelve: Participant Observation",
    "section": "Expanding Field Notes",
    "text": "Expanding Field Notes\n\nSchedule a time to do it shortly after observation\nConvert your notes into a descriptive narrative\nLayer interpretations of your observations"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#think-about-a-data-collection-practice-that-you-are-acquainted-with.-identify-one-specific-thing-that-people-try-to-always-do-when-collecting-that-data.",
    "href": "slides/Day15-ParticipantObservation.html#think-about-a-data-collection-practice-that-you-are-acquainted-with.-identify-one-specific-thing-that-people-try-to-always-do-when-collecting-that-data.",
    "title": "Day Fifteen: Participant Observation",
    "section": "Think about a data collection practice that you are acquainted with. Identify one specific thing that people try to always do when collecting that data.",
    "text": "Think about a data collection practice that you are acquainted with. Identify one specific thing that people try to always do when collecting that data."
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day15-ParticipantObservation.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Fifteen: Participant Observation",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhy do people try to do this?\nWhat meaning does it have for the data?\nHow does it shape the environments/social worlds for the people that engage in this “ritual”?"
  },
  {
    "objectID": "slides/Day16-Rituals.html#reminders",
    "href": "slides/Day16-Rituals.html#reminders",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Reminders",
    "text": "Reminders\n\nNow is the time to start thinking about MP 2!\nYou must submit your MP 1 on Moodle by 5 PM tomorrow if you wish to participate in peer review."
  },
  {
    "objectID": "slides/Day16-Rituals.html#discussion",
    "href": "slides/Day16-Rituals.html#discussion",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Discussion",
    "text": "Discussion\n\nWhat did our ethnographers notice about the data collection? What questions did our data collectors have about how to measure the height of a student?\nWhat were our shared goals and beliefs about what would make “good data”?\nHow did the rituals we designed shape the data? How did we organize ourselves to create pathways for the data to be comparable?"
  },
  {
    "objectID": "slides/Day16-Rituals.html#reading-review-what-are-rituals",
    "href": "slides/Day16-Rituals.html#reading-review-what-are-rituals",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Reading Review: What are rituals?",
    "text": "Reading Review: What are rituals?\n\nStylized repetitive activities engaged in different cultural contexts\nMay involve words, gestures, movements, exchanges\nCan sometimes be taken-for-granted and other times front and center\nMark the shared beliefs of a social group and membership in a community"
  },
  {
    "objectID": "slides/Day16-Rituals.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day16-Rituals.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nHow might this ritual be taken for granted by different social groups?\nWhat are some of the shared beliefs that shape the rationale for engaging this ritual?"
  },
  {
    "objectID": "slides/Day16-Rituals.html#north-american-bird-breeding-survey",
    "href": "slides/Day16-Rituals.html#north-american-bird-breeding-survey",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "North American Bird Breeding Survey",
    "text": "North American Bird Breeding Survey\n\n\n\nIntroduced by Chan Robbins in 1966 as a result of rising concerns about the effects of DDT on bird populations\nAims to measure trends in bird populations over time\nNot a total population count\nResults in a longitudinal dataset of bird species counts at different stops along standardized routes\n\n\n\n\nDiego Delso"
  },
  {
    "objectID": "slides/Day16-Rituals.html#take-aways",
    "href": "slides/Day16-Rituals.html#take-aways",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Take-aways",
    "text": "Take-aways\n\nShared aims of making data comparable, clean, and well-cared for are held by data collectors\nEnvironmental forces constantly threaten these aims\nData collectors organize their social worlds towards these aims (e.g. establishing rituals)\nThis shapes the context of recorded data"
  },
  {
    "objectID": "slides/Day16-Rituals.html#jot-down-some-ideas-of-what-this-means-to-you",
    "href": "slides/Day16-Rituals.html#jot-down-some-ideas-of-what-this-means-to-you",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Jot down some ideas of what this means to you:",
    "text": "Jot down some ideas of what this means to you:\n\nThis chapter makes one basic point: the work of producing, preserving, and sharing data reshapes the organizational, technological, and cultural worlds around them. - Jackson and Ribes, 2013"
  },
  {
    "objectID": "slides/Day17-Incentives.html#reminders",
    "href": "slides/Day17-Incentives.html#reminders",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "",
    "text": "If you have not started on MP2 yet, you are late in doing so.\nThere is reading from Cooking Data due this Thursday."
  },
  {
    "objectID": "slides/Day17-Incentives.html#consider-a-time-that-you-would-be-rewarded-or-penalized-based-on-your-performance-towards-a-numeric-metric.",
    "href": "slides/Day17-Incentives.html#consider-a-time-that-you-would-be-rewarded-or-penalized-based-on-your-performance-towards-a-numeric-metric.",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Consider a time that you would be rewarded or penalized based on your performance towards a numeric metric.",
    "text": "Consider a time that you would be rewarded or penalized based on your performance towards a numeric metric."
  },
  {
    "objectID": "slides/Day17-Incentives.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day17-Incentives.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nWhat are some things you are doing or would like to do to navigate an emotionally charged time?"
  },
  {
    "objectID": "slides/Day17-Incentives.html#nyc-stop-question-and-frisk",
    "href": "slides/Day17-Incentives.html#nyc-stop-question-and-frisk",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "NYC Stop, Question, and Frisk",
    "text": "NYC Stop, Question, and Frisk\n\n\n\nPermits officers to stop individuals when “reasonable suspicion” of crime committed\n2011 District Court Floyd and Ourlicht vs. City of New York\n\nPresents data to show degree of racial profiling in practice\nAggregated from series of UF-250 forms officers fill out\n\n\n\n\nj-No, Flickr\n\n\n\nIn 2011, New York City’s controversial stop, question, and frisk policy – which permits officers to stop and question an individual when they have “reasonable suspicion” but not “probable cause” that the individual committed a crime – went before the US District Court. David Floyd and David Ourlicht argued that the NYPD had stopped them without reasonable suspicion, and the resulting high-profile case centered around issues of racial profiling in NYC policing. Much of the case against stop and frisk was built from data documenting racial breakdowns of the individuals stopped each year, along with the reason for the stop and the results of the stop. This data was aggregated from a series of forms (known as UF-250s) that officers are required to fill out and report following a stop. These forms collectively showed that 85% of those stopped were Black and Latino despite these sub-groups comprising just over half of the city’s population."
  },
  {
    "objectID": "slides/Day17-Incentives.html#hon.-scheindlins-ruling",
    "href": "slides/Day17-Incentives.html#hon.-scheindlins-ruling",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Hon. Scheindlin’s Ruling",
    "text": "Hon. Scheindlin’s Ruling\n\n\n\nJoel Spector ⓒ2013\n\n\nBecause it is impossible to individually analyze each of those stops, plaintiffs’ case was based on the imperfect information contained in the NYPD’s database of forms (‘UF-250s’) that officers are required to prepare after each stop.\n\n\n\n\nUltimately Judge Shira Schiedlen ruled that stop and frisk was being carried out in an unconstitutional way and ordered a scaling back of the practice. I want to start off the presentation by reciting an (admittedly) long quote from Sheindlen’s ruling that I think captures the key issues of this presentation:\nBecause it is impossible to individually analyze each of those stops, plaintiffs’ case was based on the imperfect information contained in the NYPD’s database of forms (‘UF-250s’) that officers are required to prepare after each stop. The central flaws in this database all skew toward underestimating the number of unconstitutional stops that occur: the database is incomplete, in that officers do not prepare a UF-250 for every stop they make; it is one-sided, in that the UF250 only records the officer’s version of the story; the UF-250 permits the officer to merely check a series of boxes, rather than requiring the officer to explain the basis for her suspicion; and many of the boxes on the form are inherently subjective and vague (such as ‘furtive movements’). Nonetheless, the analysis of the UF-250 database reveals that at least 200,000 stops were made without reasonable suspicion."
  },
  {
    "objectID": "slides/Day17-Incentives.html#juking-the-stats",
    "href": "slides/Day17-Incentives.html#juking-the-stats",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Juking the Stats",
    "text": "Juking the Stats\n\n\n\nCompStat: crime reduction strategy instituted in NYC in the 1990s\nUsed crime and deployment data as performance metrics\nInstitutionally incentivized data manipulation\n\n\n\npardonmeforasking, Flickr\n\n\n\nThe “imperfections” in this dataset have a history. This database came into existence alongside CompStat- a crime reduction strategy implemented in NYC in the 1990s. CompStat required precincts to produce weekly statistics regarding crime rates, officer deployments, stops, and arrests in their communities in order to generate evidence of policing effectiveness. The idea was to hold officers accountable through “timely and accurate intelligence” and “relentless follow-up and assessment.” However, with certain consequences tied to failures to demonstrate reductions in crime, the policies institutionally incentivized data manipulation - an issue colloquially referred to as “juking the stats.” Whistleblowing officers have presented audio recordings of their commanders demanding that they conduct more stops to meet quotas, and deceptively classifying crimes."
  },
  {
    "objectID": "slides/Day17-Incentives.html#disclosure-datasets",
    "href": "slides/Day17-Incentives.html#disclosure-datasets",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Disclosure Datasets",
    "text": "Disclosure Datasets\n\n\n\nTabular datasets that aggregate information produced and reported by the same institutions they are meant to hold accountable.\n\n\n\nSelf-disclosure concerns:\n\n“Juking the stats” (policing)\n“Cooking the books” (campaign finance)\n“Phantom reductions” (environmental monitoring)\n\n\n\n\n\nNotably, this idea of “juking the stats” has equivalent terms in other domains involving data produced by self-disclosure. For instance, we talk of political candidates “cooking the books” when it comes to disclosing data about campaign contributions and expenditures, and as I will turn to soon, environmental activists refer to industries reporting “phantom reductions” when it comes to disclosing information about their polluting activities. These are all examples of data quality issues regarding “disclosure datasets” - tabular datasets produced in accordance with laws requiring various kinds of disclosure. The most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Further, the values reported in disclosure datasets can lead to adverse actions - either formal or informal – taken against the reporting institutions. Combined, these issues institutionally incentivize misreporting and deceptive accounting practices."
  },
  {
    "objectID": "slides/Day17-Incentives.html#classes-of-accountability-data",
    "href": "slides/Day17-Incentives.html#classes-of-accountability-data",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Classes of Accountability Data",
    "text": "Classes of Accountability Data\n\n\n\nDisclosure Data\n\nDan Nguyen\n\n\n\nEvaluative Data\n Digits.co.uk Images\n\n\n\nMonitoring Data\n\n^Ivan Radic, on Flickr^\n\n\n\n\nDisclosure data is just one class of data used to hold people and institutions accountable. We might, for instance, contrast disclosure data with evaluative data – where people and institutions are not self-assessed, but instead assessed by an external evaluator. Examples include Yelp reviews of businesses and student evaluations of teaching effectiveness. We might also contrast disclosure data with surveillance data – where real-time technological systems track activities to hold folks accountable. Examples of such data include police body cameras and consumer credit tracking. Karen Levy’s work on electronic monitoring systems in the US trucking industry provides a compelling narrative on the social impact of surveillance data. In recognition that each of these datasets emerge from situated and delimited perspectives, we can point to concerns regarding data quality and bias in all of these classes of accountability data.\nIn what follows, I will focus on three kinds of data quality concerns that arise based on the institutional configurations that underpin the production of records for disclosure datasets:"
  },
  {
    "objectID": "slides/Day17-Incentives.html#false-reporting",
    "href": "slides/Day17-Incentives.html#false-reporting",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "False Reporting",
    "text": "False Reporting\n\n\n\nLying or misreporting data\nAuditing can be challenging\n\n\n ^Sample HMDA Data Collection Form^\n\n\n\nFalse reporting involves deliberate efforts to falsify records; it’s flat out lying. While institutions that aggregate disclosure reports into a dataset often attempt to curb false reporting through auditing efforts, small scale instances of false reporting can be inordinately difficult to detect given that the data systems are designed to tell the story from the perspective of the reporters.\nFor example, in order to ensure that financial institutions are in compliance with fair lending laws in the U.S. (such as the Equal Credit Opportunity Act and the Fair Housing Act), lenders are required to collect and report data on an applicant’s ethnicity, race, gender, and income when they apply for a mortgage. When reporting data relating to race, ethnicity, and gender, lenders are legally required to submit the information that applicants self-report when filling out a loan application to the Consumer Financial Protection Bureau (CFPB). However, in cases where an applicant elects not to provide their demographic data, lenders are required to record race, ethnicity, and gender based on visual observation of the applicant or the applicant’s surname. In one of the most notable cases of intentional HMDA misreporting, a CFPB investigation found that, for over three years, loan officers at Freedom Mortgage (one of the top ten lending institutions in the U.S.) were instructed to list “non-Hispanic White” as the race and ethnicity for every applicant that elected not to provide demographic data.\nSimilarly one of the most novel disclosure datasets my lab has been looking at is called Open Payments. To monitor for potential medical conflicts of interest, medical drug and device companies are required to disclose payments and other transfers of value made to physicians and teaching hospitals. This is mandated by the passing of the Physician Payments Sunshine Act (2010). Until recently, there has been little enforcement of the Sunshine Act, so cases of misreporting or failures to report have gone largely unrecognized. …but in 2020, the first settlement for violations to the Sunshine Act was announced. In efforts to get a South Dakota neurosurgeon to adopt their infusion pumps, Medtronic agreed to sponsor more than 100 events at a restaurant the neurosurgeon owned. Medtronic agreed to pay $9.2 million to resolve allegations for failure to report.\nIn terms of data quality, false reporting calls into question the accuracy and validity of the data."
  },
  {
    "objectID": "slides/Day17-Incentives.html#deceptive-accounting",
    "href": "slides/Day17-Incentives.html#deceptive-accounting",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Deceptive Accounting",
    "text": "Deceptive Accounting\n\n\n\nNot technically false but deliberately misleading\nTakes advantage of ambiguities in standards or laws\nOften involves “creative” approaches to measurement or classification\n\n\n\n\n\n\nWe can trace other cases of reporting that result in information that is not technically false, but deliberately misleading. For instance, institutions might take advantage of loopholes in reporting standards or leverage vagueness in the reporting requirements to cast their activities in a more positive light.\nAn example of this comes from the environmental health domain. The EPA’s Emergency Planning and Community Right to Know Act (EPCRA) of 1986 established the Toxic Release Inventory as a mechanism to monitor and inform the public of toxic emissions released in their communities. Every year, certain U.S. industrial facilities are required to report to the EPA the amounts of certain chemical on-site and off-site releases.\nNotably, while this Act mandates reporting of emissions, it does not mandate monitoring of emissions. While other environmental regulations do set certain monitoring standards for specific TRI chemicals and pollution activities, for all other chemicals and activities, facilities are required to report based on a “reasonable estimate” of releases and other waste management quantities. Because the data is self-reported, this provides a lot of flexibility to facilities to determine how they go about measuring emissions. Studies have shown that year-to-year changes in emissions at large facilities often have more to do with changes in estimation methods and interpretations of the law, rather than actual reductions in emissions. These are often called “phantom reductions” - where emissions just disappear from the books without indication of how the facility actually cleaned up their act.\nIn our research into disclosure datasets, my lab has identified a number of other examples of deceptive accounting. For instance, when reporting campaign spending, candidates will sometimes hide large payments by submitting them to consulting firms that then disburse the payments to other organizations. Historically, candidates did not have to disclose the payments’ ultimate recipients. …and the concept of “juking the stats” in policing is a direct example of this deceptive accounting practice, with officers learning to play the CompStat numbers game.\nIn terms of data quality, deceptive accounting calls into question the representativeness of the data - or in other words, the degree to which it represents what we think it represents."
  },
  {
    "objectID": "slides/Day17-Incentives.html#discursive-risk-of-regulatory-burden",
    "href": "slides/Day17-Incentives.html#discursive-risk-of-regulatory-burden",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Discursive Risk of Regulatory Burden",
    "text": "Discursive Risk of Regulatory Burden\n\nScope of dataset determined by reporting thresholds\nStakeholders have advocated for strengthening or loosening thresholds in line with certain political commitments\n“Regulatory burden” discourse has been powerful tool for loosening reporting requirements\n\n\nFinally, the information that ultimately appears in a disclosure dataset is often shaped by a series of legal standards regarding who has to report and when they have to report. These standards are not neutral, and tend to evolve as stakeholders with certain political commitments towards transparency and institutional accountability call for amendments. Certain stakeholders advocate on behalf of stricter requirements and more data reporting in efforts to secure transparency and accountability, whereas other stakeholders advocate on behalf of loosening the requirements. Notably, one of the most powerful weapons against strengthening disclosure dataset programs has been discourse around “regulatory burden”. Businesses cite the cost and lack of feasibility of filling out paperwork when advocating against data collection. For instance, under the George W. Bush Administration, the TRI “Burden Reduction Rule” significantly raised the threshold regarding how much pollution a facility needed to emit before reporting requirements kicked in. Just recently, amendments to HMDA raised the threshold regarding the volume of loans a bank needs a originate for reporting requirements to kick in. The pressures Covid-19 had placed on banks was cited as a reason for this change. … and regulatory burden discourse has been effectively leveraged to keep the US’s National Use of Force database – where police officers disclose when they use force against citizens – a voluntary data program.\nWhen regulatory burden discourse is successful, fewer institutions are required to report on fewer activities, diminishing what is tracked in the data. Thresholds tend to fluctuate in line with political changeover. For instance, the stringency of TRI reporting often correlates with the political leanings of presidential administrations. This has meant that the definitions underpinning disclosure datasets are quite malleable - subject to change in conjunction with different modes of social advocacy. This malleability is a double-edged sword for many community groups calling for increased accountability. In one sense, it ensures that disclosure programs can continuously be strengthened. However, changing reporting standards makes it difficult to perform year-to-year comparisons of the data. …which is often needed to measure whether institutions are “cleaning up” their acts.\nIn terms of data quality, this discursive risk calls into question the scope and comprehensiveness of the data."
  },
  {
    "objectID": "slides/Day19-Mobilization.html#what-does-the-following-statement-mean-to-you",
    "href": "slides/Day19-Mobilization.html#what-does-the-following-statement-mean-to-you",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "What does the following statement mean to you?",
    "text": "What does the following statement mean to you?\n\nOn April 10, 2023 at 9AM Northampton’s reported AQI was 54."
  },
  {
    "objectID": "slides/Day19-Mobilization.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day19-Mobilization.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nWhat information do we need to be able to interpret the meaning of the number 54 in the following slide?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#what-is-evidence",
    "href": "slides/Day19-Mobilization.html#what-is-evidence",
    "title": "Day Nineteen: Mobilizating Meaning",
    "section": "What is ‘evidence’?",
    "text": "What is ‘evidence’?\n\nEtymologically dual meaning:\n\nTransparent, obvious, clear, speaking for itself\nAppearance or indication, requiring interpretation\n\n\n\nBiruk, Cal. 2018. Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books."
  },
  {
    "objectID": "slides/Day19-Mobilization.html#turn-to-a-neighbor-and-discuss-1",
    "href": "slides/Day19-Mobilization.html#turn-to-a-neighbor-and-discuss-1",
    "title": "Day Nineteen: Mobilizating Meaning",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nHow do you know if this is a big or small quantity?\nDo you have a yardstick to help you determine this?\nHow might this number mean different things to different social groups?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#interpretive-flexibility",
    "href": "slides/Day19-Mobilization.html#interpretive-flexibility",
    "title": "Day Nineteen: Mobilizating Meaning",
    "section": "Interpretive Flexibility",
    "text": "Interpretive Flexibility\n\nAnti-determinist frameworks\nRefers to the differing meanings that social groups assign to technologies\nExplain how technologies are “socially constructed”"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#mobilizing-data-narratives",
    "href": "slides/Day19-Mobilization.html#mobilizing-data-narratives",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Mobilizing Data Narratives",
    "text": "Mobilizing Data Narratives\n\nMobilization refers to the processes by which people prepare something to be put to use or into action\nStakeholders strategically engage in meaning-making activities around data\nShapes societal interpretations of data"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#framing-statistics",
    "href": "slides/Day19-Mobilization.html#framing-statistics",
    "title": "Day Nineteen: Mobilizating Meaning",
    "section": "Framing Statistics",
    "text": "Framing Statistics\n\nComparison/Ranking\n\nIs x more/less than y?\n\nStandards-Setting/Benchmarking\n\nDoes x meet z standard/benchmark?\n\nAppealing to Emotion/Lived Experience\n\nHow does x make me feel/relate to my life?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#take-aways",
    "href": "slides/Day19-Mobilization.html#take-aways",
    "title": "Day Nineteen: Mobilizating Meaning",
    "section": "Take-aways",
    "text": "Take-aways\n\nThe meaning/significance of numbers is not self-evident.\nData becomes meaningful as stakeholders develop tools for narrating the significance of a number.\nEthnographers can study these meaning-making practices."
  },
  {
    "objectID": "slides/Day19-Mobilization.html#announcements",
    "href": "slides/Day19-Mobilization.html#announcements",
    "title": "Day Nineteen: Mobilizating Meaning",
    "section": "Announcements",
    "text": "Announcements\n\n\n\nOptional group work time with Rose on Thursday.\nDon’t forget to fill out the group evaluations.\nAdditional enrichment option\n\nAttend this event and write 400-word memo analyzing the topic using one of the key concepts from data activism readings (Concept must be approved by me.)"
  },
  {
    "objectID": "grading_contract.html#final-project-resources",
    "href": "grading_contract.html#final-project-resources",
    "title": "Grading Contract",
    "section": "Final Project Resources",
    "text": "Final Project Resources\nThis semester your project group will develop a user guide for a federal dataset. You will complete a series of exercises in class to support you in writing the guide. These exercises are not graded; they are meant to frame your research and help you generate material for the project. You should work on relevant aspects of them weekly with your group in order to make progress towards the final product. While these exercises will include both qualitative and computational exercises, the point of this project is not to perform a statistical analysis on this dataset. Instead, you will study the dataset ethnographically and strive to effectively communicate how the dataset came to be, how it has disseminated, how it can be used, and what some of its limitations are.\n\nProject Description\nRose’s Project Guide Sheet\nDataset Loading Instructions\nExample User Guide\n\nBelow I’ve outlined completion criteria for the two non-written assignments for this course.\n\nReading Annotations\nEach week a selection of course readings will be posted as a single assignment on Perusall - a system for students to collaboratively annotate course readings. To earn this credit, you will be expected to post 3 quality annotations per assignment to Perusall. (Note that a single assignment can have multiple readings, but you need only submit 3 annotations total. You do not need to annotate every reading in the assignment.) A quality annotation is one in which you synthesize concepts, ask thought-provoking questions, or connect ideas to external issues. I have found that students get the most out of Perusall when they respond to each other’s annotations. Annotations must be completed before class to receive credit.\n\n\nCommunity Labor\nEthnography is at its best when it involves collaborative inquiry and interpretation. Because of this, I want to encourage us to foster a cooperative community in our classroom. Ethnographers know that building and sustaining strong communities are important and often invisible forms of labor. In an effort to foreground and reward that labor, I’ve built opportunities to contribute to the course community into our grading contract. There are four opportunities for earning community labor points in this course. It will be your responsibility to keep track of your community labor via our course labor log (see FAQ).\n\nContributing on Slack\nThe first opportunity for earning community labor points is through posting in the #sds-237-discussions Slack channel. For every conversation that you initiate in this channel, you will earn 1 community labor point. For every conversation that you respond to in this channel with substantive summary, critique, or reflection, you will earn 1 community labor point. Finally, for every question that you answer in the #sds-237-questions channel, you will earn 1 community labor point.\n\n\nCompleting Group Evaluations\nMid-way through the semester you will have an opportunity to evaluate your own contributions to your group work, along with that of your peers. Your feedback will be shared with members of your group. Completing group evaluations will count for 1 community labor point.\n\n\nParticipating in Peer Review Workshops\nThroughout the semester, I will set up workshops where students may submit their written work for peer review in preparation for assignment submissions. Each peer review workshop that you participate in will count for 2 community labor points.\n\n\nContributing Class Notes\nFollowing a class period, up to two students may type up a 1-page outline of what was covered in that class period and post a link to that outline in #sds-237-class-notes. You can sign-up to serve as the notetaker for a certain class here. These notes should be a full, single-space page, and should make sense to someone that was not present in class. Each class note outline that you contribute will count for 2 community labor points.\nNote that throughout the semester, I may offer additional opportunities to earn community labor points as unexpected forms of course labor arise.\n\nThe final project will be completed in groups, and considerable chunks of the group project will be completed in class on Thursdays. While I will not formally be taking attendance for this course, I reserve the right to adjust your final grade by a letter grade if you’ve missed more than three classes involving final project group work. Be sure to reach out to me early on if there are extenuating circumstances that will consistently impact your attendance."
  },
  {
    "objectID": "slides/Day22-Credibility.html#take-out-a-sheet-of-paper-and-draw-your-map-of-science.",
    "href": "slides/Day22-Credibility.html#take-out-a-sheet-of-paper-and-draw-your-map-of-science.",
    "title": "Day Twenty-Two: Credibility",
    "section": "Take out a sheet of paper and draw your “map of science”.",
    "text": "Take out a sheet of paper and draw your “map of science”.\n\nWhat are the other features on the map?\nWhat is central and what is in the periphery?\nIs there a place for art/religion?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day22-Credibility.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Twenty-One: Data Activism",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nWhat do you suppose the designers of this system believe about the data it’s collecting?\nWhat would it look like to resist this system?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#boundary-work",
    "href": "slides/Day22-Credibility.html#boundary-work",
    "title": "Day Twenty-Two: Credibility",
    "section": "Boundary Work",
    "text": "Boundary Work\n\nThomas Gieryn, Cultural Cartographies of Science\nAcknowledges the lack of stable criteria for demarcating science from non-science.\nDifferent people justify what makes something scientific vs unscientific differently.\nAs credibility contests emerge, people make claims based on their own maps of science.\nCulturally chart the boundaries around what we consider scientific."
  },
  {
    "objectID": "slides/Day22-Credibility.html#how-do-we-draw-the-boundaries-of-credibility-in-data-work",
    "href": "slides/Day22-Credibility.html#how-do-we-draw-the-boundaries-of-credibility-in-data-work",
    "title": "Day Twenty-Two: Credibility",
    "section": "How do we draw the boundaries of credibility in data work?",
    "text": "How do we draw the boundaries of credibility in data work?\n\nWho gets seen as a legitimate claims-maker in data science work, and whose voices are often denied a seat at the table?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#example-lois-gibbs-and-love-canal",
    "href": "slides/Day22-Credibility.html#example-lois-gibbs-and-love-canal",
    "title": "Day Twenty-Two: Credibility",
    "section": "Example: Lois Gibbs and Love Canal",
    "text": "Example: Lois Gibbs and Love Canal"
  },
  {
    "objectID": "slides/Day22-Credibility.html#deficit-model",
    "href": "slides/Day22-Credibility.html#deficit-model",
    "title": "Day Twenty-Two: Credibility",
    "section": "Deficit Model",
    "text": "Deficit Model\n\nBelief that the reason people distrust science is because they lack knowledge\nOften casts scientific illiteracy as both a technical problem and a moral problem\nPresumed solution to increasing public trust in science is to teach more science\n\n\nSTS rejects the deficit model…\n\n\nIgnores public’s situated knowledge\nSuggests knowledge is context-free\nDoesn’t get at the root of distrust"
  },
  {
    "objectID": "slides/Day22-Credibility.html#enactments-of-expertise",
    "href": "slides/Day22-Credibility.html#enactments-of-expertise",
    "title": "Day Twenty-Two: Credibility",
    "section": "Enactments of Expertise",
    "text": "Enactments of Expertise\n\n\n\nExpertise as a signal of authority\nEnacted through credentialing, use of certain vocabularies, creation of exclusive social circles, clothing, norms of behavior\nEnactments also serve a gate-keeping role in determining who gets to contribute to science/data science\n\n\nIn her history of the medical activism by the Black Panthers, Alondra Nelson discusses how one way the Black Panthers enacted their expertise was in their choice to wear white lab coats.\n > Sickle cell anemia testing. Oakland, Calif. 1972. credit NY Times"
  },
  {
    "objectID": "slides/Day22-Credibility.html#citizen-science",
    "href": "slides/Day22-Credibility.html#citizen-science",
    "title": "Day Twenty-Two: Credibility",
    "section": "Citizen Science",
    "text": "Citizen Science\n\nExclusionary science as a democratic problem\nExamples of citizen engagement:\n\nCounting declining bird populations\nAir and water quality monitoring\nWriting health books for underrepresented populations\n\nMust learn how to enact expertise to earn a seat at the table"
  },
  {
    "objectID": "slides/Day22-Credibility.html#social-movement-theory",
    "href": "slides/Day22-Credibility.html#social-movement-theory",
    "title": "Day Twenty-One: Data Activism",
    "section": "Social Movement Theory",
    "text": "Social Movement Theory\n\nSociological research that studies how social movements form, operate, sustain, and fizzle out\nPolitical Process Theory: suggests that certain political formations enable social movements to form and operate\n\nPolitical opportunities\nMobilizing structures\nFraming processes\nProtest cycles\nRepertoires of contention\n\nResource Mobilization Theory: suggests that the ability to leverage certain resources enable social movements to mobilize\n\nMaterials resources, human resources, organizational resources, cultural resources, etc."
  },
  {
    "objectID": "slides/Day23-Ignorance.html#feminist-arguments-on-the-study-of-ignorance",
    "href": "slides/Day23-Ignorance.html#feminist-arguments-on-the-study-of-ignorance",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Feminist arguments on the study of ignorance",
    "text": "Feminist arguments on the study of ignorance\n\nIgnorance is more than a gap in knowledge.\n“Ignorance, like knowledge, is situated.”\n\n\nTuana, Nancy. 2006. “The Speculum of Ignorance: The Women’s Health Movement and Epistemologies of Ignorance.” Hypatia 21 (3): 1–19. https://doi.org/10.1111/j.1527-2001.2006.tb01110.x."
  },
  {
    "objectID": "slides/Day23-Ignorance.html#taxonomy-of-ignorance",
    "href": "slides/Day23-Ignorance.html#taxonomy-of-ignorance",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Taxonomy of Ignorance",
    "text": "Taxonomy of Ignorance\n\nEngineered Ignorance\nUndone Science\nOrganized Ignorance\nLoving Ignorance"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#big-tobacco-hides-cancer-risks",
    "href": "slides/Day23-Ignorance.html#big-tobacco-hides-cancer-risks",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Big Tobacco Hides Cancer Risks",
    "text": "Big Tobacco Hides Cancer Risks\n\nEarly 1950s, considerable expansion of research on tobacco health risks (e.g. lung cancer)\nTobacco industry seeks to produce knowledge that will undo effects of emerging research:\n\nEnlisted public relations firm Hill & Knowlton\nDetermined that advertising won’t work because self-interested by definition\nSought out credentialed medical skeptics to engineer controversy\nCreated Tobacco Industry Research Committee and funded research to sew doubt/uncertainty"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#merchants-of-doubt",
    "href": "slides/Day23-Ignorance.html#merchants-of-doubt",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Merchants of Doubt",
    "text": "Merchants of Doubt"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#undone-science-areas-of-research-that-go-unfunded-left-incomplete-or-get-ignored",
    "href": "slides/Day23-Ignorance.html#undone-science-areas-of-research-that-go-unfunded-left-incomplete-or-get-ignored",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Undone science: Areas of research that go unfunded, left incomplete, or get ignored",
    "text": "Undone science: Areas of research that go unfunded, left incomplete, or get ignored\n\nFrickel, Scott, Sahra Gibbon, Jeff Howard, Joanna Kempner, Gwen Ottinger, and David J. Hess. 2010. “Undone Science: Charting Social Movement and Civil Society Challenges to Research Agenda Setting.” Science, Technology & Human Values 35 (4): 444–73. https://doi.org/10.1177/0162243909345836."
  },
  {
    "objectID": "slides/Day23-Ignorance.html#medical-funding",
    "href": "slides/Day23-Ignorance.html#medical-funding",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Medical Funding",
    "text": "Medical Funding\n\nNature reports that much medical funding goes to the “big three” - HIV/AIDS, malaria and tuberculosis\nFunding diverted away from “neglected tropical diseases”\n\nImpact billions of people and disproportionately impact most vulnerable populations\nExamples: dengue, leprosy, trachoma\n\nCovid-19 further diverted funding from these programs"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#our-bodies-ourselves",
    "href": "slides/Day23-Ignorance.html#our-bodies-ourselves",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Our Bodies Ourselves",
    "text": "Our Bodies Ourselves\n\n\n\nTuana, Nancy. 2006. “The Speculum of Ignorance: The Women’s Health Movement and Epistemologies of Ignorance.” Hypatia 21 (3): 1–19. https://doi.org/10.1111/j.1527-2001.2006.tb01110.x."
  },
  {
    "objectID": "slides/Day23-Ignorance.html#environmental-testing-following-hurricane-katrina",
    "href": "slides/Day23-Ignorance.html#environmental-testing-following-hurricane-katrina",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Environmental Testing following Hurricane Katrina",
    "text": "Environmental Testing following Hurricane Katrina\n\nOnly test contaminants designated by law (<200 of 8200 in TSCA) on public properties\nTend to test where there was flooding, not where there may be the most contaminants\n\nMost serious flooding escaped “sliver along the river” - location of the city’s industrial corridor\nUntested areas include several brownfields and areas with history of manufacturing expansion"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#colony-collapse-disease",
    "href": "slides/Day23-Ignorance.html#colony-collapse-disease",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Colony Collapse Disease",
    "text": "Colony Collapse Disease\n\nUS beekeepers have lost almost 1/3 of bees every year since 2006\nScientific studies of insecticides on bees were designed to test the individual lethal effects of a chemical on a bee population\n\nPart of a “control-oriented” scientific culture\n\nFailed to account for slow, cumulative effects over time and the interactions of chemicals and pathogens\n\n\nKleinman, Daniel Lee, and Sainath Suryanarayanan. 2013. “Dying Bees and the Social Production of Ignorance.” Science, Technology & Human Values 38 (4): 492–517. https://doi.org/10.1177/0162243912442575."
  },
  {
    "objectID": "assignments.html#fieldnotes",
    "href": "assignments.html#fieldnotes",
    "title": "Assignments",
    "section": "Fieldnotes",
    "text": "Fieldnotes\nInstructions for the fieldnote assignment can be found here."
  },
  {
    "objectID": "assignments.html#mini-projects-substantive-revision",
    "href": "assignments.html#mini-projects-substantive-revision",
    "title": "Assignments",
    "section": "Mini-Projects + Substantive Revision",
    "text": "Mini-Projects + Substantive Revision\nThe course mini-projects will provide you with opportunities to practice ethnographic research and cultural analysis. Mini-projects are the predominant way that you will practice the Data Ethnography Methods learning dimension in this course.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that you may complete the mini-projects independently or in a pair. If you choose to complete the project as a pair, both students must participate in data collection, analysis, and writing. Be prepared to submit a short paragraph describing the breakdown of labor, along with your write-up.\n\n\n\nLearning Objectives\n\nIdentify, implement, and critique data ethnography research methods\nDevelop observational habits in data-saturated environments\nDevelop skill in analyzing social forces and systems\nPlan and execute a substantive writing revision, while evaluating feedback\n\n\n\nInstructions\n\nInfrastructural Analysis (MP 1)Discourse Analysis (MP 1)Interview (MP 2)Participant Observation (MP 2)\n\n\nFor this assignment, you will select a standard classification system that supports a dataset or system. You will then investigate that information infrastructure through a socio-cultural lens, aiming to respond to the question: What is a cultural assumption embedded in this classification system that goes on to shape the social outcomes of the data it supports? You may briefly research the history of changes to the infrastructure; however, this assignment is not a literature review, and you should limit your use of secondary sources to support your arguments. This paper should represent your cultural analysis of this dataset, not that of others. To do so, you should closely examine the structure of the classification system, using the Infrastructural Analysis Worksheet.\n\nSelecting a data infrastructure\nBefore you select an infrastructure, you should identify a public dataset that organizes information into a series categories\nExamples of classification systems that I’ve seen referenced in public datasets include:\n\nStandard Industrial Classification System (SIC) or the North American Industrial Classification System (NAICS)\nCarnegie Classification of Institutions of Higher Education\nBISAC Subject Headings List\nDiagnostic and Statistical Manual of Mental Disorders (DSM)\nUniform Crime Reporting crime classifications\nAssociation of Computing Machinery (ACM) Computing Classification System\nCandid’s Philanthropy Classification System (PCS)\n\n\n\nAnalyzing the data infrastructure\nTo analyze this infrastructure, you should complete the Infrastructural Analysis Worksheet, which you will submit along with the final write-up. You should aim to answer most of the questions in this worksheet, but you do not need to answer all of them. To answer these questions, you can rely on a diverse range of resources. First and foremost, you should browse the classification system itself, noting how concepts are organized. However, to answer the questions in the worksheet, you may also need to review the websites of the organizations that created/steward the classification, review changelogs that identify changes to the classification, and conduct research to find journal articles, news articles, blog posts, or other grey literature, discussing the classification.Finally, you should consider how everything you learned shaped the way values are presented in the dataset where you originally identified this infrastructure.\n\n\nWrite-up\nFollowing your analysis, you will write-up a 750-1000 word memo presenting an an answer to the question: What is a cultural assumption embedded in this classification system that goes on to shape the social outcomes of the data it supports? I encourage you to use this memo template for outlining your write-up. You should engage thick description as you detail the structure and organization of the infrastructure and interpret its symbolic meaning.\n\n\n\n\n\n\nImportant\n\n\n\nI want to emphasize that this is not a literature review, and you should not approach this assignment simply by regurgitating what you learn in sources you find. Instead, you should focus on analyzing the ways in which the infrastructure both enables and delimits knowledge production by closely studying how its component parts are organized and how they impact your dataset, along with which stakeholder groups advocated for certain changes. The sources you find should serve to provide context for the points that you make. Your write-up should have introductory and concluding paragraphs that clearly indicate the central point you are making in the write-up and that point should be carried through all of the examples and arguments presented in the write-up.\n\n\n\n\nCriteria for completion\n\nStudent submits a mostly complete Infrastructural Analysis Worksheet.\nStudent writes a 750-1000 word memo describing and interpreting the data infrastructure. All sources are properly cited in the write-up.\nStudent synthesizes evidence into one argument, presented at the start of the write-up and carried through each paragraph.\n\n\n\n\nFor this assignment, you have two options: you can 1) watch 1 video (at least 45 minutes long) of a talk about data delivered by a data scientist, or 2) three TED talks (at least 15 minutes each) about data. Please confirm your selection with me ahead of time by DM’ing me on Slack.\n\nAnalyzing the talks\nFollowing the framework in the Discourse Analysis Worksheet, you should examine how data is talked about in the video(s). Consider the epistemologies vocalized, the binary oppositions that structure the language, the rhetorical strategies used, and what this tells us about data culture. You should take detailed notes by responding to several sections within the worksheet. As “data” to back up claims regarding what you notice about the discourse, you should transcribe a series of quotes from the talks. You can add these quotes directly to the worksheet. You will submit the worksheet with your final write-up.\n\n\nWrite-up\nFollowing your analysis, you will write-up a 750-1000 word memo detailing an argument about the data discourse conveyed in the talk(s). What cultural assumptions about (choose one): 1) what data is, 2) data’s relationship with truth, or 3) what makes data good vs. bad … are reflected in discourse about data? I encourage you to use this memo template for outlining your write-up. Direct quotations should be referenced throughout (and cited) to back up your claims, but do not count towards the overall word count. Your write-up should have introductory and concluding paragraphs that clearly indicate the central point you are making in the write-up and that point should be carried through all of the examples and arguments presented in the write-up.\n\n\n\n\n\n\nWarning\n\n\n\nIf you choose to watch three TED Talks, please note that you should still only present one argument, synthesizing what you learn across all three videos. This means that you should compare discourse across multiple talks in each paragraph. Assignments that present three distinct discourse analyses/do not offer in-paragraph synthesis and comparison will not receive full credit.\n\n\n\n\nCriteria for completion\n\nStudent submits the Discourse Analysis Worksheet with several sections completed.\nStudent writes a 750-1000 word memo detailing the discourse of the talk(s). Direct quotations are included as evidence, and not included in the overall word count.\nStudent synthesizes evidence into one argument, presented at the start of the write-up and carried through each paragraph. If analyzing multiple talks, student offers comparison and synthesis in each paragraph.\n\n\n\n\nFor this assignment you will conduct a 30-minute Zoom interview with an individual for whom data figures centrally in their work. The goal of your interview will be to deepen understanding of the cultures of data work.\n\nIdentifying a data professional, scheduling an interview, and obtaining consent\nIf you choose this assignment, it will be your responsibility to identify a data professional, reach out to them (cc’ing me), and coordinate an interview time well in advance of the project due date. There is a template for emailing an individual provided here, along with a consent form the person you interview should sign. Keep in mind that, while your SDS professors may be an obvious first choice, they cannot support interviews with all of the students in our course. I encourage you to think expansively about who you might interview. I’m happy to offer suggestions on identifying an appropriate person in office hours.\n\n\nPreparing for the interview\nYour interview should address the question: How does the interviewee define the boundaries of data science? How do they define/what do they consider legitimate data science expertise? Write up 10 open-ended questions in preparation for the interview that aim to unpack this theme. You should submit these 10 interview questions along with your memo in your Google Drive.\n\n\nDuring the interview\nAsk the interviewee if they are comfortable with you recording the conversation. You may record with their express permission. If they do not agree to recording, you should be prepared to take notes during the interview.\n\n\nAnalyzing the interview\nYou may wish to use this worksheet to help you with your analysis. However, you are not required to submit this worksheet with your memo.\n\n\nWrite-up\nFollowing your interview, you will write-up a 750-1000 word memo describing and interpreting what was discussed throughout the conversation. Your write-up should address the prompt: How does the interviewee define the boundaries of data science? How do they define/what do they consider legitimate data science expertise? I encourage you to use this memo template for outlining your write-up. Your write-up should have introductory and concluding paragraphs that clearly indicate the central point you are making in the write-up and that point should be carried through all of the examples and arguments presented in the write-up.\n\n\nCriteria for completion\n\nStudent writes/submits 10 relevant interview questions developed in preparation for the interview.\nStudent writes/submits a 750-1000 word memo describing and interpreting what was discussed in the interview.\nStudent synthesizes evidence into one argument, presented at the start of the write-up and carried through each paragraph.\n\n\n\n\nFor this assignment, you will observe a setting in which data figures centrally. You may choose to observe a class engaging in some form of data collection, a hack-a-thon, or even a friend working on a coding assignment for a data science course. The only data setting that I discourage for this assignment is talks that are being presented on-line. These are better suited for the discourse analysis mini-project. As you observe, you will be aiming to gather data that addresses one of the following two questions: 1) How are data practices embodied in this data environment, and with what outcomes/consequences for the data? OR 2) What rituals are engaged in this data, and with what outcomes/consequences for the data?\n\nBefore the Event\nPrior to your observation, you need to obtain permission to observe. If you are observing a class, you should get permission from the instructor. If you are observing an event, you should get permission from the organizer. …if a lab, get permission from the lab manager or faculty director, etc. Be sure to cc me when you email to obtain permission. You may use this email template when obtaining permission.\n\n\nDuring the Observation\nThroughout the event you should take extensive field notes (at least 6 double-spaced, typed pages) about the physical environment of the event, how people behave and interact, how people talk about and data, the rituals they perform, the infrastructures they engage, and more generally how data figures into the social setting. You may talk to individuals in the environment to ask clarifying question, but if you do, you should disclose that you are an ethnographer studying the space.\n\n\nAnalyzing the Observation\nYou may wish to use this worksheet to help you with your analysis. However, you are not required to submit this worksheet with your memo.\n\n\nWrite-up\nFollowing your observation, you will write-up a 750-1000 word memo describing the event and offering a qualitative interpretive analysis responding to either 1) How are data practices embodied in this data environment, and with what outcomes/consequences for the data? OR 2) What rituals are engaged in this data, and with what outcomes/consequences for the data?. I encourage you to use this memo template for outlining your write-up. Your write-up should have introductory and concluding paragraphs that clearly indicate the central point you are making in the write-up and that point should be carried through all of the examples and arguments presented in the write-up.\n\n\nCriteria for completion\n\nStudent submits the equivalent of about 6 double-spaced typed pages of field notes taken during the event.\nStudent writes/submits a 750-1000 word memo describing and interpreting the event.\nStudent synthesizes evidence into one argument, presented at the start of the write-up and carried through each paragraph.\n\n\n\n\n\n\n\nRevision\nAfter receiving feedback from me on both submissions, you will have an opportunity to revise one of your submissions. This assignment has three steps:\n\nRead this document on creating a revision plan.\nComplete the Revision Plan worksheet.\nRevise your submission in the same document you had previously been working in based on what you outlined in your revision plan.\n\n\nRemember that grammar, spelling, and punctuation concerns should not be included in a revision plan. Those concerns are addressed in a proofreading stage, not a revising stage.\n\n\n\nResources\n\nMemo template is here\nExample Mini-Project 1 are here\n\n\n\n\n\n\n\nNote\n\n\n\nDue to IRB restrictions, I cannot provide examples of past Mini-Project 2 assignments. If you are struggling to figure out how to compose the write-up, please plan to visit office hours."
  },
  {
    "objectID": "assignments.html#final-project",
    "href": "assignments.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\nIn May 2009, Data.gov - a web portal for accessing US government datasets - was launched by then federal Chief Information Officer Vivek Kundra. Following this, in December 2009, then US President Barack Obama signed the Open Government Data Directive, requiring that all federal agencies post at least 3 high value datasets on data.gov within 45 day. A few years later in May 2013, Pres. Obama signed an Executive Order to: “Mak[e] Open and Machine Readable the New Default for Government Information.”\nThe Order required that the US Office of Management and Budgeting, in collaboration with the CIO and CTO, put out and oversee an Open Data Policy. This policy required the following:\n\nData needs to be published in machine-readable formats\nData needs to be licensed openly\nData needs to be described with metadata.\n\n\nPrompt\nFor this assignment, I would like you to imagine that the Data.gov Program Management Office in US General Services Administration Technology Transformation Services, the Office of Government and Information Services (OGIS), and the Office of Management and Budget (OMB) are jointly championing a new initiative to improve the accessibility of open government metadata. Administrative metadata describing how datasets are managed or stewarded gets published with most open government datasets. Similarly, descriptive metadata gets published in the form of data dictionaries that provide official definitions of observations and variables present in the dataset. However, user-friendly descriptions of how the dataset was produced, how standard definitions were chosen, how categories were divided, how measurements get taken, what assumptions and judgments are built into the data, why certain information may be missing, and where and how the data gets referenced is often harder to come by and rarely published in a succinct format accessible to diverse users.\nTo help advance this initiative, imagine the offices will be contracting with a number of teams of consultants to develop a series of dataset user guides. Unlike many existing metadata formats, dataset user guides will be narrative documents. In the process of advancing a similar municipal level initiative, the NYC Open Data Team (with Julia Marden, Tiny Panther Consulting, Sharon Lintz and Jo Polanco) defined dataset user guides as describing “the content of a dataset, how it was created, the agency who maintains it, and how users can begin to use the data.”\nThis semester your project group will develop a user guide for a federal dataset. Each week you will work through exercises that help apply course concepts from that week to the study of this dataset. Note that you will study the dataset ethnographically and strive to effectively communicate how the dataset came to be, how it has disseminated, how it can be used, and what some of its limitations are.\nYour final user guide may be presented in any format you wish, and I encourage you to be creative. However, the format you choose should be tailored to an audience that you identify in advance.\n\n\nLearning Objectives\n\nApply the course concepts in studying the provenance of a dataset\nDevelop skill in analyzing social forces and systems\nProduce effective and descriptive data documentation that puts data in context\nPlan and execute a substantive writing revision, while evaluating feedback\n\n\n\nExercises\n\nGroup Contract\nSemiotic Analysis\nPeopling the Data\nData Collection Rituals\nInstitutional Incentives\nDiscourse Analysis\n\n\n\nAssignment Expectations\n\nThe exercises I provide are not graded and not reviewed by me; they are meant to frame your research and help you generate material for the project. I will only be formally reviewing your final user guide submission.\nBecause each group is working on a different dataset, there will likely be components of each exercise that are not relevant to your group; I only expect you to complete about 70% of each exercise. Skip the parts that aren’t relevant to you.\nYou should be meeting with your groups at least one hour each week outside of class to coordinate your work and plan to work an additional hour independently to make progress towards your research.\nThe point of this project is not to perform a statistical analysis on this dataset. Instead, you will study the dataset ethnographically and strive to effectively communicate how the dataset came to be, how it has disseminated, how it can be used, and what some of its limitations are.\nBy November, at least part of your group work time should transition from researching the dataset to composing a user guide. Your final submission should not be a compilation of text from the exercises you completed. That text was written for you, not for an outside audience. Part of the goal of this project is to be able to translate your research into information that is accessible to an audience.\n\n\n\nProject Resources\n\nExample User Guide\n\n\n\nCriteria for Completion\n\nStudent team submits a completed group contract, first draft, and final project draft on time.\nStudent team generates a user guide that demonstrates having completed substantial research across all five project exercises.\nStudent team generates a user guide with language, visuals, and formats tailored towards a particular audience."
  },
  {
    "objectID": "assignments.html#reading-annotations",
    "href": "assignments.html#reading-annotations",
    "title": "Assignments",
    "section": "",
    "text": "Each week a selection of course readings will be posted as a single assignment on Perusall - a system for students to collaboratively annotate course readings. To earn this credit, you will be expected to post 3 quality annotations per assignment to Perusall. A quality annotation is one in which you synthesize concepts, ask thought-provoking questions, or connect ideas to external issues. I have found that students get the most out of Perusall when they respond to each other’s annotations. Annotations must be completed before class to receive credit.\n\n\n\n\n\n\nNote\n\n\n\nA single assignment can have multiple readings, but you need only submit 3 annotations total. You do not need to annotate every reading in the assignment."
  },
  {
    "objectID": "assignments.html#community-labor",
    "href": "assignments.html#community-labor",
    "title": "Assignments",
    "section": "Community Labor",
    "text": "Community Labor\nEthnography is at its best when it involves collaborative inquiry and interpretation. Because of this, I want to encourage us to foster a cooperative community in our classroom. Ethnographers know that building and sustaining strong communities are important and often invisible forms of labor. In an effort to foreground and reward that labor, I’ve built opportunities to contribute to the course community into our grading contract. There are four opportunities for earning community labor points in this course. It will be your responsibility to keep track of your community labor via our course labor log.\n\nContributing on Slack\nThe first opportunity for earning community labor points is through posting in the #sds-237-discussions Slack channel. For every conversation that you initiate in this channel, you will earn 1 community labor point. For every conversation that you respond to in this channel with substantive summary, critique, or reflection, you will earn 1 community labor point. Finally, for every question that you ask or answer in the #sds-237-questions channel, you will earn 1 community labor point.\n\n\nCompleting Group Evaluations\nMid-way through the semester you will have an opportunity to evaluate your own contributions to your group work, along with that of your peers. Completing group evaluations will count for 1 community labor point.\n\n\nParticipating in Peer Review Workshops\nThroughout the semester, I will set up workshops where students may submit their written work for peer review in preparation for assignment submissions. Each peer review workshop that you participate in will count for 2 community labor points.\n\n\nContributing Class Notes\nFollowing a class period, up to two students may type up a 1-page outline of what was covered in that class period and post a link to that outline in #sds-237-class-notes. You can sign-up to serve as the notetaker for a certain class here. These notes should be a full, single-space page, and should make sense to someone that was not present in class. Each class note outline that you contribute will count for 2 community labor points.\nNote that throughout the semester, I may offer additional opportunities to earn community labor points as unexpected forms of course labor arise."
  },
  {
    "objectID": "assignments.html#enrichment",
    "href": "assignments.html#enrichment",
    "title": "Assignments",
    "section": "Enrichment",
    "text": "Enrichment\nInstructions for enrichment assignments can be found here."
  },
  {
    "objectID": "schedule.html#september-05-2023",
    "href": "schedule.html#september-05-2023",
    "title": "Schedule",
    "section": "September 05, 2023",
    "text": "September 05, 2023\n\nIntroductions\nBECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Fill out the First Day of Class Questionnaire\n\n\n Course slides are here."
  },
  {
    "objectID": "schedule.html#september-07-2023",
    "href": "schedule.html#september-07-2023",
    "title": "Schedule",
    "section": "September 07, 2023",
    "text": "September 07, 2023\n\nIntroductions\nBECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Fill out the First Day of Class Questionnaire\n\n\n Course slides are here."
  },
  {
    "objectID": "schedule.html#september-12-2023",
    "href": "schedule.html#september-12-2023",
    "title": "Schedule",
    "section": "September 12, 2023",
    "text": "September 12, 2023\n\nHegemonic Backdrops of Big Data\nEPISTEMOLOGY DISCOURSE ANALYSIS BECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Elish, M. C. and danah boyd (2018). “Situating Methods in the Magic of Big Data and AI”. In: Communication Monographs 85.1, pp. 57-80. (Visited on Sep. 01, 2023). Read in Perusall\n Fill out the Trigger Warnings Questionnaire in Moodle.\n Install Desktop version of Slack and configure notifications for our course.\n Create and share Labor Log\n\n\n Course slides are here\n boyd, danah and Kate Crawford (2012). “Critical Questions for Big Data”. In: Information, Communication & Society 15.5, pp. 662-679. (Visited on Jan. 19, 2018).\n Kitchin, Rob (2014). “Big Data, new epistemologies and paradigm shifts”. En. In: Big Data & Society 1.1, p. 2053951714528481. (Visited on Jul. 16, 2019).\n Leonelli, S. (2014). “What difference does quantity make? On the epistemology of Big Data in biology:”. En. In: Big Data & Society. Publisher: SAGE PublicationsSage UK: London, England. (Visited on Mar. 28, 2020).\n Onuoha, Mimi (2016). The Point of Collection. En. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#september-14-2023",
    "href": "schedule.html#september-14-2023",
    "title": "Schedule",
    "section": "September 14, 2023",
    "text": "September 14, 2023\n\nMetaphors of Big Data\nDISCOURSE ANALYSIS DISCOURSE\n\nDue TodayFurther Resources\n\n\n Levy Karen, Tim Hwang (2015). ‘The Cloud’ and Other Dangerous Metaphors. En. Section: Technology. (Visited on Aug. 29, 2021). Read in Perusall\n Puschmann, Cornelius and Jean Burgess (2014). “Metaphors of Big Data”. En. In: International Journal of Communication 8.0, p. 20. (Visited on May. 02, 2016). Read in Perusall\n Create a GitHub account if you don’t have one\n Click on the Student Portfolio GitHub Repo in Moodle to create your portfolio\n Create and share Labor Log if you haven’t\n Sign-up to take class notes for community labor\n Acknowledge that you’ve read and understand the grading contract by completing the Grading Contract Acknowledgement in Moodle\n\n\n Course slides are here\n Discourse Analysis in Nine Steps is here\n Here is the article we will engage in today’s activity.\n Watson, Sarah M. (2021). Metaphors of Big Data. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#september-19-2023",
    "href": "schedule.html#september-19-2023",
    "title": "Schedule",
    "section": "September 19, 2023",
    "text": "September 19, 2023\n\nBinary Oppositions in Big Data Discourse\nDISCOURSE ANALYSIS INTERPRETING CULTURAL MEANING\n\nDue TodayFurther Resources\n\n\n Complete course infrastructure set-up by following instructions here. Please note that the instructions in your individual repo are outdated. The video links do not work. These links have been updated in the link I just posted.\n Fill out CATME Survey (link sent to your email)\n DM Professor if you’d like to lead a class discussion for enrichment\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#september-21-2023",
    "href": "schedule.html#september-21-2023",
    "title": "Schedule",
    "section": "September 21, 2023",
    "text": "September 21, 2023\n\nScheduling Workshop - Class on Zoom\n\nDue TodayFurther Resources\n\n\n Start working on Team Contract\n\n\n Strategic Plan\n Link to Group Project Repo (enter or select CATME group number)"
  },
  {
    "objectID": "schedule.html#september-26-2023",
    "href": "schedule.html#september-26-2023",
    "title": "Schedule",
    "section": "September 26, 2023",
    "text": "September 26, 2023\n\nThick Data for Big Data\nANALYZING SOCIAL FORCES AND SYSTEMS EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n Fiore-Silfvast, Brittany (2014). Hacked Ethnographic Fieldnotes. En. (Visited on Feb. 18, 2021). Read in Perusall\n Burrell, Jenna (2012). The Ethnographer’s Complete Guide to Big Data: Small Data People in a Big Data World. (Visited on Aug. 20, 2021). Read in Perusall\n DM Professor if you’d like to lead a class discussion for enrichment\n Join Group Project Repo (enter or select CATME group number)\n Start working on Fieldnote 1\n\n\n Course slides are here\n Here’s an example of some very short “thick description” write-ups of two data environments from my own research.\n Wang, Tricia (2013). Big Data Needs Thick Data. (Visited on Sep. 10, 2019)."
  },
  {
    "objectID": "schedule.html#september-28-2023",
    "href": "schedule.html#september-28-2023",
    "title": "Schedule",
    "section": "September 28, 2023",
    "text": "September 28, 2023\n\nEthnography in Data Land\nDATA DOCUMENTATION COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Introduction , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1. Read in Perusall\n Team Contract Due\n Continue working on Fieldnote 1\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#october-03-2023",
    "href": "schedule.html#october-03-2023",
    "title": "Schedule",
    "section": "October 03, 2023",
    "text": "October 03, 2023\n\nDocumenting Datasets\n\nDue TodayFurther Resources\n\n\n Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, et al. (2020). “Datasheets for Datasets”. In: arXiv:1803.09010 [cs]. arXiv: 1803.09010. (Visited on Jan. 24, 2021). Read in Perusall\n Continue working on Fieldnote 1\n\n\n Course slides are here\n Bender, Emily M. and Batya Friedman (2018). “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science”. In: Transactions of the Association for Computational Linguistics 6, pp. 587-604. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#october-05-2023",
    "href": "schedule.html#october-05-2023",
    "title": "Schedule",
    "section": "October 05, 2023",
    "text": "October 05, 2023\n\nEthnographies of Infrastructure\nCULTURAL ANALYSIS OF INFRASTRUCTURE BECOMING OBSERVANT INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Star, Susan Leigh (1999). “The Ethnography of Infrastructure”. En. In: American Behavioral Scientist 43.3, pp. 377-391. (Visited on Feb. 18, 2016). Read in Perusall\n Get approval for dataset\n Fieldnote 1 Due\n Start working on Fieldnote 2\n\n\n Course slides are here\n Lampland, Martha and Susan Leigh Star, ed. (2008). Standards and Their Stories: How Quantifying, Classifying, and Formalizing Practices Shape Everyday Life. 1 edition. Ithaca: Cornell University Press. ISBN: 978-0-8014-7461-3.\n Ottinger, Gwen (2010). “Buckets of Resistance: Standards and the Effectiveness of Citizen Science”. En. In: Science, Technology, & Human Values 35.2, pp. 244-270. (Visited on Oct. 05, 2019).\n Timmermans, Stefan and Steven Epstein (2010). “A World of Standards but not a Standard World: Toward a Sociology of Standards and Standardization*“. In: Annual Review of Sociology 36.1, pp. 69-89. (Visited on Oct. 16, 2014)."
  },
  {
    "objectID": "schedule.html#october-10-2023",
    "href": "schedule.html#october-10-2023",
    "title": "Schedule",
    "section": "October 10, 2023",
    "text": "October 10, 2023\n\nAutumn Recess\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Work on semiotic analysis\n Continue working on Fieldnote 2\n Start working on Mini-Project 1\n Be sure to get approval for the TED Talks you plan to view for Mini-Project 1."
  },
  {
    "objectID": "schedule.html#october-12-2023",
    "href": "schedule.html#october-12-2023",
    "title": "Schedule",
    "section": "October 12, 2023",
    "text": "October 12, 2023\n\nSorting Things Out: Cultural Analyses of Categories\n\nDue TodayFurther Resources\n\n\n Bowker, Geoffrey C. (1998). “The Kindness of Strangers: Kinds and Politics in Classification Systems”. En. In: Library Trends 47.2, pp. 255-292. (Visited on Oct. 14, 2019). Read in Perusall\n Work on semiotic analysis\n Fieldnote 2 Due\n Start working on Fieldnote 3\n Continue working on Mini-Project 1\n\n\n Course slides are here\n Link to Web Archive 1\n Social Advocacy for Racial Classifications here\n More recent docket here\n Bowker, Geoffrey C. and Susan Leigh Star (1999). Sorting Things Out: Classification and Its Consequences. En. Cambridge, MA: MIT Press. ISBN: 978-0-262-52295-3.\n Waterton, Claire (2002). “From Field to Fantasy: Classifying Nature, Constructing Europe”. En. In: Social Studies of Science 32.2, pp. 177-204. (Visited on May. 15, 2019).\n Kirksey, Eben (2015). “Species: a praxiographic study”. Fr. In: Journal of the Royal Anthropological Institute 21.4, pp. 758-780. (Visited on Oct. 05, 2019)."
  },
  {
    "objectID": "schedule.html#october-17-2023",
    "href": "schedule.html#october-17-2023",
    "title": "Schedule",
    "section": "October 17, 2023",
    "text": "October 17, 2023\n\nInfrastructure Field Day\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Work on stakeholder analysis\n Continue working on Fieldnote 3\n Continue working on Mini-Project 1"
  },
  {
    "objectID": "schedule.html#october-19-2023",
    "href": "schedule.html#october-19-2023",
    "title": "Schedule",
    "section": "October 19, 2023",
    "text": "October 19, 2023\n\nData Ghost Work\nINTERVIEWING ANALYZING SOCIAL FORCES AND SYSTEMS LABOR\n\nDue TodayFurther Resources\n\n\n Chapter 1 , Gray, Mary L. and Siddharth Suri (2019). Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass. Illustrated edition. Boston: Mariner Books. ISBN: 978-1-328-56624-9. Read in Perusall\n Work on stakeholder analysis\n Fieldnote 3 Due\n Continue working on Mini-Project 1\n\n\n Course slides are here\n Irani, Lilly (2015). Justice for “Data Janitors”. En-US. (Visited on Dec. 13, 2018).\n Plantin, Jean-Christophe (2019). “Data Cleaners for Pristine Datasets: Visibility and Invisibility of Data Processors in Social Science”. En. In: Science, Technology, & Human Values 44.1. Publisher: SAGE Publications Inc, pp. 52-73. (Visited on Aug. 20, 2021).\n Forsythe, Diana E. (1993). “The Construction of Work in Artificial Intelligence”. En. In: Science, Technology, & Human Values 18.4. Publisher: SAGE Publications Inc, pp. 460-479. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#october-24-2023",
    "href": "schedule.html#october-24-2023",
    "title": "Schedule",
    "section": "October 24, 2023",
    "text": "October 24, 2023\n\nSocial Constructions of Expertise in Data Work\nINTERVIEWING INTERPRETING CULTURAL MEANING LABOR\n\nDue TodayFurther Resources\n\n\n Chapter 2 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1. Read in Perusall\n Work on stakeholder analysis\n Continue working on Mini-Project 1\n\n\n Course slides are here\n Gieryn, Thomas F. (1999). Cultural Boundaries of Science: Credibility on the Line. En. University of Chicago Press. ISBN: 978-0-226-29261-8."
  },
  {
    "objectID": "schedule.html#october-26-2023",
    "href": "schedule.html#october-26-2023",
    "title": "Schedule",
    "section": "October 26, 2023",
    "text": "October 26, 2023\n\nHow Data Domesticates Us: Rituals for Data Cleaning\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Ribes, David and Steven J Jackson (2013). “Data bite man: The work of sustaining a long-term study”. In: Raw data” is an oxymoron. Ed. by Lisa Gitelman. Cambridge, MA: MIT Press, pp. 147-166. Read in Perusall\n Work on ritual analysis\n Mini-Project 1 Due\n\n\n Course slides are here\n Bowker, Geoffrey C. (2000). “Biodiversity Datadiversity”. En. In: Social Studies of Science 30.5, pp. 643-683. (Visited on May. 14, 2014).\n Walford, Antonia (2017). “Raw Data: Making Relations Matter”. En_US. In: Social Analysis 61.2. Publisher: Berghahn Journals Section: Social Analysis, pp. 65-80. (Visited on Aug. 20, 2021).\n Pink, Sarah, Shanti Sumartojo, Deborah Lupton, et al. (2017). “Mundane data: The routines, contingencies and accomplishments of digital living”. En. In: Big Data & Society 4.1. Publisher: SAGE Publications Ltd, p. 2053951717700924. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#october-31-2023",
    "href": "schedule.html#october-31-2023",
    "title": "Schedule",
    "section": "October 31, 2023",
    "text": "October 31, 2023\n\nData Collection Rituals\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Work on ritual analysis\n Group evaluations open\n Start working on Mini-Project 2\n MP 1 Peer Review Submission open\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#november-02-2023",
    "href": "schedule.html#november-02-2023",
    "title": "Schedule",
    "section": "November 02, 2023",
    "text": "November 02, 2023\n\nCromwell Day\n\nDue TodayFurther Resources\n\n\n Work on ritual analysis\n Start working on Fieldnote 4\n Continue working on group evaluations\n MP 1 Peer Review Submission close and assessment opens\n Continue working on Mini-Project 2"
  },
  {
    "objectID": "schedule.html#november-07-2023",
    "href": "schedule.html#november-07-2023",
    "title": "Schedule",
    "section": "November 07, 2023",
    "text": "November 07, 2023\n\nWork on Group Project\n\nDue TodayFurther Resources\n\n\n Group Evaluations Due\n Work on user guide\n Continue working on Fieldnote 4\n Continue working on Mini-Project 2\n Continue working on Peer Review"
  },
  {
    "objectID": "schedule.html#november-09-2023",
    "href": "schedule.html#november-09-2023",
    "title": "Schedule",
    "section": "November 09, 2023",
    "text": "November 09, 2023\n\nWork on Group Project\n\nDue TodayFurther Resources\n\n\n Work on user guide\n Fieldnote 4 Due\n MP 1 Peer Review Submission Due\n Continue working on Mini-Project 2"
  },
  {
    "objectID": "schedule.html#november-14-2023",
    "href": "schedule.html#november-14-2023",
    "title": "Schedule",
    "section": "November 14, 2023",
    "text": "November 14, 2023\n\nInstitutional Incentives\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Work on institutional analysis\n Continue working on Mini-Project 2\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#november-16-2023",
    "href": "schedule.html#november-16-2023",
    "title": "Schedule",
    "section": "November 16, 2023",
    "text": "November 16, 2023\n\nEconomies of Data Production\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Chapter 3 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1. Read in Perusall\n Work on institutional analysis\n Mini-Project 2 Due\n\n\n Gerlitz, Carolin and Anne Helmond (2013). “The like economy: Social buttons and the data-intensive web”. En. In: New Media & Society 15.8. Publisher: SAGE Publications, pp. 1348-1365. (Visited on Aug. 30, 2021).\n Beer, David (2015). “Productive measures: Culture and measurement in the context of everyday neoliberalism”. En. In: Big Data & Society 2.1. Publisher: SAGE Publications Ltd, p. 2053951715578951. (Visited on Aug. 29, 2021)."
  },
  {
    "objectID": "schedule.html#november-21-2023",
    "href": "schedule.html#november-21-2023",
    "title": "Schedule",
    "section": "November 21, 2023",
    "text": "November 21, 2023\n\nMobilizing Data: Making Numbers Actionable\nDISCOURSE ANALYSIS SITUATING KNOWLEDGE MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Ottinger, Gwen and Rachel Zurer (2011). New Voices, New Approaches: Drowning in Data. En-US. (Visited on Dec. 13, 2018). Read in Perusall\n Work on discourse analysis\n\n\n Course slides are here.\n Pine, Kathleen H. and Max Liboiron (2015). “The Politics of Measurement and Action”. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery, pp. 3147-3156. ISBN: 978-1-4503-3145-6. (Visited on Aug. 30, 2021).\n Dourish, Paul and Edgar Gómez Cruz (2018). “Datafication and data fiction: Narrating data and narrating with data”. En. In: Big Data & Society 5.2. Publisher: SAGE Publications Ltd, p. 2053951718784083. (Visited on Apr. 05, 2021)."
  },
  {
    "objectID": "schedule.html#november-23-2023",
    "href": "schedule.html#november-23-2023",
    "title": "Schedule",
    "section": "November 23, 2023",
    "text": "November 23, 2023\n\nThanksgiving Break\n\nDue TodayFurther Resources\n\n\n Work on discourse analysis"
  },
  {
    "objectID": "schedule.html#november-28-2023",
    "href": "schedule.html#november-28-2023",
    "title": "Schedule",
    "section": "November 28, 2023",
    "text": "November 28, 2023\n\nMobilizing Data Otherwise: Citizen Science and Sensing\nDISCOURSE ANALYSIS COMMUNICATING (IN) CONTEXT MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Gabrys, Jennifer, Helen Pritchard, and Benjamin Barratt (2016). “Just good enough data: Figuring data citizenships through air pollution sensing and data stories”. En. In: Big Data & Society 3.2. Publisher: SAGE Publications Ltd, p. 2053951716679677. (Visited on Mar. 28, 2020). Read in Perusall\n Work on user guide\n Start working on Mini-Project Revisions\n Peer Review Submission Opens\n\n\n Course slides are here.\n Calvillo, Nerea (2018). “Political airs: From monitoring to attuned sensing air pollution”. En. In: Social Studies of Science 48.3, pp. 372-388. (Visited on Sep. 24, 2019).\n Jalbert, Kirk and Abby J. Kinchy (2016). “Sense and Influence: Environmental Monitoring Tools and the Power of Citizen Science”. In: Journal of Environmental Policy & Planning 18.3. Publisher: Routledge _eprint: https://doi.org/10.1080/1523908X.2015.1100985, pp. 379-397. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#november-30-2023",
    "href": "schedule.html#november-30-2023",
    "title": "Schedule",
    "section": "November 30, 2023",
    "text": "November 30, 2023\n\nData Activism and Advocacy\nDISCOURSE SITUATING KNOWLEDGE MOBILIZATION CREDIBILITY\n\nDue TodayFurther Resources\n\n\n Liboiron, Max (2015). “Disaster Data, Data Activism : Grassroots Responses to Representing Superstorm Sandy”. En. In: Extreme Weather and Global Media. Ed. by Julia Leyda and Diane Negra. Taylor & Francis Group. (Visited on Aug. 27, 2019). Read in Perusall\n Work on user guide\n Continue working on Mini-Project Revisions\n Start working on Fieldnote 5\n Peer Review Submission Closes and Assessment Opens\n\n\n Course slides are here\n Bruno, Isabelle, Emmanuel Didier, and Tommaso Vitale (2014). Statactivism: Forms of Action between Disclosure and Affirmation. En. SSRN Scholarly Paper ID 2466882. Rochester, NY: Social Science Research Network. (Visited on Dec. 18, 2018).\n Milan, Stefania and Lonneke van der Velden (2016). “The Alternative Epistemologies of Data Activism”. In: Digital Culture & Society 2.2, pp. 57-74. (Visited on Jul. 16, 2019).\n Currie, Morgan, Britt S Paris, Irene Pasquetto, et al. (2016). “The conundrum of police officer-involved homicides: Counter-data in Los Angeles County”. En. In: Big Data & Society 3.2, p. 2053951716663566. (Visited on Aug. 08, 2018)."
  },
  {
    "objectID": "schedule.html#december-05-2023",
    "href": "schedule.html#december-05-2023",
    "title": "Schedule",
    "section": "December 05, 2023",
    "text": "December 05, 2023\n\nData Agnotology: Ignorance and Knowledge Gaps\nEPISTEMOLOGY IGNORANCE SITUATING KNOWLEDGE\n\nDue TodayFurther Resources\n\n\n mimimimimi (2021). On Missing Data Sets. original-date: 2016-02-03T16:30:28Z. (Visited on Aug. 20, 2021). Read in Perusall\n Milan, Stefania and Emiliano Treré (2020). “The Rise of the Data Poor: The COVID-19 Pandemic Seen From the Margins”. En. In: Social Media + Society 6.3. Publisher: SAGE Publications Ltd, p. 2056305120948233. (Visited on Aug. 31, 2021). Read in Perusall\n First Draft Due\n Continue working on Mini-Project Revisions\n Continue working on Fieldnote 5\n Peer Review Assessment Due\n\n\n Course slides are here\n D’Ignazio, Catherine and Lauren F. Klein (2020). Data Feminism. Cambridge, Massachusetts: The MIT Press. ISBN: 978-0-262-04400-4."
  },
  {
    "objectID": "schedule.html#december-07-2023",
    "href": "schedule.html#december-07-2023",
    "title": "Schedule",
    "section": "December 07, 2023",
    "text": "December 07, 2023\n\nData and Algorithmic Power\nPOWER SITUATING KNOWLEDGE EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n Eubanks, Virginia (2018). “A Child Abuse Prediction Model Fails Poor Families”. In: Wired. (Visited on Mar. 28, 2019). Read in Perusall\n Fieldnote 5 Due\n Continue working on Mini-Project Revisions\n\n\n Brayne, Sarah (2017). “Big Data Surveillance: The Case of Policing”. In: American Sociological Review 82.5. Publisher: SAGE Publications Inc, pp. 977-1008. (Visited on Aug. 18, 2021).\n Christin, Angèle (2020). “The ethnographer and the algorithm: beyond the black box”. En. In: Theory and Society 49.5, pp. 897-918. (Visited on Aug. 31, 2021).\n Seaver, Nick (2017). “Algorithms as culture: Some tactics for the ethnography of algorithmic systems”. En. In: Big Data & Society 4.2. Publisher: SAGE Publications Ltd, p. 2053951717738104. (Visited on Jan. 22, 2021)."
  },
  {
    "objectID": "schedule.html#december-12-2023",
    "href": "schedule.html#december-12-2023",
    "title": "Schedule",
    "section": "December 12, 2023",
    "text": "December 12, 2023\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Work on user guide revisions\n MP Revisions Due"
  },
  {
    "objectID": "schedule.html#december-14-2023",
    "href": "schedule.html#december-14-2023",
    "title": "Schedule",
    "section": "December 14, 2023",
    "text": "December 14, 2023\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Final Project Due\n Enrichment Due\n Community Labor Due"
  },
  {
    "objectID": "schedule.html#calendar-view",
    "href": "schedule.html#calendar-view",
    "title": "Schedule",
    "section": "Calendar View",
    "text": "Calendar View"
  },
  {
    "objectID": "schedule.html#date-view",
    "href": "schedule.html#date-view",
    "title": "Schedule",
    "section": "Date View",
    "text": "Date View\n\nSeptember 05, 2023\n\nIntroductions\nBECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n Fill out the First Day of Class Questionnaire\n\n\n Course slides are here.\n\n\n\n\n\n\nSeptember 07, 2023\n\nHegemonic Backdrops of Big Data\nEPISTEMOLOGY DISCOURSE ANALYSIS BECOMING OBSERVANT\n\nDue TodayFurther Resources\n\n\n boyd, danah and Kate Crawford (2012). “Critical Questions for Big Data”. In: Information, Communication & Society 15.5, pp. 662-679. (Visited on Jan. 19, 2018).\n Fill out the Trigger Warnings Questionnaire in Moodle.\n Install Desktop version of Slack and configure notifications for our course.\n\n\n Course slides are here\n Kitchin, Rob (2014). “Big Data, new epistemologies and paradigm shifts”. En. In: Big Data & Society 1.1, p. 2053951714528481. (Visited on Jul. 16, 2019).\n Leonelli, S. (2014). “What difference does quantity make? On the epistemology of Big Data in biology:”. En. In: Big Data & Society. Publisher: SAGE PublicationsSage UK: London, England. (Visited on Mar. 28, 2020).\n Onuoha, Mimi (2016). The Point of Collection. En. (Visited on Aug. 20, 2021).\n\n\n\n\n\n\nSeptember 12, 2023\n\nMetaphors of Big Data\nDISCOURSE ANALYSIS DISCOURSE\n\nDue TodayFurther Resources\n\n\n Levy Karen, Tim Hwang (2015). ‘The Cloud’ and Other Dangerous Metaphors. En. Section: Technology. (Visited on Aug. 29, 2021).\n Puschmann, Cornelius and Jean Burgess (2014). “Metaphors of Big Data”. En. In: International Journal of Communication 8.0, p. 20. (Visited on May. 02, 2016).\n Create a GitHub account if you don’t have one\n Click on the Student Portfolio GitHub Repo in Moodle to create your portfolio\n Acknowledge that you’ve read and understand the grading contract by completing the Grading Contract Acknowledgement in Moodle\n\n\n Course slides are here\n Here is the article we will engage in today’s activity.\n Discourse Analysis in Nine Steps is here.\n Watson, Sarah M. (2021). Metaphors of Big Data. (Visited on Aug. 30, 2021).\n\n\n\n\n\n\nSeptember 14, 2023\n\nBinary Oppositions in Big Data Discourse\nDISCOURSE ANALYSIS INTERPRETING CULTURAL MEANING\n\nDue TodayFurther Resources\n\n\n Complete course infrastructure set-up by following instructions in the setting-up-r-environment directory in your GitHub portfolio\n DM Professor if you’d like to lead a class discussion for enrichment\n\n\n Course slides are here\n\n\n\n\n\n\nSeptember 19, 2023\n\nThick Data for Big Data\nTHICK DESCRIPTION BECOMING OBSERVANT COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Fiore-Silfvast, Brittany (2014). Hacked Ethnographic Fieldnotes. En. (Visited on Feb. 18, 2021).\n Burrell, Jenna (2012). The Ethnographer’s Complete Guide to Big Data: Small Data People in a Big Data World. (Visited on Aug. 20, 2021).\n Fill out CATME Survey (link sent to your email)\n DM Professor if you’d like to lead a class discussion for enrichment\n Start working on Fieldnote 1\n\n\n Course slides are here\n Here’s an example of some very short “thick description” write-ups of two data environments from my own research.\n Wang, Tricia (2013). Big Data Needs Thick Data. (Visited on Sep. 10, 2019).\n\n\n\n\n\n\nSeptember 21, 2023\n\nScheduling Workshop - Class on Zoom\n\nDue TodayFurther Resources\n\n\n Start working on Team Contract\n Continue working on Fieldnote 1\n\n\n\n\n\n\n\n\n\nSeptember 26, 2023\n\nEthnography in Data Land\nANALYZING SOCIAL FORCES AND SYSTEMS EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n Introduction , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n Team Contract Due\n Fieldnote 1 Due\n\n\n Course slides are here\n\n\n\n\n\n\nSeptember 28, 2023\n\nDocumenting Datasets\nDATA DOCUMENTATION COMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, et al. (2020). “Datasheets for Datasets”. In: arXiv:1803.09010 [cs]. arXiv: 1803.09010. (Visited on Jan. 24, 2021).\n Get approval for dataset\n Start working on Fieldnote 2\n\n\n Course slides are here\n Bender, Emily M. and Batya Friedman (2018). “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science”. In: Transactions of the Association for Computational Linguistics 6, pp. 587-604. (Visited on Aug. 20, 2021).\n\n\n\n\n\n\nOctober 03, 2023\n\nMaking Measures Commensurate: Translation and Reductionism\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING TRANSLATION INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Chapter 1 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n Continue working on Fieldnote 2\n\n\n Course slides are here\n Activity link is here and here and here\n Espeland, Wendy Nelson and Mitchell L. Stevens (1998). “Commensuration as a Social Process”. In: Annual Review of Sociology 24.1. Publisher: Annual Reviews, pp. 313-343. (Visited on Aug. 30, 2021).\n Merry, Sally Engle (2016). The Seductions of Quantification: Measuring Human Rights, Gender Violence, and Sex Trafficking. En. Google-Books-ID: 0FcqDAAAQBAJ. University of Chicago Press. ISBN: 978-0-226-26131-7.\n\n\n\n\n\n\nOctober 05, 2023\n\nEthnographies of Infrastructure\nCULTURAL ANALYSIS OF INFRASTRUCTURE BECOMING OBSERVANT INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Star, Susan Leigh (1999). “The Ethnography of Infrastructure”. En. In: American Behavioral Scientist 43.3, pp. 377-391. (Visited on Feb. 18, 2016).\n Fieldnote 2 Due\n\n\n Course slides are here\n Lampland, Martha and Susan Leigh Star, ed. (2008). Standards and Their Stories: How Quantifying, Classifying, and Formalizing Practices Shape Everyday Life. 1 edition. Ithaca: Cornell University Press. ISBN: 978-0-8014-7461-3.\n Ottinger, Gwen (2010). “Buckets of Resistance: Standards and the Effectiveness of Citizen Science”. En. In: Science, Technology, & Human Values 35.2, pp. 244-270. (Visited on Oct. 05, 2019).\n Timmermans, Stefan and Steven Epstein (2010). “A World of Standards but not a Standard World: Toward a Sociology of Standards and Standardization*“. In: Annual Review of Sociology 36.1, pp. 69-89. (Visited on Oct. 16, 2014).\n\n\n\n\n\n\nOctober 10, 2023\n\nSorting Things Out: Cultural Analyses of Categories\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Bowker, Geoffrey C. (1998). “The Kindness of Strangers: Kinds and Politics in Classification Systems”. En. In: Library Trends 47.2, pp. 255-292. (Visited on Oct. 14, 2019).\n Work on semiotic analysis\n Start working on Mini-Project 1\n Be sure to get approval for the TED Talks you plan to view for Mini-Project 1.\n\n\n Bowker, Geoffrey C. and Susan Leigh Star (1999). Sorting Things Out: Classification and Its Consequences. En. Cambridge, MA: MIT Press. ISBN: 978-0-262-52295-3.\n Waterton, Claire (2002). “From Field to Fantasy: Classifying Nature, Constructing Europe”. En. In: Social Studies of Science 32.2, pp. 177-204. (Visited on May. 15, 2019).\n Kirksey, Eben (2015). “Species: a praxiographic study”. Fr. In: Journal of the Royal Anthropological Institute 21.4, pp. 758-780. (Visited on Oct. 05, 2019).\n\n\n\n\n\n\nOctober 12, 2023\n\nAutumn Recess\n\nDue TodayFurther Resources\n\n\n Work on semiotic analysis\n Start working on Fieldnote 3\n Continue working on Mini-Project 1\n\n\n\n\n\n\n\n\n\nOctober 17, 2023\n\nInfrastructure Field Day\nCULTURAL ANALYSIS OF INFRASTRUCTURE INTERPRETING CULTURAL MEANING INFRASTRUCTURES\n\nDue TodayFurther Resources\n\n\n Work on stakeholder analysis\n Continue working on Fieldnote 3\n Continue working on Mini-Project 1\n\n\n\n\n\n\n\n\n\nOctober 19, 2023\n\nData Ghost Work\nINTERVIEWING ANALYZING SOCIAL FORCES AND SYSTEMS LABOR\n\nDue TodayFurther Resources\n\n\n Chapter 1 , Gray, Mary L. and Siddharth Suri (2019). Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass. Illustrated edition. Boston: Mariner Books. ISBN: 978-1-328-56624-9.\n Work on stakeholder analysis\n Fieldnote 3 Due\n Continue working on Mini-Project 1\n\n\n Course slides are here\n Irani, Lilly (2015). Justice for “Data Janitors”. En-US. (Visited on Dec. 13, 2018).\n Plantin, Jean-Christophe (2019). “Data Cleaners for Pristine Datasets: Visibility and Invisibility of Data Processors in Social Science”. En. In: Science, Technology, & Human Values 44.1. Publisher: SAGE Publications Inc, pp. 52-73. (Visited on Aug. 20, 2021).\n Forsythe, Diana E. (1993). “The Construction of Work in Artificial Intelligence”. En. In: Science, Technology, & Human Values 18.4. Publisher: SAGE Publications Inc, pp. 460-479. (Visited on Aug. 20, 2021).\n\n\n\n\n\n\nOctober 24, 2023\n\nSocial Constructions of Expertise in Data Work\nINTERVIEWING INTERPRETING CULTURAL MEANING LABOR\n\nDue TodayFurther Resources\n\n\n Chapter 2 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n Work on stakeholder analysis\n Continue working on Mini-Project 1\n\n\n Gieryn, Thomas F. (1999). Cultural Boundaries of Science: Credibility on the Line. En. University of Chicago Press. ISBN: 978-0-226-29261-8.\n\n\n\n\n\n\nOctober 26, 2023\n\nHow Data Domesticates Us: Rituals for Data Cleaning\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Ribes, David and Steven J Jackson (2013). “Data bite man: The work of sustaining a long-term study”. In: Raw data” is an oxymoron. Ed. by Lisa Gitelman. Cambridge, MA: MIT Press, pp. 147-166.\n Work on ritual analysis\n Mini-Project 1 Due\n\n\n Course slides are here\n Bowker, Geoffrey C. (2000). “Biodiversity Datadiversity”. En. In: Social Studies of Science 30.5, pp. 643-683. (Visited on May. 14, 2014).\n Walford, Antonia (2017). “Raw Data: Making Relations Matter”. En_US. In: Social Analysis 61.2. Publisher: Berghahn Journals Section: Social Analysis, pp. 65-80. (Visited on Aug. 20, 2021).\n Pink, Sarah, Shanti Sumartojo, Deborah Lupton, et al. (2017). “Mundane data: The routines, contingencies and accomplishments of digital living”. En. In: Big Data & Society 4.1. Publisher: SAGE Publications Ltd, p. 2053951717700924. (Visited on Aug. 30, 2021).\n\n\n\n\n\n\nOctober 31, 2023\n\nData Collection Rituals\nPARTICIPANT OBSERVATION COMMUNICATING (IN) CONTEXT RITUALS\n\nDue TodayFurther Resources\n\n\n Work on ritual analysis\n Group evaluations open\n Start working on Mini-Project 2\n\n\n Course slides are here\n\n\n\n\n\n\nNovember 02, 2023\n\nCromwell Day\n\nDue TodayFurther Resources\n\n\n Work on ritual analysis\n Start working on Fieldnote 4\n Continue working on Mini-Project 2\n\n\n\n\n\n\n\n\n\nNovember 07, 2023\n\nWork on Group Project\n\nDue TodayFurther Resources\n\n\n Group Evaluations Due\n Work on user guide\n Continue working on Fieldnote 4\n Continue working on Mini-Project 2\n\n\n\n\n\n\n\n\n\nNovember 09, 2023\n\nWork on Group Project\n\nDue TodayFurther Resources\n\n\n Work on user guide\n Fieldnote 4 Due\n Continue working on Mini-Project 2\n\n\n\n\n\n\n\n\n\nNovember 14, 2023\n\nInstitutional Incentives\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Work on institutional analysis\n Continue working on Mini-Project 2\n\n\n Course slides are here\n\n\n\n\n\n\nNovember 16, 2023\n\nEconomies of Data Production\nANALYZING SOCIAL FORCES AND SYSTEMS INCENTIVES\n\nDue TodayFurther Resources\n\n\n Chapter 3 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1.\n Work on institutional analysis\n Mini-Project 2 Due\n\n\n Gerlitz, Carolin and Anne Helmond (2013). “The like economy: Social buttons and the data-intensive web”. En. In: New Media & Society 15.8. Publisher: SAGE Publications, pp. 1348-1365. (Visited on Aug. 30, 2021).\n Beer, David (2015). “Productive measures: Culture and measurement in the context of everyday neoliberalism”. En. In: Big Data & Society 2.1. Publisher: SAGE Publications Ltd, p. 2053951715578951. (Visited on Aug. 29, 2021).\n\n\n\n\n\n\nNovember 21, 2023\n\nMobilizing Data: Making Numbers Actionable\nDISCOURSE ANALYSIS SITUATING KNOWLEDGE MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Ottinger, Gwen and Rachel Zurer (2011). New Voices, New Approaches: Drowning in Data. En-US. (Visited on Dec. 13, 2018).\n Work on discourse analysis\n\n\n Course slides are here.\n Pine, Kathleen H. and Max Liboiron (2015). “The Politics of Measurement and Action”. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery, pp. 3147-3156. ISBN: 978-1-4503-3145-6. (Visited on Aug. 30, 2021).\n Dourish, Paul and Edgar Gómez Cruz (2018). “Datafication and data fiction: Narrating data and narrating with data”. En. In: Big Data & Society 5.2. Publisher: SAGE Publications Ltd, p. 2053951718784083. (Visited on Apr. 05, 2021).\n\n\n\n\n\n\nNovember 23, 2023\n\nThanksgiving Break\n\nDue TodayFurther Resources\n\n\n Work on discourse analysis\n\n\n\n\n\n\n\n\n\nNovember 28, 2023\n\nMobilizing Data Otherwise: Citizen Science and Sensing\nDISCOURSE ANALYSIS COMMUNICATING (IN) CONTEXT MOBILIZATION\n\nDue TodayFurther Resources\n\n\n Gabrys, Jennifer, Helen Pritchard, and Benjamin Barratt (2016). “Just good enough data: Figuring data citizenships through air pollution sensing and data stories”. En. In: Big Data & Society 3.2. Publisher: SAGE Publications Ltd, p. 2053951716679677. (Visited on Mar. 28, 2020).\n Work on user guide\n Start working on Mini-Project Revisions\n\n\n Course slides are here.\n Calvillo, Nerea (2018). “Political airs: From monitoring to attuned sensing air pollution”. En. In: Social Studies of Science 48.3, pp. 372-388. (Visited on Sep. 24, 2019).\n Jalbert, Kirk and Abby J. Kinchy (2016). “Sense and Influence: Environmental Monitoring Tools and the Power of Citizen Science”. In: Journal of Environmental Policy & Planning 18.3. Publisher: Routledge _ eprint: https://doi.org/10.1080/1523908X.2015.1100985, pp. 379-397. (Visited on Aug. 30, 2021).\n\n\n\n\n\n\nNovember 30, 2023\n\nData Activism and Advocacy\nDISCOURSE SITUATING KNOWLEDGE MOBILIZATION CREDIBILITY\n\nDue TodayFurther Resources\n\n\n Liboiron, Max (2015). “Disaster Data, Data Activism : Grassroots Responses to Representing Superstorm Sandy”. En. In: Extreme Weather and Global Media. Ed. by Julia Leyda and Diane Negra. Taylor & Francis Group. (Visited on Aug. 27, 2019).\n Work on user guide\n Continue working on Mini-Project Revisions\n Start working on Fieldnote 5\n\n\n Bruno, Isabelle, Emmanuel Didier, and Tommaso Vitale (2014). Statactivism: Forms of Action between Disclosure and Affirmation. En. SSRN Scholarly Paper ID 2466882. Rochester, NY: Social Science Research Network. (Visited on Dec. 18, 2018).\n Milan, Stefania and Lonneke van der Velden (2016). “The Alternative Epistemologies of Data Activism”. In: Digital Culture & Society 2.2, pp. 57-74. (Visited on Jul. 16, 2019).\n Currie, Morgan, Britt S Paris, Irene Pasquetto, et al. (2016). “The conundrum of police officer-involved homicides: Counter-data in Los Angeles County”. En. In: Big Data & Society 3.2, p. 2053951716663566. (Visited on Aug. 08, 2018).\n\n\n\n\n\n\nDecember 05, 2023\n\nData Agnotology: Ignorance and Knowledge Gaps\nEPISTEMOLOGY IGNORANCE SITUATING KNOWLEDGE\n\nDue TodayFurther Resources\n\n\n mimimimimi (2021). On Missing Data Sets. original-date: 2016-02-03T16:30:28Z. (Visited on Aug. 20, 2021).\n Milan, Stefania and Emiliano Treré (2020). “The Rise of the Data Poor: The COVID-19 Pandemic Seen From the Margins”. En. In: Social Media + Society 6.3. Publisher: SAGE Publications Ltd, p. 2056305120948233. (Visited on Aug. 31, 2021).\n First Draft Due\n Continue working on Mini-Project Revisions\n Continue working on Fieldnote 5\n\n\n Course slides are here\n D’Ignazio, Catherine and Lauren F. Klein (2020). Data Feminism. Cambridge, Massachusetts: The MIT Press. ISBN: 978-0-262-04400-4.\n\n\n\n\n\n\nDecember 07, 2023\n\nData and Algorithmic Power\nPOWER SITUATING KNOWLEDGE EVALUATING ETHICAL DILEMMAS\n\nDue TodayFurther Resources\n\n\n Eubanks, Virginia (2018). “A Child Abuse Prediction Model Fails Poor Families”. In: Wired. (Visited on Mar. 28, 2019).\n Fieldnote 5 Due\n Continue working on Mini-Project Revisions\n\n\n Brayne, Sarah (2017). “Big Data Surveillance: The Case of Policing”. In: American Sociological Review 82.5. Publisher: SAGE Publications Inc, pp. 977-1008. (Visited on Aug. 18, 2021).\n Christin, Angèle (2020). “The ethnographer and the algorithm: beyond the black box”. En. In: Theory and Society 49.5, pp. 897-918. (Visited on Aug. 31, 2021).\n Seaver, Nick (2017). “Algorithms as culture: Some tactics for the ethnography of algorithmic systems”. En. In: Big Data & Society 4.2. Publisher: SAGE Publications Ltd, p. 2053951717738104. (Visited on Jan. 22, 2021).\n\n\n\n\n\n\nDecember 12, 2023\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Work on user guide revisions\n MP Revisions Due\n\n\n\n\n\n\n\n\n\nDecember 14, 2023\n\nFinal Projects\nCOMMUNICATING (IN) CONTEXT\n\nDue TodayFurther Resources\n\n\n Final Project Due\n Enrichment Due\n Community Labor Due"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Our course syllabus is a contract that identifies expectations that I have of you, expectations you can have of me, and how we can effectively use resources to create an impactful learning environment. It serves you to know the syllabus inside and out; knowing the syllabus means that you are aware of resources available to you if you are struggling and that you won’t be surprised by course policies that may negatively impact your grade. It also serves the whole class to have everyone know the course syllabus; knowing the syllabus sets standards for how we will learn as a community, and it also reduces the class time spent answering questions that have already been documented.\nAfter reading the syllabus, you should complete the 20 question syllabus quiz in Perusall. This syllabus quiz is graded on correctness. Please note that this is an open note quiz, and you may work with other students on this quiz."
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#new-epistemologies-of-big-data",
    "href": "slides/Day2-Epstemologies.html#new-epistemologies-of-big-data",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "",
    "text": "“End of theory” and the need for experimentation\nKnowledge creation is purely inductive\nData can “speak for itself”; it is self-evident\nPatterns in data are inherently meaningful\nMeaning presented by data transcends context\n\n…according to (Kitchin 2013)"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day3-BigDataDiscourse.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Three: Big Data Discourse",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nWhat assumptions are built into this metaphor?\nWhat are the some social consequences of associating data with this idea?"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#reading-discussion",
    "href": "slides/Day3-BigDataDiscourse.html#reading-discussion",
    "title": "Day Three: Big Data Discourse",
    "section": "Reading Discussion",
    "text": "Reading Discussion\n\nWhat connotations are wrapped into the metaphors often used to characterize data? Why do the metaphors we use to describe data matter?\n\n\nAssign someone to take notes and someone else to facilitate.\nTo start, everyone will take a turn to offer their brief initial perspective.\nOpen for broader discussion. The facilitator should keep time and ensure the conversation stays on topic.\nPrepare two bullet points that summarize the key takeaways from your discussion to report out."
  },
  {
    "objectID": "slides/Day5-ThickDescription.html#thursdays-reading",
    "href": "slides/Day5-ThickDescription.html#thursdays-reading",
    "title": "Day Five: Thick Data for Big Data",
    "section": "Thursday’s Reading",
    "text": "Thursday’s Reading\n\nWhat are Biruk’s research questions?\nWhat does Biruk make known about her assumptions/beliefs/values as she begins to interpret data?"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#common-rule",
    "href": "slides/Day6-Reflexivity.html#common-rule",
    "title": "Day Six: Reflexivity",
    "section": "Common Rule",
    "text": "Common Rule\n\nIn the US, research organizations receiving federal funding are subject to the Common Rule\n\nLaws and regulations regarding how Institutional Review Boards are to operate\nIRBs are organizational bodies that review the ethics of human subjects research in order to protect human welfare, rights, and privacy before a study gets carried out\n\nUsually composed of representatives from an institution with a diverse background"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#tuskegee-study",
    "href": "slides/Day6-Reflexivity.html#tuskegee-study",
    "title": "Day Six: Reflexivity",
    "section": "Tuskegee Study",
    "text": "Tuskegee Study\n\nConducted from 1932 to 1972 by US Public Health Services and Center for Disease Control, in collaboration with Tuskegee University in Alabama\nInvolved 400 African Americans with syphilis\nStudy of leaving the disease untreated even though it was treatable\n\nWere told they would receive free medical care\nNever informed of their diagnosis\nProvided with placebos and ineffective methods"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#facebooks-emotional-contagion-study",
    "href": "slides/Day6-Reflexivity.html#facebooks-emotional-contagion-study",
    "title": "Day Six: Reflexivity",
    "section": "Facebook’s Emotional Contagion Study",
    "text": "Facebook’s Emotional Contagion Study\n\nJanuary 2012, Facebook data scientists manipulated what 700,000 users saw on feeds to examine emotional contagion\n\nSome shown happy, positive content\nOthers shown sad, negative content\n\nLegal?\nEthical?"
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#ethics-of-ethnographic-research",
    "href": "slides/Day6-Reflexivity.html#ethics-of-ethnographic-research",
    "title": "Day Six: Reflexivity",
    "section": "Ethics of ethnographic research",
    "text": "Ethics of ethnographic research\n\nEthnography different than many other forms of human subjects research in that it takes place in natural settings vs in clinical settings\nQuestion of how to balance benefits of the research with the potential harms posed to the participants\nPotential harms:\n\nReputation\nDisclosure of personal information\nDisruption of relationships\nLoss of claims"
  },
  {
    "objectID": "slides/Day9-DataDocumentation.html#discussion-of-final-project",
    "href": "slides/Day9-DataDocumentation.html#discussion-of-final-project",
    "title": "Day Nine: Data Documentation",
    "section": "Discussion of Final Project",
    "text": "Discussion of Final Project"
  },
  {
    "objectID": "example_fieldnotes.html",
    "href": "example_fieldnotes.html",
    "title": "Example Field Notes",
    "section": "",
    "text": "This entry documents a data environment Casey MacGibbon observed on 2023-02-10 in The Human Performance Labratory, Scott Gym, Smith College. The observations were written up on 2023-02-19.\nMy stopwatch hits 20:00:00.\n“Alright, it’s been five minutes since I last asked, how rigorous do you feel your exercise is on a scale of 6-20?” I say, holding up a reference board for the participant. She points to a 12, which is labeled as “Somewhat hard.” She is on minute twenty of her acute bout of exercise, where she is walking at a moderate pace on the treadmill, and we are monitoring her heart rate from an electronic wristband.\nRecently, I have begun research work and analysis for the Witkowski Vascular Function Lab, and this was the first time I went in to observe a visit with a participant. I am new to the lab, and my perspective still feels a bit like an outsiders’. After the visit, I asked Dr. Witkowski explained that each number on the reference board corresponded to a heart rate by multiplying the number by 10, essentially acting as a participant’s heart rate experience. She explained to me how there are two different “data streams” throughout the lab, one being objective and the other subjective.\nThe idea of objective/subjective data is an example of a binary opposition: a pair of terms with opposite meanings. I noticed how much more “objective” data collection was valued, while a lot of the data collected from the participant was thought of as “subjective,” and thought of to be more subject to inaccuracy. Data is categorized as one or the other.\nNo data can be subjective, because the mere decision to begin collecting data means that there are expectations and biases put into the collection of data. Though the target heart rate is an “objective” part of the study because it is numerical, does not mean it is without bias. Heart rate calculations have historically ignored weight, height, average physical activity, and many other factors. This is just one example of how all of the data is biased, and cannot be placed into these categorizations. Additionally, the “data streams” figure of speech is a part of popular data discourse, contributing to the idea that data is a natural resource waiting to be found, untainted and untouched by human interference.\nDiscourse is the way that people talk about and engage with a certain thing/person, and dominant discourse can often characterize how we think about this topic as a society. The imagery of a data stream indicates that data is natural and is something we can “drink” from at any time. In reality, all data is collected from humans, who are limited in their standpoints and views."
  },
  {
    "objectID": "example_fieldnotes.html#sds-237-spring-23-elm-markert",
    "href": "example_fieldnotes.html#sds-237-spring-23-elm-markert",
    "title": "Example Field Notes",
    "section": "SDS 237 Spring ’23: Elm Markert",
    "text": "SDS 237 Spring ’23: Elm Markert\nThis entry documents a data environment Elm Markert observed on 2023-03-24 in CMB and email inbox. The observations were written up on 2023-03-29.\n\nPlease note that certain details in this entry were anonymized for the purposes of sharing. Names and titles have been changed.\n\nThe Center [redacted] is on the first floor of [redacted] down a long hall of labs. I go there regularly for my research project and am always greeted with a friendly face – Morgan. They’re the technical director of the Center and help students with research. This includes teaching students to use instruments that some senior researchers don’t even get their hands on. They’re a patient teacher and helpful during experiments. They’re also a core part of a lot of the data labor (work that goes into creating, supporting, processing, and analyzing information) that happens in my research field.\nWhile Morgan is highly valued on a personal amongst colleagues, on an institutional level, they disappear a little bit. While most people outside the department recognize the research professors do, Morgan fades into the background a little more. They also don’t get to do their own research, and the Center’s budget is generally devoted to making sure professors get what they need for their research. As a result, despite the amount that they help with student research, Morgan rarely gets put on papers as an author.\nHowever, Morgan does love their job. They’re excellent at training people on equipment and prefer being technical support to lecturing. They know the ins and outs of the instruments and do more hands-on work than most professors. Data produced in the Center is as accurate and uncontaminated as possible because of the care that they take when working. The data provides an excellent basis for publications. However, the labor that went into creating that data is hidden in the final publication.\nThis obscuration of labor says a lot about what kind of labor academia values. Spoiler alert: it’s not “grunt” work. Instead, it values high level thinking. This ignores that the “grunt” work also requires expertise and is vital to the research that professors do. As an undergraduate researcher who gets to be first author but also is doing the “grunt” work, it is easy to see this discrepancy. I do actively benefit from this hierarchy in many ways because of my educational and career trajectory, but it appears that the more one benefits, the harder it is to notice. This is partially because the people who benefit most often don’t do much lab work.\n…I will add more as I get permission from former students."
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#relationality-of-infrastructure",
    "href": "slides/Day11-Infrastructure.html#relationality-of-infrastructure",
    "title": "Day Eight: Ethnographies of Infrastructure",
    "section": "Relationality of Infrastructure",
    "text": "Relationality of Infrastructure\n\ninfrastructue is a fundamentally relational concept, becoming real infrastructure in relation to organized practice.\n\n\nOften we think of infrastructure as “just there”\nAssumes that infrastructure is the same thing for all people all of the time\nViewing infrastructure as “relational” acknowledges that certain things become infrastructure to different people in different moments\nNot so much “what is infrastructure” but “when is infrastructure”"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#master-narratives",
    "href": "slides/Day11-Infrastructure.html#master-narratives",
    "title": "Day Eight: Ethnographies of Infrastructure",
    "section": "Master Narratives",
    "text": "Master Narratives\n\nSingular voice designed into infrastructures that do not attend to diversity\nAssumed to speak for everyone\nIdentifying involves asking: what has been made other by the design of this infrastructure?"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#invisible-work",
    "href": "slides/Day11-Infrastructure.html#invisible-work",
    "title": "Day Eight: Ethnographies of Infrastructure",
    "section": "Invisible Work",
    "text": "Invisible Work\n\nWork that fades into the background when presenting an infrastructure\nSometimes invisible because that work is “tacit” or rote\nSometimes structurally hidden from view\nIdentifying involves asking: what forms of labor tend to go unrecognized when looking at this infrastructure"
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#bringing-an-ethnographic-sensibility-to-archives",
    "href": "slides/Day11-Infrastructure.html#bringing-an-ethnographic-sensibility-to-archives",
    "title": "Day Eleven: Ethnographies of Infrastructure",
    "section": "Bringing an Ethnographic Sensibility to Archives",
    "text": "Bringing an Ethnographic Sensibility to Archives\n\nSwitch slide deck if there is time…"
  },
  {
    "objectID": "slides/Day12-Classification.html#datasets-that-rely-on-the-icd",
    "href": "slides/Day12-Classification.html#datasets-that-rely-on-the-icd",
    "title": "Day Nine: Classification",
    "section": "Datasets that rely on the ICD",
    "text": "Datasets that rely on the ICD\n\nNavigate to data.gov and search for “ICD”\nHow do these datasets rely on the ICD?"
  },
  {
    "objectID": "slides/Day12-Classification.html#mini-project-1",
    "href": "slides/Day12-Classification.html#mini-project-1",
    "title": "Day Nine: Classification",
    "section": "Mini-Project 1",
    "text": "Mini-Project 1\n\nDue October 17 (in 2 weeks)\nChoice between a discourse analysis or a cultural analysis of infrastructure\nBoth involve practicing the methods we have discussed in class\nBoth involve producing an argument from ethnographic evidence\nArgument should be descriptive, not of value or policy\nVideo describing both projects in depth will be available on Perusall"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day13-Interviewing.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nHow would the response to this question deepen understanding of the cultural underpinnings of a data infrastructure?"
  },
  {
    "objectID": "slides/Day14-Expertise.html#draw-a-map-of-data-science---sort-of-like-you-would-draw-a-map-of-the-world.-whats-at-the-center-of-your-map-what-features-are-at-the-periphery-where-are-the-mountains-rivers-centers-forbidden-areas",
    "href": "slides/Day14-Expertise.html#draw-a-map-of-data-science---sort-of-like-you-would-draw-a-map-of-the-world.-whats-at-the-center-of-your-map-what-features-are-at-the-periphery-where-are-the-mountains-rivers-centers-forbidden-areas",
    "title": "Day Fourteen: Expertise",
    "section": "Draw a “map of data science” - sort of like you would draw a map of the world. What’s at the center of your map? What features are at the periphery? Where are the mountains? Rivers? Centers? Forbidden areas?",
    "text": "Draw a “map of data science” - sort of like you would draw a map of the world. What’s at the center of your map? What features are at the periphery? Where are the mountains? Rivers? Centers? Forbidden areas?"
  },
  {
    "objectID": "slides/Day14-Expertise.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day14-Expertise.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Eleven: Boundary Work",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nHow does your map reflect certain assumptions of what counts as data science vs. non-data-science?"
  },
  {
    "objectID": "slides/Day14-Expertise.html#boundary-work",
    "href": "slides/Day14-Expertise.html#boundary-work",
    "title": "Day Eleven: Boundary Work",
    "section": "Boundary Work",
    "text": "Boundary Work\n\nExamines the work that society does to set the boundaries of science vs. non-science\nAcknowledges that differently situated people have different ways of justifying what makes something scientific vs. non-scientific\nHelps establish science’s epistemic authority through credibility contests and cartographic contrast"
  },
  {
    "objectID": "slides/Day14-Expertise.html#examining-the-boundaries-of-data-science-expertise",
    "href": "slides/Day14-Expertise.html#examining-the-boundaries-of-data-science-expertise",
    "title": "Day Fourteen: Expertise",
    "section": "Examining the Boundaries of Data Science Expertise",
    "text": "Examining the Boundaries of Data Science Expertise\n\nWhat separates an expert from a non-expert?\nTraining? Credentials? Publications? The way they dress? The way they talk?\nCan you draw a distinguishing line around a data science expert?\nHow does this create barriers to participation?"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#jot-down-some-ideas-of-what-this-means-to-you",
    "href": "slides/Day15-ParticipantObservation.html#jot-down-some-ideas-of-what-this-means-to-you",
    "title": "Day Fifteen: Participant Observation",
    "section": "",
    "text": "This chapter makes one basic point: the work of producing, preserving, and sharing data reshapes the organizational, technological, and cultural worlds around them. - Jackson and Ribes, 2013"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#reading-review-what-are-rituals",
    "href": "slides/Day15-ParticipantObservation.html#reading-review-what-are-rituals",
    "title": "Day Fifteen: Participant Observation",
    "section": "Reading Review: What are rituals?",
    "text": "Reading Review: What are rituals?\n\nStylized repetitive activities engaged in different cultural contexts\nMay involve words, gestures, movements, exchanges\nCan sometimes be taken-for-granted and other times front and center\nMark the shared beliefs of a social group and membership in a community"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day15-ParticipantObservation.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Twelve: Participant Observation",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:\n\nHow would you characterize the role and importance of your body in taking that measurement?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#some-terminology-in-semiotics",
    "href": "slides/Day19-Mobilization.html#some-terminology-in-semiotics",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Some Terminology in Semiotics",
    "text": "Some Terminology in Semiotics\n\nSignifier: a material thing that signfies something else (e.g. a number, an word, an expression)\nSignified: a concept that the signifier represents\nIndexical: cases where the signifier is assumed to have a direct causal relationship with the signified\n\ne.g. My dog’s tail wag signifies that she is happy.\ne.g. 5 signifies the number of fingers on my hand.\n\nSymbolic: cases where is the signifier takes on meaning through culturally-specific interpretation of the signified"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#framing-statistics-comparisonranking",
    "href": "slides/Day19-Mobilization.html#framing-statistics-comparisonranking",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Framing Statistics: Comparison/Ranking",
    "text": "Framing Statistics: Comparison/Ranking\n\nIs x more/less than y?\n\ne.g. “Women make 77 cents for every dollar a man makes.”\ne.g. “The U.S. ranks first in firearm homicides in high income countries, with a rate of 4.52 firearm homicides per 100,000 population.”"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#framing-statistics-standards-settingbenchmarking",
    "href": "slides/Day19-Mobilization.html#framing-statistics-standards-settingbenchmarking",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Framing Statistics: Standards-setting/Benchmarking",
    "text": "Framing Statistics: Standards-setting/Benchmarking\n\nIdentifying the yardstick to indicate “good” or “healthy”\n\ne.g. “A credit score of 800 or above is ‘excellent’.”\ne.g. “An AQI of 202 is ‘very unhealthy’.”"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#free-writing-consider-a-numeric-metric-you-use-to-measure-something-about-yourself.-pick-something-that-has-special-meaning-for-you.-is-there-a-number-that-you-strive-for-how-do-you-know-when-your-score-is-good",
    "href": "slides/Day19-Mobilization.html#free-writing-consider-a-numeric-metric-you-use-to-measure-something-about-yourself.-pick-something-that-has-special-meaning-for-you.-is-there-a-number-that-you-strive-for-how-do-you-know-when-your-score-is-good",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Free-writing: Consider a numeric metric you use to measure something about yourself. Pick something that has special meaning for you. Is there a number that you strive for? How do you know when your score is “good”?",
    "text": "Free-writing: Consider a numeric metric you use to measure something about yourself. Pick something that has special meaning for you. Is there a number that you strive for? How do you know when your score is “good”?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#principles-for-active-listening",
    "href": "slides/Day19-Mobilization.html#principles-for-active-listening",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Principles for Active Listening",
    "text": "Principles for Active Listening\n\nSuspend judgment\nBe curious\nHalt internal commentary\nListen with intention\nExpress gratitude for sharing"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#structured-story-tellling",
    "href": "slides/Day19-Mobilization.html#structured-story-tellling",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Structured Story-tellling",
    "text": "Structured Story-tellling\n\nFind a partner, and decide who will speak first and who will speak second.\nTwo minutes:\n\n\nFirst speaker: Provides uninterrupted narration about this measure. What does it mean to you? How did it come to have its meaning? How do you decide what success looks like?\nSecond speaker: Listens with intention: What does this number mean to this person?\n\n\nOne minute:\n\n\nSecond speaker: Reflect back on what you heard/ask questions, etc. (e.g. “From what I understand…”)\nFirst speaker: Affirm, correct, clarify, but also listen for what they heard\n\n\nSwap and repeat.\n\n\n\nThrough what mechanisms did people decide what was good?\nDid you identify social structures at play in making this number meaningful?\nDid you hear your partner talk about ways this number might mean different things to different people?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#narrating-data",
    "href": "slides/Day19-Mobilization.html#narrating-data",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "Narrating Data",
    "text": "Narrating Data\n\nCurating plot details and trajectories\n\nChoosing variables to report on\nChoosing how to sequence statistics\n\nEngaging literary or rhetorical devices\n\nTechniques of communication designed to evoke a reaction from the listener\n\nIdentifying the sense-making tools\n\nVisuals\nStatistical terminology (e.g. “statistically significant”)"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life",
    "href": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-1",
    "href": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-1",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-2",
    "href": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-2",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-3",
    "href": "slides/Day19-Mobilization.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-3",
    "title": "Day Nineteen: Mobilizing Meaning",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?\n\n\n\n\nAd banned in 2023\nEmphasized the renewables at the expense of high-carbon initiatives (which make up bulk of Shell production)\n“We strongly disagree with the ASA’s decision, which could slow the UK’s drive towards renewable energy,” said a spokesperson for Shell. “People are already well aware that Shell produces the oil and gas they depend on today. When customers fill up at our petrol stations across the UK, it’s under the instantly recognisable Shell logo.”\n“However, they [are] unlikely to be aware of the details of this in relation to specific companies,” the ASA said. “Ads [are] therefore likely to mislead consumers if they [have] misrepresented the contribution that lower-carbon initiatives played, or would play in the near future, as part of the overall balance of a company’s activities.”\n\n\n\n\n“The actual number of Americans jailed or imprisoned, about 2.3 million.” - Matt Korostoff\n\n\nhttps://mkorostoff.github.io/incarceration-in-real-numbers/"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#turn-to-a-neighbor-and-discuss",
    "href": "slides/Day20-MobilizationOtherwise.html#turn-to-a-neighbor-and-discuss",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Turn to a neighbor and discuss:",
    "text": "Turn to a neighbor and discuss:"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#mobilizing-data-narratives",
    "href": "slides/Day20-MobilizationOtherwise.html#mobilizing-data-narratives",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Mobilizing Data Narratives",
    "text": "Mobilizing Data Narratives\n\nMobilization refers to the processes by which people prepare something to be put to use or into action\nStakeholders strategically engage in meaning-making activities around data\nShapes societal interpretations of data"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#narrating-data",
    "href": "slides/Day20-MobilizationOtherwise.html#narrating-data",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Narrating Data",
    "text": "Narrating Data\n\nCurating plot details and trajectories\n\nChoosing variables to report on\nChoosing how to sequence statistics\n\nEngaging literary or rhetorical devices\n\nTechniques of communication designed to evoke a reaction from the listener\n\nIdentifying the sense-making tools\n\nVisuals\nStatistical terminology (e.g. “statistically significant”)"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life",
    "href": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-1",
    "href": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-1",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-2",
    "href": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-2",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-3",
    "href": "slides/Day20-MobilizationOtherwise.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-3",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?\n\n\n\n\nAd banned in 2023\nEmphasized the renewables at the expense of high-carbon initiatives (which make up bulk of Shell production)\n“We strongly disagree with the ASA’s decision, which could slow the UK’s drive towards renewable energy,” said a spokesperson for Shell. “People are already well aware that Shell produces the oil and gas they depend on today. When customers fill up at our petrol stations across the UK, it’s under the instantly recognisable Shell logo.”\n“However, they [are] unlikely to be aware of the details of this in relation to specific companies,” the ASA said. “Ads [are] therefore likely to mislead consumers if they [have] misrepresented the contribution that lower-carbon initiatives played, or would play in the near future, as part of the overall balance of a company’s activities.”"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#what-is-an-ethnographic-argument",
    "href": "slides/Day20-MobilizationOtherwise.html#what-is-an-ethnographic-argument",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "What is an ethnographic argument?",
    "text": "What is an ethnographic argument?\n\nCentral theme derived from an ethnographic study\nIdentifies a certain cultural pattern or phenomena\nExamples:\n\nHow data is talked about\nThe cultural assumptions underpinning a data infrastructure\nHow labor is recognized in a data practice"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#writing-an-ethnographic-argument",
    "href": "slides/Day20-MobilizationOtherwise.html#writing-an-ethnographic-argument",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Writing an Ethnographic Argument",
    "text": "Writing an Ethnographic Argument\n\nParagraph 1: Introduction and thesis\nParagraph 2-x: Sub-arguments supporting thesis\n\nPresents evidence from data collection to support sub-argument\n\nParagraph Final: Summarize thesis and wrap-up"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#what-counts-as-evidencedata-in-ethnographic-arguments",
    "href": "slides/Day20-MobilizationOtherwise.html#what-counts-as-evidencedata-in-ethnographic-arguments",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "What counts as evidence/data in ethnographic arguments?",
    "text": "What counts as evidence/data in ethnographic arguments?\n\nDiscourse Analysis/Interview\n\nDirect quotes and their context\n\nParticipant Observation\n\nVignettes/thick description\n\nCultural Analysis of Infrastructure\n\nMaterial from secondary sources\nFacts about the infrastructure’s historical development\nDescriptions of the organization of the infrastructure"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#revision-expectations",
    "href": "slides/Day20-MobilizationOtherwise.html#revision-expectations",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Revision Expectations",
    "text": "Revision Expectations\n\nRevision = re - vision = seeing again\nInvolves more than spelling and grammar fixes and word changes; editing is only one component of revision\nThings to consider:\n\nRecognize and re-articulate the purpose\nRefine the focus (audience and presentation)\nClarify the argument\nStrengthen and hone evidence\n\n\n\nhttps://writingcenter.unc.edu/tips-and-tools/revising-drafts/"
  },
  {
    "objectID": "slides/Day22-Credibility.html#consider-a-time-when-you-felt-surveiled-misrespresented-or-under-represented-by-a-data-system.-how-did-you-respond",
    "href": "slides/Day22-Credibility.html#consider-a-time-when-you-felt-surveiled-misrespresented-or-under-represented-by-a-data-system.-how-did-you-respond",
    "title": "Day Twenty-One: Data Activism",
    "section": "Consider a time when you felt surveiled, misrespresented, or under-represented by a data system. How did you respond?",
    "text": "Consider a time when you felt surveiled, misrespresented, or under-represented by a data system. How did you respond?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#data-activism",
    "href": "slides/Day22-Credibility.html#data-activism",
    "title": "Day Twenty-One: Data Activism",
    "section": "Data Activism",
    "text": "Data Activism\n\nA range of practices that confront the politics of datafication\nMay involve affirmative data action\nMay involve resistance to data systems"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day23-Ignorance.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nHow do your definitions compare?\nWhat is assumed about how ignorance comes to be from your definition?"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#scales-of-data-knowledge-production",
    "href": "slides/Day23-Ignorance.html#scales-of-data-knowledge-production",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Scales of Data Knowledge Production",
    "text": "Scales of Data Knowledge Production\n\nEpistemology\nInfrastructure\nIndividual Labor\nCollective Rituals\nInstitutions and Incentives\nAdvocacy, Activism, and Discourse"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#enter-up-to-three-words-or-short-phrases-that-indicate-what-ignorance-is.",
    "href": "slides/Day23-Ignorance.html#enter-up-to-three-words-or-short-phrases-that-indicate-what-ignorance-is.",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Enter up to three words or (short) phrases that indicate what ‘ignorance’ is.",
    "text": "Enter up to three words or (short) phrases that indicate what ‘ignorance’ is.\nPollEv.com/lindsaysmith"
  },
  {
    "objectID": "slides/Day23-Ignorance.html#ignorance-concept-map",
    "href": "slides/Day23-Ignorance.html#ignorance-concept-map",
    "title": "Day Twenty-Three: Ignorance",
    "section": "Ignorance Concept Map",
    "text": "Ignorance Concept Map\n\nSelect at least eight words from the cloud to write on sticky notes.\nOrganize these terms in a meaningful way on your orange paper. Feel free to draw lines between interconnected terms.\nIdentify three categories of ignorance represented on your map. Be sure to write them somewhere on the map.\nTake a photo the finished map, and post it in the #random channel on Slack."
  },
  {
    "objectID": "assignments.html#field-notes",
    "href": "assignments.html#field-notes",
    "title": "Assignments",
    "section": "Field Notes",
    "text": "Field Notes\nEthnographers keep field journals to thickly document what they observe in social environments. Typically field notes will describe a physical environment and the participants in that environment (including appearances, roles, behaviors, moods, interactions, conversations, etc.). Field notes will also interpret what was observed, considering how the empirical material affirms, contests, or transforms how we understand cultural phenomena. In data ethnography, the social environment under consideration can be anywhere data is produced, applied, presented, or talked about. This assignment is about paying attention to the numbers (and other forms of data) that inhabit daily life and practicing cultural interpretation.\n\nLearning Objectives\n\nRecall, define, explain, apply, and synthesize course concepts and ideas\nEngage the genre and practice of thick description\nDevelop observational habits in data-saturated environments\nDevelop skill in interpreting cultural meaning\n\n\n\nInstructions\nConsider a statistic, dataset, data standard, data infrastructure, or data collection environment that you encountered in the past two weeks. Write 300-400 words, first, detailing the environment and, second, responding to one of the following prompts.\n\nWhat cultural assumptions about what knowledge is and how knowledge is produced can you glean from this data environment?\nHow do binary oppositions structure discourse about data in this environment? With what consequences?\nWhat metaphors are used to characterize data in this environment, and what assumptions about data are conveyed through these metaphors?\nWhat is one master narrative architecturally designed into this data infrastructure? How does that narrative manifest both semiotically and materially through the data infrastructure?\nWhat invisible forms of labor sustain this data infrastructure or data environment? How do the individuals that engage in that labor approach and care for their work? What social forces render that work invisible?\nHow is data science defined in this space? Through what kinds of boundary work do social groups advocate for what should count as legitimate data science? What are the social consequences of this definitional work?\nWhat counts as credible data expertise in this data environment? How do these ideas mediate who gets a voice in the data environment?\nIn what ways are data practices embodied in this environment? How does attention to embodied practice change the ways we understand the data?\nWhat rituals do data collectors engage in this data environment? What shared beliefs motivate these rituals, and how do the rituals manifest in the “raw” data that gets produced?\nWhat stakes do individuals have in how numbers in this data environment get reported? How do social or financial incentives to have the numbers meet or beat certain thresholds animate how individuals behave and the choices they make in relation to this data collection practice, data analysis, or data reporting? Why do those incentives exist?\nHow is the meaning of this statistic or data value socially debated, negotiated, or settled upon? At what thresholds do the data values become actionable, and how do social groups determine levels of actionability? How and under what conditions do these decisions get made?\nHow are data socially mobilized in this space? What strategies do individuals, institutions, or social groups engage to render the data persuasive?\n\n\n\nSample Work\n\nPrevious Student Field Notes here\nHere’s an example of some very short ““thick description”” write-ups of two data environments from my own research.\n\n\n\nCriteria for completion\n\nStudent writes 300-400 words about a statistic, dataset, data standard, or data collection environment.\nStudent writes about a data situation encountered in the past two weeks.\nStudent responds to a prompt, engaging course concepts to interpret the cultural meaning of the data situation analyzed."
  },
  {
    "objectID": "assignments.html#write-up-1",
    "href": "assignments.html#write-up-1",
    "title": "Assignments",
    "section": "Write-up",
    "text": "Write-up\nFollowing your analysis, you will write-up a 750-1000 word memo detailing an argument about the data discourse conveyed in the talk(s). What cultural assumptions about data are reflected in the discourse, and through what linguistic structures are those assumptions conveyed? Direct quotations should be referenced throughout (and cited) to back up your claims, but do not count towards the overall word count. Your write-up should have introductory and concluding paragraphs that clearly indicate the central point you are making in the write-up and that point should be carried through all of the examples and arguments presented in the write-up.\n\n\n\n\n\n\nWarning\n\n\n\nIf you choose to watch three TED Talks, please note that you should still only present one argument, synthesizing what you learn across all three videos. This means that you should compare discourse across multiple talks in each paragraph. Assignments that present three distinct discourse analyses/do not offer in-paragraph synthesis and comparison will not receive full credit.\n\n\n\nCriteria for completion\n\nStudent responds to at least one of the questions in each of the nine sections of the Discourse Analysis Worksheet.\nStudent writes a 750-1000 word memo detailing the discourse of the talk(s). Direct quotations are included as evidence, and not included in the overall word count.\nStudent synthesizes evidence into one argument, presented at the start of the write-up and carried through each paragraph. If analyzing multiple talks, student offers comparison and synthesis in each paragraph."
  },
  {
    "objectID": "index.html#course-instructor",
    "href": "index.html#course-instructor",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "Lindsay Poirier, she/her/hers.\n\n\n\n\n\nI am a cultural anthropologist that studies how civic data gets produced, how communities think about and interface with data, and how data infrastructure can be designed more equitably. My Ph.D. is in an interdisciplinary discipline called Science and Technology Studies - a field that studies the intricate ways science, technology, culture, and politics all co-constitute each other. I work on a number of collaborative research projects that leverage public data to deepen understanding of social and environmental inequities in the US, while also qualitatively studying the politics behind data gaps and inconsistencies. As an instructor, I prioritize active learning and often structure courses as flipped classrooms. You can expect in-class time to involve lectures, activities, and labs.\n\n\n\nSlackMeeting with Me\n\n\nI can best support students in this course when I can readily keep tabs on our course-related communication. Because of this, I ask that you please don’t email me regarding course-related questions or issues. The best way to get in touch with me is via our course Slack. If you have course-related questions, I encourage you to ask them in the #sds-192-questions channel. When discretion is needed, feel free to DM. Please reserve more formal concerns like grades or accommodation requests for an in-person (or in-person virtual) conversation.\nDuring the week, I will try my best to answer all Slack messages within 24 hours of receiving them. Please note that to maintain my own work-life balance, I don’t answer Slack messages late in the evenings or on the weekends. It’s important that you plan when you start your assignments accordingly.\n\n\nMeeting with me outside of class is a great opportunity for us to chat about what you’re learning in the course, clarify expectations on assignments, and review work in progress. I also love when students drop in to office hours to request book recommendations, discuss career or research paths, or just to say hi!\nThere are two ways to meet with me. If you would like to have a one-on-one private conversation, I ask that you schedule an appointment with me via the booking form on Moodle. For support on class topics, you may drop-in during my regularly scheduled office hours.\n\nWednesday, 2:45 PM - 3:45 PM, McConnell 214\nFriday, 11:00 AM - 12:00 PM, McConnell 214"
  },
  {
    "objectID": "index.html#course-texts",
    "href": "index.html#course-texts",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "I will make all course readings available on Perusall, which can be accessed through our course Moodle page."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "Course Syllabus Quiz: 3%\nCourse Infrastructure Set-up: 2%\nReading Quizzes: 5%\nLabs (10): 30%\nProjects (3): 30%\nMid-term Exam: 15%\nFinal Exam: 15%\n\nAlso note that your grade may be impacted if you have more than 3 unexcused absences. See the course Attendance Policy below.\n\n\n\n\n\n\nSpinelli Center\n\n\n\nSmith’s Spinelli Center offers a number of resources to support SDS students. Spinelli Center Data Assistants will visit our classroom regularly to support you through lab work. The Center also offers drop-in tutoring hours Sunday through Thursday 7-9 PM. Finally, you can drop-in to Seelye 207D or schedule an appointment with the Data Research and Statistics Counselor (Kenneth Jeong). To schedule an appointment, email qlctutor@smith.edu."
  },
  {
    "objectID": "index.html#policies",
    "href": "index.html#policies",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "PreparationAttendanceExtensionsAcademic HonestyGenerative AI\n\n\nThis is a 4-credit course with 4.5 hours per week of in-classroom instructions. Smith expects students to devote 7.5 out-of-class hours per week to 4-credit classes. I have designed the course assignments and selected the course readings with this target in mind.\n\n\nAttending class is not only important for your learning but also an act of community. Attendance will be taken each class period. That said, we all have reasons we can’t be available from time-to-time. You may miss three classes with no penalty. You do not need to inform me that you will be absent in these cases. After the third unexcused absence, your grade may drop by a modifier for each class missed. I understand that you may need to be absent beyond these three sessions. Additional absences may be excused due to family/personal difficulties, sickness, or school or career-related activities; however, I will require some form of documentation for these absences. Please speak with your class dean or the Accessibility Resource Center so that we can get documentation of your need.\nI also ask that you make every effort to arrive to class on time. This is a large course, and when students arrive late, it can be distracting for me as the instructor, and it can be distracting to other students in the course. It also makes it difficult for me to plan group activities. Students arriving more than 10 minutes late for class without having informed me ahead of time will be marked as absent.\nIf you must miss a class entirely, you should contact a peer to discuss what was missed. Please note that the SDS Program has adopted a shared policy regarding in-person attendance:\n\nIn keeping with Smith’s core identity and mission as an in-person, residential college, SDS affirms College policy (as per the Provost and Dean of the College) that students will attend class in person. SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Accessibility Resource Center will be the only exceptions to this policy. As with any other kind of ADA accommodations, please notify your instructor during the first week of classes to discuss how we can meet your accommodations.\n\n\n\nThere is an automatic 24-hour grace period on all lab and project assignments. There will be no penalties for submitting the project within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any project or lab assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted without an accommodation from a class dean or from the ARC.\nNote that this policy does not apply to reading assignments/Perusall annotations. Reading assignments/Perusall annotations need to be completed by the due date for credit.\n\n\n\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. Students and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information, and honest and respectful use of college resources. Any cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\n\n\nSubmitting work completed by another student as your own.\nCopying and pasting words from sources without quoting and citing the author.\nParaphrasing material from another source without citing the author.\nFailing to cite your sources correctly.\nFalsifying or misrepresenting information in submitted work.\nPaying another student or service to complete assignments for you.\nSubmitting work generated by artificially intelligent tools such as chatGPT\n\n\n\nIn this course, you are learning, not only how to produce code, but also how to think like a data scientist - i.e. how to develop logical solutions to problems, how to discern a good plot from a bad one, and how to spot errors in reasoning that can lead to misleading results. If you are using generative AI to write your code, then you are not developing these foundational skills. Further, it’s important to keep in mind that, as generative AI is increasingly replacing entry level data science labor, some of the most important jobs in data science will involve being able to explain how an AI came to its results and to audit AI for errors and bias. If you don’t develop the skills to understand how the underlying code is composed/works, then you will not be prepared for this kind of work.\nWith this in mind, any use of generative AI to complete assignments or produce content for this course is prohibited. Prohibited forms of usage include but are not limited to: summarizing course readings, drafting responses to written prompts, drafting comments for Slack, composing code, formatting code, answering lab, quiz, or exam questions, or proofreading text. Any use of generative artificial intelligence in this course will be considered a case of academic dishonesty/plagiarism and will be reported to the Academic Honor Board."
  },
  {
    "objectID": "index.html#community",
    "href": "index.html#community",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "Code of ConductPrinciples of CommunityPronouns\n\n\nAs the instructor for this course, I am committed to making participation in this course a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned to this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0, available here.\n\n\nI hope that we can foster a collaborative and caring environment in this classroom: one that celebrates successes, respects individual strengths and weaknesses, demonstrates compassion for each other’s struggles, and affirms diverse identities. Here are some ideas that I have for creating this environment in our course:\n\nCheck-in with colleagues before starting collaborative work. “What three words describe how you’re feeling?” “Name one challenge and one success from this week.” “What are you doing for self-care right now?” Thank each other for sharing where they’re at.\nConsider when to step up and when to step back in class discussions, creating space for others to contribute. Listening is just as important to community-building as speaking.\nAcknowledge that there is much we don’t know about how our colleagues experience the world. …but don’t ask colleagues to speak on behalf of a social group you perceive them to be a part of.\nCheer on colleagues as they give presentations or try something out for the first time.\nAsk questions often in our #sds-192-questions channel. Help each other out by answering questions when you can.\nMistakes happen. I will certainly make mistakes in class. Admit mistakes, and then move on.\n\n\n\nUsing the proper pronouns for our students is foundational to a safe, respectful classroom environment that creates a culture of trust. For information on pronouns and usage, please see the Office of Equity and Inclusion link here: Pronouns"
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "AccommodationsStudent Well-beingTrigger Warnings\n\n\nIt is my goal for everyone to succeed in this course. If you have personal circumstances that may impact your experience of our classroom, I encourage you to contact Accessibility Resource Center in College Hall 104 or at arc@smith.edu. The Center will generate a letter that indicates to me what kind of support you need and how I can make your classroom experience more accommodating. Once you have this letter, you are welcome to visit my office hours or email me to discuss ideas about how we can tailor the course accordingly. While you can request accommodations at any time, the sooner we start this conversation, the better. If you have concerns about the course that are not addressed through ARC, please contact me. At no point will I ask you to divulge details about your personal circumstances to me.\n\n\nCollege life is stressful, and life outside of college can be overwhelming. It is my position that attending to your physical and mental health and well-being should be a top priority. I will remind you of this often throughout the semester. I encourage you to schedule a time to talk with me if you are struggling with this course. If you, or anyone you know, is experiencing distress, there are numerous campus resources that can provide support via the Schacht Center. I can point you to these resources at any time throughout the semester.\n\n\nA trigger is a topic or image that can precipitate an intense emotional response. When common triggering topics are to be covered in this course, I will do my best to provide a trigger warning in advance of the discussion. However, I can’t always anticipate triggers. With this in mind I’ve set up an anonymous form, available on Moodle, where you can indicate topics for which you would like me to provide a warning."
  },
  {
    "objectID": "index.html#infrastructure",
    "href": "index.html#infrastructure",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "MoodlePerusallGitHubSlack\n\n\nGrades, forms, and handouts will be available on the course Moodle.\n\n\nAll course readings and recorded lectures will be available on Perusall. You can access Perusall via our course Moodle page.\n\n\nI will be using GitHub Classroom to distribute several course assignments, including labs and projects. You will submit assignments by pushing changes to template documents to a private GitHub repository. I will provide guidance on how to do this early in the semester.\n\n\nThis class will use the R statistical software package. If you haven’t already, you will install and configure R and RStudio in SDS 100. You should let me know in the first week of the course if you are using a Chromebook or tablet.\n\n\n\nOutside of class almost all of our communication will happen via Slack. You can use the following channels\n\n#general: Course announcements (only I can post)\n#sds-192-discussions: Share news articles and relevant opportunities\n#sds-192-questions: Ask and answer questions about our course\nYou can also create private Slack channels with your project group members."
  },
  {
    "objectID": "example_fieldnotes.html#sds-237-spring-23-casey-macgibbon",
    "href": "example_fieldnotes.html#sds-237-spring-23-casey-macgibbon",
    "title": "Example Field Notes",
    "section": "",
    "text": "This entry documents a data environment Casey MacGibbon observed on 2023-02-10 in The Human Performance Labratory, Scott Gym, Smith College. The observations were written up on 2023-02-19.\nMy stopwatch hits 20:00:00.\n“Alright, it’s been five minutes since I last asked, how rigorous do you feel your exercise is on a scale of 6-20?” I say, holding up a reference board for the participant. She points to a 12, which is labeled as “Somewhat hard.” She is on minute twenty of her acute bout of exercise, where she is walking at a moderate pace on the treadmill, and we are monitoring her heart rate from an electronic wristband.\nRecently, I have begun research work and analysis for the Witkowski Vascular Function Lab, and this was the first time I went in to observe a visit with a participant. I am new to the lab, and my perspective still feels a bit like an outsiders’. After the visit, I asked Dr. Witkowski explained that each number on the reference board corresponded to a heart rate by multiplying the number by 10, essentially acting as a participant’s heart rate experience. She explained to me how there are two different “data streams” throughout the lab, one being objective and the other subjective.\nThe idea of objective/subjective data is an example of a binary opposition: a pair of terms with opposite meanings. I noticed how much more “objective” data collection was valued, while a lot of the data collected from the participant was thought of as “subjective,” and thought of to be more subject to inaccuracy. Data is categorized as one or the other.\nNo data can be subjective, because the mere decision to begin collecting data means that there are expectations and biases put into the collection of data. Though the target heart rate is an “objective” part of the study because it is numerical, does not mean it is without bias. Heart rate calculations have historically ignored weight, height, average physical activity, and many other factors. This is just one example of how all of the data is biased, and cannot be placed into these categorizations. Additionally, the “data streams” figure of speech is a part of popular data discourse, contributing to the idea that data is a natural resource waiting to be found, untainted and untouched by human interference.\nDiscourse is the way that people talk about and engage with a certain thing/person, and dominant discourse can often characterize how we think about this topic as a society. The imagery of a data stream indicates that data is natural and is something we can “drink” from at any time. In reality, all data is collected from humans, who are limited in their standpoints and views."
  },
  {
    "objectID": "grading_contract.html#assignments",
    "href": "grading_contract.html#assignments",
    "title": "Grading Contract",
    "section": "Assignments",
    "text": "Assignments\nDescriptions of all written and project assignments are available here. For an assignment to be considered complete, it must meet the minimum criteria outlined in the assignment description. It also must be completed “in good faith” - meaning in a way that demonstrates integrity to the spirit of the assignment."
  },
  {
    "objectID": "slides/Day2-Epstemologies.html",
    "href": "slides/Day2-Epstemologies.html",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "",
    "text": "What does it mean that you know this to be true? What counts as knowing?\nHow/through what means do you know this to be true?\nWhat are the limits of your knowledge on this?\n\n\n\n\n\n\nGreek words\n\nEpisteme”: knowledge; understanding\n“Logos”: reason; argument\n\nPhilosophical study of the nature and limits of knowledge\n\nWhat conditions must be met for us to say that we “know” something to be true?\nHow do a group of people come to acquire knowledge?\n\n\n\n\n\n\n\nHistorically knowledge defined as “justified true belief”\nDoes knowledge exist independently of a knowing mind?\n\nPositivists claim yes, there is objective truth independent of a knower\nInterpretivists and constructivists claim no, truths are subjective or tied to a knowing mind\n\n\n\n\n\n\n\nEmpiricists claim that knowledge emerges from direct observation\nRationalists claim that knowledge emerges from logic and reason\n…and then there’s testimony, memory, intuition, feeling\n\n\n\n\n\n\n“End of theory” and the need for experimentation\nKnowledge creation is purely inductive\nData can “speak for itself”; it is self-evident\nPatterns in data are inherently meaningful\nMeaning presented by data transcends context\n\n…according to (Kitchin 2013)"
  },
  {
    "objectID": "slides/Day17-Incentives.html",
    "href": "slides/Day17-Incentives.html",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "",
    "text": "If you have not started on MP2 yet, you are late in doing so.\nThere is reading from Cooking Data due this Thursday."
  },
  {
    "objectID": "slides/Day1-Intro.html",
    "href": "slides/Day1-Intro.html",
    "title": "Day One: Introductions",
    "section": "",
    "text": "study of human culture and social relations\ninvolves interactions and observations, recording, and analysis\ndata collection methods are predominantly qualitative\nanalysis is predominantly inductive and interpretive"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html",
    "href": "slides/Day15-ParticipantObservation.html",
    "title": "Day Fifteen: Participant Observation",
    "section": "",
    "text": "This chapter makes one basic point: the work of producing, preserving, and sharing data reshapes the organizational, technological, and cultural worlds around them. - Jackson and Ribes, 2013"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html",
    "href": "slides/Day20-MobilizationOtherwise.html",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "",
    "text": "Mobilization refers to the processes by which people prepare something to be put to use or into action\nStakeholders strategically engage in meaning-making activities around data\nShapes societal interpretations of data\n\n\n\n\n\n\nCurating plot details and trajectories\n\nChoosing variables to report on\nChoosing how to sequence statistics\n\nEngaging literary or rhetorical devices\n\nTechniques of communication designed to evoke a reaction from the listener\n\nIdentifying the sense-making tools\n\nVisuals\nStatistical terminology (e.g. “statistically significant”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAd banned in 2023\nEmphasized the renewables at the expense of high-carbon initiatives (which make up bulk of Shell production)\n“We strongly disagree with the ASA’s decision, which could slow the UK’s drive towards renewable energy,” said a spokesperson for Shell. “People are already well aware that Shell produces the oil and gas they depend on today. When customers fill up at our petrol stations across the UK, it’s under the instantly recognisable Shell logo.”\n“However, they [are] unlikely to be aware of the details of this in relation to specific companies,” the ASA said. “Ads [are] therefore likely to mislead consumers if they [have] misrepresented the contribution that lower-carbon initiatives played, or would play in the near future, as part of the overall balance of a company’s activities.”\n\n\n\n\n“The actual number of Americans jailed or imprisoned, about 2.3 million.” - Matt Korostoff\n\n\nhttps://mkorostoff.github.io/incarceration-in-real-numbers/"
  },
  {
    "objectID": "assignments/semiotic-analysis.html",
    "href": "assignments/semiotic-analysis.html",
    "title": "Semiotic Analysis",
    "section": "",
    "text": "In this checkpoint, referencing data documentation, you will study the definitions underlying the dataset, considering issues such as - What counts as an observation? How are observations counted or measured? How are observations categorized? In other words, in this assignment you will be performing an ethnography of data infrastructure - considering how the standards used to produce this dataset implicates what kinds of narratives we can draw from the data."
  },
  {
    "objectID": "assignments/semiotic-analysis.html#purpose-and-instructions",
    "href": "assignments/semiotic-analysis.html#purpose-and-instructions",
    "title": "Semiotic Analysis",
    "section": "",
    "text": "In this checkpoint, referencing data documentation, you will study the definitions underlying the dataset, considering issues such as - What counts as an observation? How are observations counted or measured? How are observations categorized? In other words, in this assignment you will be performing an ethnography of data infrastructure - considering how the standards used to produce this dataset implicates what kinds of narratives we can draw from the data."
  },
  {
    "objectID": "assignments/semiotic-analysis.html#units-of-observation",
    "href": "assignments/semiotic-analysis.html#units-of-observation",
    "title": "Semiotic Analysis",
    "section": "Units of Observation",
    "text": "Units of Observation\n\nAny time we count something in the world, we are not only engaging in a process of tabulation; we are also engaged in a process of defining. If I count the number of students in a class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I’m going to define “student,” those decisions impact the numbers that I produce. When I change my definition of “student,” how I go about tabulating students also changes. Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined.\n\n\nWhat does each row in your dataset represent?\nIn this dataset, each row is a _____.\n\n\nHow does that observation get defined in the data documentation?\n\nFind a definition for your unit of observation in the data documentation and cite it (with a citation) below.\n\nFill your response here.\n\n\nDevelop a timeline for this definition.\n\nDo some research on the agency website to determine when this definition was established and how it has changed over time. List key dates, along with what happened to the definition on those dates below.\n\n\n\n\n\n\n\n\n\n\nHow do social groups talk about this definition?\n\nSearch Google for the following: “what counts as a ______” Fill the blank with your unit of observation. For instance, if I was studying the National Bridge Inventory, I might type “what counts as a federal bridge”. Read through articles from multiple stakeholder perspectives (i.e. not just agencies). Can you identify any controversies or ambiguities about “what counts”? What are those controversies/ambiguities, and why did they emerge?\n\nFill your response here.\n\n\nWhat potentially relevant concerns are left out of the data due to the scope of this definition?\nFill your response here.\n\n\nThere are two sets of questions below - one set specific for categorical variables, and one set specific for numeric variables. You should select what you consider to be the three most important variables for producing claims in your dataset and respond to the questions below. It can be any combination of categorical/numeric. (e.g. 3 categorical; 2 cateogorical and 1 numeric, etc.)"
  },
  {
    "objectID": "assignments/semiotic-analysis.html#categorical-variables-skip-if-your-dataset-doesnt-have-categorical-variables",
    "href": "assignments/semiotic-analysis.html#categorical-variables-skip-if-your-dataset-doesnt-have-categorical-variables",
    "title": "Semiotic Analysis",
    "section": "Categorical Variables (Skip if your dataset doesn’t have categorical variables)",
    "text": "Categorical Variables (Skip if your dataset doesn’t have categorical variables)\n\nSelect a categorical variable in your dataset. Be sure to select a specific variable.\n\n\nHow do concepts get divided in that categorical variable? What are the possible terms?\nFill response here.\n\n\nCan you identify certain people or things that might not fit neatly into these categories?\nFill response here.\n\n\nExamine the residual categories\n\nResidual categories are the catch-all categories for those things not easily classified. (e.g. Other, Multiple, etc.) Who or what might fall into a residual category in this dataset? In other words, who or what gets othered through the categories?\n\nFill response here."
  },
  {
    "objectID": "assignments/semiotic-analysis.html#numeric-variables-skip-if-your-dataset-doesnt-have-numeric-variables",
    "href": "assignments/semiotic-analysis.html#numeric-variables-skip-if-your-dataset-doesnt-have-numeric-variables",
    "title": "Semiotic Analysis",
    "section": "Numeric Variables (Skip if your dataset doesn’t have numeric variables)",
    "text": "Numeric Variables (Skip if your dataset doesn’t have numeric variables)\n\nSelect a count or measurement in your dataset. Be sure to select a specific variable.\n\n\nAccording to what standards are these numbers reported?\n\nRead the data documentation to learn how the numbers are generated. What standards must the data collectors following when generating the number?\n\nFill response here.\n\n\nThrough what social processes were those definitions and standards agreed upon?\n\nThrough Google searching, can you find information about how these standards of data collection were agreed upon?\n\nFill response here."
  },
  {
    "objectID": "assignments/data-collection-rituals.html",
    "href": "assignments/data-collection-rituals.html",
    "title": "Peopling the Data",
    "section": "",
    "text": "This checkpoint aims to document the rituals of collection that shape your final project dataset."
  },
  {
    "objectID": "assignments/data-collection-rituals.html#purpose-and-instructions",
    "href": "assignments/data-collection-rituals.html#purpose-and-instructions",
    "title": "Peopling the Data",
    "section": "",
    "text": "This checkpoint aims to document the rituals of collection that shape your final project dataset."
  },
  {
    "objectID": "assignments/data-collection-rituals.html#part-1-data-commensurability",
    "href": "assignments/data-collection-rituals.html#part-1-data-commensurability",
    "title": "Peopling the Data",
    "section": "Part 1: Data Commensurability",
    "text": "Part 1: Data Commensurability\nWhat changes to data environments might make the data collected in one time period or setting incommensurable with data collected in another time period or setting?\nFill response here."
  },
  {
    "objectID": "assignments/data-collection-rituals.html#part-2-detail-a-data-collection-practice",
    "href": "assignments/data-collection-rituals.html#part-2-detail-a-data-collection-practice",
    "title": "Peopling the Data",
    "section": "Part 2: Detail a Data Collection Practice",
    "text": "Part 2: Detail a Data Collection Practice\nDetermine a stakeholder that is predominantly responsible for data collection in your dataset. Find a YouTube video of this stakeholder at work or an instruction manual detailing the steps they should take to collect the data. Thickly describe the rituals of the data collection. In your writing, see if you can make the “familiar strange” by paying close attention to details that we might otherwise take for granted. Questions you might consider include:\n\nWhat standards (if any) must the data collectors follow when they encounter the environment on which they are collecting data?\nWhat tools do they use?\nWhat do they wear?\nDuring what time periods do they collect the data?\nWhere do they go to collect the data? Are they alone or accompanied by others?\nHow do they engage their senses when collecting the data?\n\n\nStakeholder: ____\nFill response here."
  },
  {
    "objectID": "assignments/data-collection-rituals.html#part-3-identify-how-standards-stylize-data",
    "href": "assignments/data-collection-rituals.html#part-3-identify-how-standards-stylize-data",
    "title": "Peopling the Data",
    "section": "Part 3: Identify How Standards Stylize Data",
    "text": "Part 3: Identify How Standards Stylize Data\nIn what ways do the habits you identified in Part 2 address some of the commensurability concerns raised in Part 1? How might the numbers look different if different rituals were engaged?\nFill response here."
  },
  {
    "objectID": "assignments/inst-incentives.html",
    "href": "assignments/inst-incentives.html",
    "title": "Institutional Incentives",
    "section": "",
    "text": "This checkpoint aims to document the institutions involved in the production and shaping of your final project dataset."
  },
  {
    "objectID": "assignments/inst-incentives.html#purpose-and-instructions",
    "href": "assignments/inst-incentives.html#purpose-and-instructions",
    "title": "Institutional Incentives",
    "section": "",
    "text": "This checkpoint aims to document the institutions involved in the production and shaping of your final project dataset."
  },
  {
    "objectID": "assignments/inst-incentives.html#part-1-institution-mapping",
    "href": "assignments/inst-incentives.html#part-1-institution-mapping",
    "title": "Institutional Incentives",
    "section": "Part 1: Institution Mapping",
    "text": "Part 1: Institution Mapping\nBelow create long lists of institutions involved in each phase of the dataset’s production. You should do some research to track down the names of specific government offices, private companies, or other partners involved in each phase. You may think back to some of the organizations that people you identified in the “peopling the data” checkpoint worked for. Your list should be more specific than “news outlets” or “legislatures.”\n\nData Infrastructure Creation\n\n\n\n\n\nData Collection\n\n\n\n\n\nData Cleaning\n\n\n\n\n\nData Aggregation\n\n\n\n\n\nData Analysis\n\n\n\n\n\nData Dissemination\n\n\n\n\n\nData Consumption"
  },
  {
    "objectID": "assignments/inst-incentives.html#part-2-identify-institutions",
    "href": "assignments/inst-incentives.html#part-2-identify-institutions",
    "title": "Institutional Incentives",
    "section": "Part 2: Identify institutions",
    "text": "Part 2: Identify institutions\n\nSelect three institutions from your list that appear to have the largest influence over the final configuration and scope of the data. The goal of this section is to determine how this institution incentizes people to behave in certain ways in relation to the data.\n\n\nDo some research on each organization. Peruse their websites, reading through their About pages, their mission statements, their history, etc. Do they have pages outlining the values and commitments that guide their work? What do those pages say? Search for your dataset on their webpages. Do you find any blog posts or statements?\n\n\nPerform a web search for the organization with your dataset in quotation marks (e.g. PhRMA “Open Payments”). Be sure to look at news articles, videos, and official statements. What has this institution said/how has this institution acted in relation to your dataset?\n\nWrite a profile (about 200 words) for each institution. Specific questions you might consider include:\n\nWhat is the institution’s stated mission? How does this stated mission relate to their data work?\nWhich people are members of/work for this institution? Is participation voluntary?\nWhat benefits do people gain from membership in this institution? What are the drawbacks to being a member of this institution?\nWhich other institutions is this institution beholden to? What does this institution gain by meeting the expectations of these other institutions? What do they lose by failing to meet those expectations?\nWhat public statements has the institution put out regarding the data? Do these statements differ from those that individuals have put out regarding the data?\nWhat interests does this institution have in how the data gets reported? What influence do they have over the shape of the data?\nWhat legal and ethical bodies and infrastructures oversee how this institution conducts their data work?\nHow transparent is the institution’s data work?\nWhat repercussions might this institution face if they were to mishandle the data?\n\n\nInstitution 1:\nFill response here.\n\n\nInstitution 2:\nFill response here.\n\n\nInstitution 3:\nFill response here."
  },
  {
    "objectID": "schedule.html#september-03-2024",
    "href": "schedule.html#september-03-2024",
    "title": "Schedule",
    "section": "September 03, 2024",
    "text": "September 03, 2024\n\nIntroductions\n\nDue TodayFurther Resources\n\n\n Fill out the First Day of Class Questionnaire\n\n\n Course slides are here."
  },
  {
    "objectID": "schedule.html#september-05-2024",
    "href": "schedule.html#september-05-2024",
    "title": "Schedule",
    "section": "September 05, 2024",
    "text": "September 05, 2024\n\nHegemonic Backdrops of Big Data\n\nDue TodayFurther Resources\n\n\n Elish, M. C. and danah boyd (2018). “Situating Methods in the Magic of Big Data and AI”. In: Communication Monographs 85.1, pp. 57-80. (Visited on Sep. 01, 2023). Read in Perusall\n Complete Syllabus Quiz\n Fill out the First Day of Class Questionnaire\n Fill out the Trigger Warnings Questionnaire in Moodle.\n Sign-up to take class notes for community labor\n\n\n Course slides are here\n boyd, danah and Kate Crawford (2012). “Critical Questions for Big Data”. In: Information, Communication & Society 15.5, pp. 662-679. (Visited on Jan. 19, 2018).\n Kitchin, Rob (2014). “Big Data, new epistemologies and paradigm shifts”. En. In: Big Data & Society 1.1, p. 2053951714528481. (Visited on Jul. 16, 2019).\n Leonelli, S. (2014). “What difference does quantity make? On the epistemology of Big Data in biology:”. En. In: Big Data & Society. Publisher: SAGE PublicationsSage UK: London, England. (Visited on Mar. 28, 2020).\n Onuoha, Mimi (2016). The Point of Collection. En. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#september-10-2024",
    "href": "schedule.html#september-10-2024",
    "title": "Schedule",
    "section": "September 10, 2024",
    "text": "September 10, 2024\n\nMetaphors and Binary Oppositions of Big Data\n\nDue TodayFurther Resources\n\n\n Levy Karen, Tim Hwang (2015). ‘The Cloud’ and Other Dangerous Metaphors. En. Section: Technology. (Visited on Aug. 29, 2021). Read in Perusall\n Puschmann, Cornelius and Jean Burgess (2014). “Metaphors of Big Data”. En. In: International Journal of Communication 8.0, p. 20. (Visited on May. 02, 2016). Read in Perusall\n\n\n Course slides are here\n Watson, Sarah M. (2021). Metaphors of Big Data. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#september-12-2024",
    "href": "schedule.html#september-12-2024",
    "title": "Schedule",
    "section": "September 12, 2024",
    "text": "September 12, 2024\n\nDiscourse Analysis\n\nDue TodayFurther Resources\n\n\n Fill out CATME Survey (link sent to your email)\n DM Professor if you’d like to lead a class discussion\n\n\n Course slides are here\n Discourse Analysis in Nine Steps is here\n Here is the article we will engage in today’s activity."
  },
  {
    "objectID": "schedule.html#september-17-2024",
    "href": "schedule.html#september-17-2024",
    "title": "Schedule",
    "section": "September 17, 2024",
    "text": "September 17, 2024\n\nThick Data for Big Data\n\nDue TodayFurther Resources\n\n\n Fiore-Silfvast, Brittany (2014). Hacked Ethnographic Fieldnotes. En. (Visited on Feb. 18, 2021). Read in Perusall\n Burrell, Jenna (2012). The Ethnographer’s Complete Guide to Big Data: Small Data People in a Big Data World. (Visited on Aug. 20, 2021). Read in Perusall\n Start working on Team Contract\n DM Professor if you’d like to lead a class discussion\n Start working on Fieldnote 1\n\n\n Course slides are here\n Here’s an example of some very short “thick description” write-ups of two data environments from my own research.\n Wang, Tricia (2013). Big Data Needs Thick Data. (Visited on Sep. 10, 2019)."
  },
  {
    "objectID": "schedule.html#september-19-2024",
    "href": "schedule.html#september-19-2024",
    "title": "Schedule",
    "section": "September 19, 2024",
    "text": "September 19, 2024\n\nEthnography in Data Land\n\nDue TodayFurther Resources\n\n\n Introduction , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1. Read in Perusall\n Continue working on Fieldnote 1\n\n\n Course slides are here"
  },
  {
    "objectID": "schedule.html#september-24-2024",
    "href": "schedule.html#september-24-2024",
    "title": "Schedule",
    "section": "September 24, 2024",
    "text": "September 24, 2024\n\nDocumenting Datasets\n\nDue TodayFurther Resources\n\n\n Denton, Emily, Alex Hanna, Razvan Amironesei, et al. (2021). “On the Genealogy of Machine Learning Datasets: A Critical History of ImageNet”. In: Big Data & Society 8.2, p. 20539517211035955. (Visited on Jan. 05, 2022). Read in Perusall\n Continue working on Fieldnote 1\n\n\n Course slides are here\n Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, et al. (2020). “Datasheets for Datasets”. In: arXiv:1803.09010 [cs]. arXiv: 1803.09010. (Visited on Jan. 24, 2021).\n Bender, Emily M. and Batya Friedman (2018). “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science”. In: Transactions of the Association for Computational Linguistics 6, pp. 587-604. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#september-26-2024",
    "href": "schedule.html#september-26-2024",
    "title": "Schedule",
    "section": "September 26, 2024",
    "text": "September 26, 2024\n\nEthnographies of Infrastructure\n\nDue TodayFurther Resources\n\n\n Star, Susan Leigh (1999). “The Ethnography of Infrastructure”. En. In: American Behavioral Scientist 43.3, pp. 377-391. (Visited on Feb. 18, 2016). Read in Perusall\n Get approval for dataset\n Team Contract Due\n Fieldnote 1 Due\n\n\n Course slides are here\n Lampland, Martha and Susan Leigh Star, ed. (2008). Standards and Their Stories: How Quantifying, Classifying, and Formalizing Practices Shape Everyday Life. 1 edition. Ithaca: Cornell University Press. ISBN: 978-0-8014-7461-3.\n Ottinger, Gwen (2010). “Buckets of Resistance: Standards and the Effectiveness of Citizen Science”. En. In: Science, Technology, & Human Values 35.2, pp. 244-270. (Visited on Oct. 05, 2019).\n Timmermans, Stefan and Steven Epstein (2010). “A World of Standards but not a Standard World: Toward a Sociology of Standards and Standardization*“. In: Annual Review of Sociology 36.1, pp. 69-89. (Visited on Oct. 16, 2014)."
  },
  {
    "objectID": "schedule.html#october-01-2024",
    "href": "schedule.html#october-01-2024",
    "title": "Schedule",
    "section": "October 01, 2024",
    "text": "October 01, 2024\n\nMountain Day!\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#october-03-2024",
    "href": "schedule.html#october-03-2024",
    "title": "Schedule",
    "section": "October 03, 2024",
    "text": "October 03, 2024\n\nInfrastructure Analysis\n\nDue TodayFurther Resources\n\n\n Bowker, Geoffrey C. (1998). “The Kindness of Strangers: Kinds and Politics in Classification Systems”. En. In: Library Trends 47.2, pp. 255-292. (Visited on Oct. 14, 2019). Read in Perusall\n Work on semiotic analysis\n Continue working on Fieldnote 2\n Watch Video introducing Mini-Project 1 in Perusall\n Start working on Mini-Project 1\n Be sure to get approval for the TED Talks you plan to view for Mini-Project 1.\n\n\n Course slides are here\n ICD-11\n Infrastructural Analysis Worksheet\n Bowker, Geoffrey C. and Susan Leigh Star (1999). Sorting Things Out: Classification and Its Consequences. En. Cambridge, MA: MIT Press. ISBN: 978-0-262-52295-3.\n Waterton, Claire (2002). “From Field to Fantasy: Classifying Nature, Constructing Europe”. En. In: Social Studies of Science 32.2, pp. 177-204. (Visited on May. 15, 2019).\n Kirksey, Eben (2015). “Species: a praxiographic study”. Fr. In: Journal of the Royal Anthropological Institute 21.4, pp. 758-780. (Visited on Oct. 05, 2019)."
  },
  {
    "objectID": "schedule.html#october-08-2024",
    "href": "schedule.html#october-08-2024",
    "title": "Schedule",
    "section": "October 08, 2024",
    "text": "October 08, 2024\n\nData Ghost Work\n\nDue TodayFurther Resources\n\n\n Chapter 1 , Gray, Mary L. and Siddharth Suri (2019). Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass. Illustrated edition. Boston: Mariner Books. ISBN: 978-1-328-56624-9. Read in Perusall\n Work on peopling the data\n Continue working on Fieldnote 2\n Continue working on Mini-Project 1\n\n\n Course slides are here\n Irani, Lilly (2015). Justice for “Data Janitors”. En-US. (Visited on Dec. 13, 2018).\n Plantin, Jean-Christophe (2019). “Data Cleaners for Pristine Datasets: Visibility and Invisibility of Data Processors in Social Science”. En. In: Science, Technology, & Human Values 44.1. Publisher: SAGE Publications Inc, pp. 52-73. (Visited on Aug. 20, 2021).\n Forsythe, Diana E. (1993). “The Construction of Work in Artificial Intelligence”. En. In: Science, Technology, & Human Values 18.4. Publisher: SAGE Publications Inc, pp. 460-479. (Visited on Aug. 20, 2021)."
  },
  {
    "objectID": "schedule.html#october-10-2024",
    "href": "schedule.html#october-10-2024",
    "title": "Schedule",
    "section": "October 10, 2024",
    "text": "October 10, 2024\n\nBoundary Drawing and Social Constructions of Expertise in Data Work\n\nDue TodayFurther Resources\n\n\n Tanweer, Anissa and James Steinhoff (2023). “Academic Data Science: Transdisciplinary and Extradisciplinary Visions”. In: Social Studies of Science, p. 03063127231184443. (Visited on Jan. 02, 2024). Read in Perusall\n Work on peopling the data\n Fieldnote 2 Due\n Continue working on Mini-Project 1\n\n\n Course slides are here\n Gieryn, Thomas F. (1999). Cultural Boundaries of Science: Credibility on the Line. En. University of Chicago Press. ISBN: 978-0-226-29261-8."
  },
  {
    "objectID": "schedule.html#october-15-2024",
    "href": "schedule.html#october-15-2024",
    "title": "Schedule",
    "section": "October 15, 2024",
    "text": "October 15, 2024\n\nAutumn Recess\n\nDue TodayFurther Resources\n\n\n Work on peopling the data\n Start working on Fieldnote 3\n Continue working on Mini-Project 1"
  },
  {
    "objectID": "schedule.html#october-17-2024",
    "href": "schedule.html#october-17-2024",
    "title": "Schedule",
    "section": "October 17, 2024",
    "text": "October 17, 2024\n\nIn-Class Interview\n\nDue TodayFurther Resources\n\n\n Work on peopling the data\n Mini-Project 1 Due\n Continue working on Fieldnote 3"
  },
  {
    "objectID": "schedule.html#october-22-2024",
    "href": "schedule.html#october-22-2024",
    "title": "Schedule",
    "section": "October 22, 2024",
    "text": "October 22, 2024\n\nFeeling and Sensing Data\n\nDue TodayFurther Resources\n\n\n Garnett, Emma (2016). “Developing a feeling for error: Practices of monitoring and modelling air pollution data”. En. In: Big Data & Society 3.2, p. 2053951716658061. (Visited on Sep. 24, 2019). Read in Perusall\n Work on ritual analysis\n Continue working on Fieldnote 3\n Start working on Mini-Project 2\n\n\n Course slides are here\n Lorimer, Jamie (2008). “Counting Corncrakes: The Affective Science of the UK Corncrake Census”. En. In: Social Studies of Science 38.3, pp. 377-405. (Visited on May. 16, 2019)."
  },
  {
    "objectID": "schedule.html#october-24-2024",
    "href": "schedule.html#october-24-2024",
    "title": "Schedule",
    "section": "October 24, 2024",
    "text": "October 24, 2024\n\nHow Data Domesticates Us: Rituals for Data Cleaning\n\nDue TodayFurther Resources\n\n\n Ribes, David and Steven J Jackson (2013). “Data bite man: The work of sustaining a long-term study”. In: Raw data” is an oxymoron. Ed. by Lisa Gitelman. Cambridge, MA: MIT Press, pp. 147-166. Read in Perusall\n Work on ritual analysis\n Group evaluations open\n Fieldnote 3 Due\n Continue working on Mini-Project 2\n MP 1 Peer Review Submission open\n\n\n Course slides are here\n Bowker, Geoffrey C. (2000). “Biodiversity Datadiversity”. En. In: Social Studies of Science 30.5, pp. 643-683. (Visited on May. 14, 2014).\n Walford, Antonia (2017). “Raw Data: Making Relations Matter”. En_US. In: Social Analysis 61.2. Publisher: Berghahn Journals Section: Social Analysis, pp. 65-80. (Visited on Aug. 20, 2021).\n Pink, Sarah, Shanti Sumartojo, Deborah Lupton, et al. (2017). “Mundane data: The routines, contingencies and accomplishments of digital living”. En. In: Big Data & Society 4.1. Publisher: SAGE Publications Ltd, p. 2053951717700924. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#october-29-2024",
    "href": "schedule.html#october-29-2024",
    "title": "Schedule",
    "section": "October 29, 2024",
    "text": "October 29, 2024\n\nParticipant Observation\n\nDue TodayFurther Resources\n\n\n Work on ritual analysis\n MP 1 Peer Review Submission close and assessment opens\n Continue working on Mini-Project 2\n Start working on Fieldnote 4\n\n\n Mid-semester check-in instructions here\n Participant Observation Worksheet here"
  },
  {
    "objectID": "schedule.html#october-31-2024",
    "href": "schedule.html#october-31-2024",
    "title": "Schedule",
    "section": "October 31, 2024",
    "text": "October 31, 2024\n\nData Walk\n\nDue TodayFurther Resources\n\n\n Work on user guide\n Continue working on Fieldnote 4\n Continue working on Mini-Project 2\n Continue working on Peer Review"
  },
  {
    "objectID": "schedule.html#november-05-2024",
    "href": "schedule.html#november-05-2024",
    "title": "Schedule",
    "section": "November 05, 2024",
    "text": "November 05, 2024\n\nWork on Group Projects in Class\n\nDue TodayFurther Resources\n\n\n Group Evaluations Due\n Work on user guide\n Continue working on Fieldnote 4\n Continue working on Mini-Project 2\n Continue working on Peer Review"
  },
  {
    "objectID": "schedule.html#november-07-2024",
    "href": "schedule.html#november-07-2024",
    "title": "Schedule",
    "section": "November 07, 2024",
    "text": "November 07, 2024\n\nEconomies of Data Production\n\nDue TodayFurther Resources\n\n\n Chapter 3 , Biruk, Cal (2018). Cooking Data: Culture and Politics in an African Research World. Illustrated edition. Durham: Duke University Press Books. ISBN: 978-0-8223-7074-1. Read in Perusall\n Work on institutional analysis\n Work on Fieldnote 4 (deadline extended to Tuesday)\n Work on MP Peer Review in Moodle (deadline extended to Tuesday)\n Continue working on Mini-Project 2\n\n\n Course slides are here\n Institutions Worksheet\n Gerlitz, Carolin and Anne Helmond (2013). “The like economy: Social buttons and the data-intensive web”. En. In: New Media & Society 15.8. Publisher: SAGE Publications, pp. 1348-1365. (Visited on Aug. 30, 2021).\n Beer, David (2015). “Productive measures: Culture and measurement in the context of everyday neoliberalism”. En. In: Big Data & Society 2.1. Publisher: SAGE Publications Ltd, p. 2053951715578951. (Visited on Aug. 29, 2021)."
  },
  {
    "objectID": "schedule.html#november-12-2024",
    "href": "schedule.html#november-12-2024",
    "title": "Schedule",
    "section": "November 12, 2024",
    "text": "November 12, 2024\n\nCromwell Day\n\nDue TodayFurther Resources\n\n\n Work on institutional analysis\n Fieldnote 4 Due\n MP 1 Peer Review Due\n Continue working on Mini-Project 2"
  },
  {
    "objectID": "schedule.html#november-14-2024",
    "href": "schedule.html#november-14-2024",
    "title": "Schedule",
    "section": "November 14, 2024",
    "text": "November 14, 2024\n\nNo Class - Work on Group Projects on your Own\n\nDue TodayFurther Resources\n\n\n Work on discourse analysis"
  },
  {
    "objectID": "schedule.html#november-19-2024",
    "href": "schedule.html#november-19-2024",
    "title": "Schedule",
    "section": "November 19, 2024",
    "text": "November 19, 2024\n\nMobilizing Data: Making Numbers Actionable\n\nDue TodayFurther Resources\n\n\n Ottinger, Gwen and Rachel Zurer (2011). New Voices, New Approaches: Drowning in Data. En-US. (Visited on Dec. 13, 2018). Read in Perusall\n Work on discourse analysis\n Mini-Project 2 Due\n MP 2 Peer Review Submission Opens\n\n\n Course slides are here.\n Pine, Kathleen H. and Max Liboiron (2015). “The Politics of Measurement and Action”. In: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery, pp. 3147-3156. ISBN: 978-1-4503-3145-6. (Visited on Aug. 30, 2021)."
  },
  {
    "objectID": "schedule.html#november-21-2024",
    "href": "schedule.html#november-21-2024",
    "title": "Schedule",
    "section": "November 21, 2024",
    "text": "November 21, 2024\n\nMaking Data Meaningful\n\nDue TodayFurther Resources\n\n\n Poirier, Lindsay “Enacting Data Context: Fixing Meaning in Transparency Data Initiatives”. In: Big Data and Society. Read in Perusall\n Work on user guide\n Start working on Fieldnote 5\n Start working on Mini-Project Revisions\n MP 2 Peer Review Submission Opens\n\n\n Course slides are here.\n Dourish, Paul and Edgar Gómez Cruz (2018). “Datafication and data fiction: Narrating data and narrating with data”. En. In: Big Data & Society 5.2. Publisher: SAGE Publications Ltd, p. 2053951718784083. (Visited on Apr. 05, 2021)."
  },
  {
    "objectID": "schedule.html#november-26-2024",
    "href": "schedule.html#november-26-2024",
    "title": "Schedule",
    "section": "November 26, 2024",
    "text": "November 26, 2024\n\nData Activism and Advocacy\n\nDue TodayFurther Resources\n\n\n Liboiron, Max (2015). “Disaster Data, Data Activism : Grassroots Responses to Representing Superstorm Sandy”. En. In: Extreme Weather and Global Media. Ed. by Julia Leyda and Diane Negra. Taylor & Francis Group. (Visited on Aug. 27, 2019). Read in Perusall\n First Draft Due\n MP 2 Peer Review Submission Closes and Assessment Opens\n Continue working on Fieldnote 5\n Continue working on Mini-Project Revisions\n\n\n Course slides are here\n Bruno, Isabelle, Emmanuel Didier, and Tommaso Vitale (2014). Statactivism: Forms of Action between Disclosure and Affirmation. En. SSRN Scholarly Paper ID 2466882. Rochester, NY: Social Science Research Network. (Visited on Dec. 18, 2018).\n Milan, Stefania and Lonneke van der Velden (2016). “The Alternative Epistemologies of Data Activism”. In: Digital Culture & Society 2.2, pp. 57-74. (Visited on Jul. 16, 2019).\n Currie, Morgan, Britt S Paris, Irene Pasquetto, et al. (2016). “The conundrum of police officer-involved homicides: Counter-data in Los Angeles County”. En. In: Big Data & Society 3.2, p. 2053951716663566. (Visited on Aug. 08, 2018)."
  },
  {
    "objectID": "schedule.html#november-28-2024",
    "href": "schedule.html#november-28-2024",
    "title": "Schedule",
    "section": "November 28, 2024",
    "text": "November 28, 2024\n\nThanksgiving Break\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-03-2024",
    "href": "schedule.html#december-03-2024",
    "title": "Schedule",
    "section": "December 03, 2024",
    "text": "December 03, 2024\n\nData Agnotology: Ignorance and Knowledge Gaps\n\nDue TodayFurther Resources\n\n\n Kim, Youngrim, Megan Finn, Amelia Acker, et al. (2024). “Epistemologies of Missing Data: COVID Dashboard Builders and the Production and Maintenance of Marginalized COVID Data”. In: Big Data & Society 11.2, p. 20539517241259666. (Visited on Jun. 25, 2024). Read in Perusall\n Continue working on Fieldnote 5\n Continue working on Mini-Project Revisions\n MP Peer Review Due\n\n\n Course slides are here\n mimimimimi (2021). On Missing Data Sets. original-date: 2016-02-03T16:30:28Z. (Visited on Aug. 20, 2021).\n Milan, Stefania and Emiliano Treré (2020). “The Rise of the Data Poor: The COVID-19 Pandemic Seen From the Margins”. En. In: Social Media + Society 6.3. Publisher: SAGE Publications Ltd, p. 2056305120948233. (Visited on Aug. 31, 2021).\n D’Ignazio, Catherine and Lauren F. Klein (2020). Data Feminism. Cambridge, Massachusetts: The MIT Press. ISBN: 978-0-262-04400-4."
  },
  {
    "objectID": "schedule.html#december-05-2024",
    "href": "schedule.html#december-05-2024",
    "title": "Schedule",
    "section": "December 05, 2024",
    "text": "December 05, 2024\n\nFinal Projects\n\nDue TodayFurther Resources\n\n\n Work on user guide revisions\n Fieldnote 5 Due"
  },
  {
    "objectID": "schedule.html#december-10-2024",
    "href": "schedule.html#december-10-2024",
    "title": "Schedule",
    "section": "December 10, 2024",
    "text": "December 10, 2024\n\nFinal Projects\n\nDue TodayFurther Resources\n\n\n Final Project Due\n Community Labor Due\n MP Revisions Due"
  },
  {
    "objectID": "assignments/discourse-analysis.html",
    "href": "assignments/discourse-analysis.html",
    "title": "Discourse Analysis",
    "section": "",
    "text": "This checkpoint aims to document some of the dominant discourses that shape your final project dataset."
  },
  {
    "objectID": "assignments/discourse-analysis.html#purpose-and-instructions",
    "href": "assignments/discourse-analysis.html#purpose-and-instructions",
    "title": "Discourse Analysis",
    "section": "",
    "text": "This checkpoint aims to document some of the dominant discourses that shape your final project dataset."
  },
  {
    "objectID": "assignments/discourse-analysis.html#part-1-identify-discursive-sites",
    "href": "assignments/discourse-analysis.html#part-1-identify-discursive-sites",
    "title": "Discourse Analysis",
    "section": "Part 1: Identify Discursive Sites",
    "text": "Part 1: Identify Discursive Sites\nFind six news articles, blog posts, op-eds, etc. that reference your dataset. Pay attention to the political lean of the publication. Download each as PDFs, and then open the PDF in an annotation software. Highlight statements in the article referring to the value, use, or quality of your dataset. In the highlighted statements, underline specific words or phrases in the sentence that shape the tone of the sentence and indicate the author’s (or other’s) sentiments towards the dataset. Leave a comment next to the underlined word or phrase, indicating the tone that word or phrase suggested. Note that words don’t need to be sensationalist or flashy to suggest a tone. Phrases like “according to” suggest a detached or direct tone. You may wish to reference this set of “tone” words. Finally, circle terms that make up one half of a binary opposition (accurate/inaccurate, bias/unbiased, etc.)"
  },
  {
    "objectID": "assignments/discourse-analysis.html#part-2-analyze-three-quotes",
    "href": "assignments/discourse-analysis.html#part-2-analyze-three-quotes",
    "title": "Discourse Analysis",
    "section": "Part 2: Analyze Three Quotes",
    "text": "Part 2: Analyze Three Quotes\nSelect three quotes from your analysis that capture a variety of predominant sentiments towards this dataset. Be sure to cite each. Fill them in below. Following each, write a short caption to describe how the statement is reflective of broader sentiments towards the dataset. Be sure to distinguish between the author’s sentiments towards the dataset, and how the author conveys others’ sentiments towards the dataset.\n\nExample:\n\n“The U.S. Environmental Protection Agency has released its Toxic Release Inventory for 2016 and the raw numbers don’t show a whole lot of difference over the past five years, but they show a significant decline over the past decade.”\n\n“17 Groups Petition EPA For Public Reporting Of Chemical Releases From Fracking, Other Oil And Gas Operations.” 2012. CNBC. October 24, 2012. https://www.cnbc.com/2012/10/24/17-groups-petition-epa-for-public-reporting-of-chemical-releases-from-fracking-other-oil-and-gas-operations.html.\nThe Toxic Release Inventory is often portrayed as documenting “raw” information about the annual state of pollution in the US. The data is leveraged by journalists and activists to convey evidence of changing environmental health conditions in the US. While certain environmental advocates and activists celebrate the publication of this data as a form of governmental transparency and accountability, they also lament its failure to address particular environmental issues warranting public concern:\n\n“The Toxics Release Inventory brings daylight to dark corners by requiring companies to quantify and report their pollution to a public data base for everyone to see,” said Environmental Integrity Project Director Eric Schaeffer. “That makes it easier for communities to measure the environmental impact of local industries, motivates companies to reduce their emissions, and gives all of us insight into how well our environmental laws are working. The EPA estimates the oil and gas industry releases 127,000 tons of hazardous air pollutants every year, second only to power plants and more than any of the other industries already reporting to TRI. Why shouldn’t oil and gas companies be required to report these toxic releases under our Right-to-Know laws, like so many other industries already do?”\n\n“Dealing with Emissions and Waste Can Cost Businesses Multiple Millions.” 2018. AP NEWS. February 11, 2018. https://apnews.com/article/496364d1d8aa48c29bda249e1e1313ff.\nSo while the data is often cast in a revelatory way, it is also critiqued for its shortcomings. In such cases, the data’s “raw”-ness gets implicitly called into question as they fail to reveal a comprehensive state of environmental health conditions.\n\n\nQuote 1\nFill your response here.\n\n\nQuote 2\nFill your response here.\n\n\nQuote 3\nFill your response here."
  },
  {
    "objectID": "assignments/peopling-the-data.html",
    "href": "assignments/peopling-the-data.html",
    "title": "Peopling the Data",
    "section": "",
    "text": "This checkpoint is adapted from an experimental ethnographic methods exercise designed by my Ph.D. advisor Kim Fortun (UCI, Anthropology). She called it “peopling a project.” The goal will be to draw to the surface the (typically invisible) human labor behind this dataset. Complete parts 1 and 2 below."
  },
  {
    "objectID": "assignments/peopling-the-data.html#purpose-and-instructions",
    "href": "assignments/peopling-the-data.html#purpose-and-instructions",
    "title": "Peopling the Data",
    "section": "",
    "text": "This checkpoint is adapted from an experimental ethnographic methods exercise designed by my Ph.D. advisor Kim Fortun (UCI, Anthropology). She called it “peopling a project.” The goal will be to draw to the surface the (typically invisible) human labor behind this dataset. Complete parts 1 and 2 below."
  },
  {
    "objectID": "assignments/peopling-the-data.html#part-1-list-stakeholders-from-relevant-social-groups",
    "href": "assignments/peopling-the-data.html#part-1-list-stakeholders-from-relevant-social-groups",
    "title": "Peopling the Data",
    "section": "Part 1: List stakeholders from relevant social groups",
    "text": "Part 1: List stakeholders from relevant social groups\nTo start, create a long list of people that are implicated in the production, aggregation, or circulation of this dataset. It can be really difficult to find people associated with a data collection program. Here are three strategies for “peopling this dataset”.\n\nStart at LinkedIn. Enter the name of your dataset in quotation marks (e.g. “National Bridge Inventory”) in the search bar. What kind of positions come up?\nSearch Google News for your dataset in quotation marks. What kinds of people are quoted in news articles about the dataset?\nSearch Twitter for your dataset in quotation marks. What kinds of people are tweeting about the dataset?\nCheck to see if there is an organization chart for the agency that runs the dataset. Can you figure out where the dataset is managed, and who runs that program?\n\nTwo important notes:\n\nYour list should be specific. You should provide more specificity than “data collectors” or “government agency employees” for instance. Try to identify who the data collectors are and what roles at which government agencies are implicated in the data collection.\nWhile you should be specific, you don’t need to list actual human names. You might list a person’s role (e.g. statistician at the EPA)\nBe careful not to confuse people with institutions. Congress, for instance, is an institution. Government is an institution. Identify and name a specific type of person within this institution for this exercise."
  },
  {
    "objectID": "assignments/peopling-the-data.html#part-2-profile-a-social-group",
    "href": "assignments/peopling-the-data.html#part-2-profile-a-social-group",
    "title": "Peopling the Data",
    "section": "Part 2: Profile a social group",
    "text": "Part 2: Profile a social group\nSelect at least three listed social groups. Profile each group below by answering some combination of the questions belows. You should aim to write 200 words per social group.\n\nI encourage you not to make assumptions about a particular social group, but to search for relevant literature, media, or data pertaining to this social group. To enrich this section of the document, you may find YouTube videos of these individuals at work or speaking at a conference or with a news anchor. Feel free to embed those in the Markdown. Alternatively, you might find training guides or protocols directed at these social groups. Or maybe you’ll track down Glassdoor profiles that depict information about their jobs. You might also check out data published by the Bureau of Labor Statistics about typical compensation for these jobs broken down by demographic indicators. Be sure to cite all sources.\n\n\nHow are their bodies implicated in the production of the data? What senses do they engage in their work with the data? What kinds of movements and exertions must they make in relation to the data?\nWhat do their day-to-day jobs look like or entail?\nWhat kind of education or training do they have?\nWhat interest do they have in the data or incentives to report the data in a particular way?\nWhat kind of critical judgment calls must they make when producing, aggregating, or circulating the data?\nHow is the work and standpoints of these social groups valued or validated?\nWhat social norms or discourses guide how they engage with the data?\nWhat protocols or regulations are they required to abide by?\nHow do people in this social group identify themselves?\nWhat is the demographic make-up of the social group?\n\n\nSocial group 1:\nFill your response here.\n\n\nSocial group 2:\nFill your response here.\n\n\nSocial group 3:\nFill your response here."
  },
  {
    "objectID": "slides/Day1-Intro.html#my-pedagogy",
    "href": "slides/Day1-Intro.html#my-pedagogy",
    "title": "Day One: Introductions",
    "section": "My Pedagogy",
    "text": "My Pedagogy\n\nInviting students to question taken-for-granted ideas and discourses\nPrioritizing growth in critical thinking and awareness over mastery of concepts\nBalancing consistent practice with in-depth engagement\nConfronting complex concepts through diverse learning modes\nPrioritizing an ability to question/grapple over an ability to resolve/comply"
  },
  {
    "objectID": "slides/Day1-Intro.html#course-set-up-google-drive",
    "href": "slides/Day1-Intro.html#course-set-up-google-drive",
    "title": "Day One: Introductions",
    "section": "Course Set-up: Google Drive",
    "text": "Course Set-up: Google Drive\n\nCreate a folder in Google Drive called: [YOUR-NAME-SDS-237]\nShare the folder with me (Right click on the folder and Share with lpoirier@smith.edu)\nCreate a copy of this spreadsheet\nMove the spreadsheet into the folder you shared with me\nEdit the name in the title to your name"
  },
  {
    "objectID": "slides/Day1-Intro.html#course-set-up-slack",
    "href": "slides/Day1-Intro.html#course-set-up-slack",
    "title": "Day One: Introductions",
    "section": "Course Set-up: Slack",
    "text": "Course Set-up: Slack\n\nInstall Slack Desktop on your computer (Mac/Windows)\nLog-in using your Smith credentials to access the Smith College workspace\nFind the #general SDS-237-01-202501, right click on it, Move Channel &gt; Move to New Section. Name the section SDS 237.\nRepeat these steps for the following three channels\n\n\n#random SDS-237-01-202501\n#sds-237-questions SDS-237-01-202501\n#sds-237-discussions SDS-237-01-202501\n\nIf you want to get email notifications for Slack, click your profile picture &gt; Preferences, scroll the bottom, and check “Send me email notifications for mentions and direct messages”"
  },
  {
    "objectID": "slides/Day1-Intro.html#homework",
    "href": "slides/Day1-Intro.html#homework",
    "title": "Day One: Introductions",
    "section": "Homework",
    "text": "Homework\n\nFirst reading in Perusall\nSyllabus Quiz\nClass notes sign-up; First day of class questionnaire; Trigger warnings form"
  },
  {
    "objectID": "syllabus_pdf.html",
    "href": "syllabus_pdf.html",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "Course logo designed by Zoe Scheffler, Fall ’21\n\n\n\n\n\nWithout rich documentation, quantitative data can strip away critical context needed to interpret values responsibly. This course introduces students to the theory and practice of data ethnography, demonstrating how qualitative data collection and analysis can be brought to bear on the study of data settings and artifacts. Through experiential exercises, students will learn techniques in field-note writing, participant observation, in-depth interviewing, documentary analysis, and archival research and how they may be used to contextualize the cultural underpinnings of datasets. Students will learn how to visualize datasets in ways that foreground their socio-political provenance in R. Students will also learn how ethnographic methods can be leveraged to improve data documentation and communication. The course will introduce debates regarding the politics of techno-scientific fieldwork. Upon completion of this course, you should be able to:\n\nRecognize, interpret, and communicate the cultural underpinnings of data resources, infrastructures, and practices\nDefine and engage principal methods of ethnography and communicate when and how they may be used in the study of data\nStrategically apply relevant ethnographic methods to the study of a data setting or artifact\nProduce data documentation and visualizations that account for the cultural contexts of data production, categorization, measurement, and use\n\nClasses will be held on Tuesdays and Thursdays from 2:45 PM to 4:00 PM.\n\n\n\n\n\n\n\n\nLindsay Poirier, she/her/hers.\n\n\n\n\n\nI am a cultural anthropologist that studies how civic data gets produced, how communities think about and interface with data, and how data infrastructure can be designed more equitably. My Ph.D. is in an interdisciplinary discipline called Science and Technology Studies - a field that studies the intricate ways science, technology, culture, and politics all co-constitute each other. I work on a number of collaborative research projects that leverage public data to deepen understanding of social and environmental inequities in the US, while also qualitatively studying the politics behind data gaps and inconsistencies. As an instructor, I prioritize active learning and often structure courses as flipped classrooms. You can expect in-class time to predominantly involve group activities and live problem-solving exercises.\n\n\n\nSlackMeeting with Me\n\n\nI can best support students in this course when I can readily keep tabs on our course-related communication. Because of this, I ask that you please don’t email me regarding course-related questions or issues. The best way to get in touch with me is via our course Slack. If you have course-related questions, I encourage you to ask them in the #sds-237-questions channel. When discretion is needed, feel free to DM. Please reserve more formal concerns like grades or accommodation requests for an in-person (or in-person virtual) conversation.\nDuring the week, I will try my best to answer all Slack messages within 24 hours of receiving them. Please note that to maintain my own work-life balance, I don’t answer Slack messages late in the evenings or on the weekends. It’s important that you plan when you start your assignments accordingly.\n\n\nMeeting with me outside of class is a great opportunity for us to chat about what you’re learning in the course, clarify expectations on assignments, and review work in progress. I also love when students drop in to office hours to request book recommendations, discuss career or research paths, or just to say hi!\nThere are two ways to meet with me. If you would like to have a one-on-one private conversation, I ask that you schedule an appointment with me via the booking form on Moodle. For support on class topics, you may drop-in during my regularly scheduled office hours.\n\nTuesday, 1:30PM-2:30PM, McConnell 214\nThursday, 4:15-5:15PM, McConnell 214\n\n\n\n\n\n\n\n\nI will make all course readings available on Perusall, which can be accessed through our course Moodle page.\n\n\n\nThis course will be graded via a grading contract.\n\n\n\n\n\n\nJacobson Center\n\n\n\nSmith’s Jacobson Center asserts that all students can improve their communication and learning skills. I encourage all students to take advantage of their writing support services, workshops, and tutors.\n\n\n\n\n\n\nPreparationAttendanceExtensionsAcademic HonestyGenerative AI\n\n\nThis is a 4-credit course with 3 hours per week of in-classroom instructions. Smith expects students to devote 9 out-of-class hours per week to 4-credit classes. I have designed the course assignments and selected the course readings with this target in mind.\n\n\nBecause this course aims to hone certain critical thinking skills, much of the learning happens in class, in collaboration with peers. Attendance in class is both important for your own learning, and for your peers’ learning, as we will spend a good chunk of time in class engaging in group activities and discussions.\nAttendance will be taken each class period. That said, we all have reasons we can’t be available from time-to-time. You may miss three classes with no penalty. You do not need to inform me that you will be absent in these cases, but you should let your group members know. After the third unexcused absence, your grade may drop by a modifier for each class missed. I understand that you may need to be absent beyond these three sessions. Additional absences may be excused due to family/personal difficulties, sickness, or school or career-related activities; however, I will require some form of documentation for these absences. Please speak with your class dean or the Accessibility Resource Center so that we can get documentation of your need.\nI also ask that you make every effort to arrive to class on time. This is a large course, and when students arrive late, it can be distracting for me as the instructor, and it can be distracting to other students in the course. It also makes it difficult for me to plan group activities. Students arriving more than 10 minutes late for class without having informed me ahead of time will be marked as absent.\nIf you must miss a class entirely, you should contact a peer to discuss what was missed. Please note that the SDS Program has adopted a shared policy regarding in-person attendance:\n\nIn keeping with Smith’s core identity and mission as an in-person, residential college, SDS affirms College policy (as per the Provost and Dean of the College) that students will attend class in person. SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Accessibility Resource Center will be the only exceptions to this policy. As with any other kind of ADA accommodations, please notify your instructor during the first week of classes to discuss how we can meet your accommodations.\n\n\n\nThis course aims to support you in learning to become more perceptive to data environments surrounding you. Because of this I’ve deliberately spread course assignments out over the semester in order to help you gain practice in consistently paying attention to these environments. This critical mode of thinking can’t be cultivated effectively if the work is being crammed into later parts of the semester. Further, because this course emphasizes student growth over achievement, I also want to ensure that I can provide feedback on your writing in time for you to incorporate it into future assignments.\nThere is a 24-hour grace period on all written assignments. There will be no penalties for submitting the written assignment within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any written assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted.\nNote that this policy does not apply to reading assignments/Perusall annotations. Reading assignments/Perusall annotations need to be completed by the due date for credit.\n\n\n\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. Students and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information, and honest and respectful use of college resources. Any cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\n\n\nSubmitting work completed by another student as your own.\nCopying and pasting words from sources without quoting and citing the author.\nParaphrasing material from another source without citing the author.\nFailing to cite your sources correctly.\nFalsifying or misrepresenting information in submitted work.\nPaying another student or service to complete assignments for you.\nSubmitting work generated by artificially intelligent tools such as chatGPT\n\n\n\nThe use of generative artificial intelligence tools is becoming increasingly ubiquitous in a number of domains of work. Despite the urge to push forward with this innovative technology, there are both pedagogical and social reasons that I do not allow students to use generative AI in the context of this course.\nThis course aims to facilitate the development of your ability to think critically and express your ideas with care. The depth of your learning in this course cannot be assessed if Generative AI is aiding or doing this work for you. This course also encourages reflexivity - i.e. I will ask you often throughout the semester to reflect on and communicate your personal standpoint and how it impacts the way you write about cultural phenomenon. Since many generative AI algorithms act as “black boxes” (i.e. we don’t fully know what data they were trained on and how they operate internally) it’s not possible to be fully account for the standpoint from which generative AI text was generated.\nFurther, there are countless unresolved social concerns that stem from the use of generative AI - from the loss of cultural heritage, to the limiting of human aspirations and viewpoints, to the lack of formal mechanisms for regulating its potential social harms. Generative AI models often undermine scholarly values of credit by training on data harvested from across the internet that content creators have not provided express permission to use for these purposes. As an example, several academic journal articles that I have written were recently sold to Microsoft AI; I did not have an opportunity to opt out of this, and I will not receive any compensation for my contributions. Copyright laws have not been able to keep pace with the technology. This course will teach you how to interrogate how these kinds of social harms can emerge from data-based systems and imagine opportunities for mitigating them; the course itself suggests taking a precautionary approach in the absence of regulation and other safeguards.\nAccordingly, any use of generative artificial intelligence in this course will be considered a case of academic dishonesty/plagiarism and will be reported to the Academic Honor Board.\n\n\n\n\n\n\n\nCode of ConductPrinciples of CommunityPronouns\n\n\nAs the instructor for this course, I am committed to making participation in this course a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned to this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0, available here.\n\n\nI hope that we can foster a collaborative and caring environment in this classroom: one that celebrates successes, respects individual strengths and weaknesses, demonstrates compassion for each other’s struggles, and affirms diverse identities. Here are some ideas that I have for creating this environment in our course:\n\nCheck-in with colleagues before starting collaborative work. “What three words describe how you’re feeling?” “Name one challenge and one success from this week.” “What are you doing for self-care right now?” Thank each other for sharing where they’re at.\nConsider when to step up and when to step back in class discussions, creating space for others to contribute. Listening is just as important to community-building as speaking.\nAcknowledge that there is much we don’t know about how our colleagues experience the world. …but don’t ask colleagues to speak on behalf of a social group you perceive them to be a part of.\nCheer on colleagues as they give presentations or try something out for the first time.\nAsk questions often in our #sds-237-questions channel. Help each other out by answering questions when you can.\nMistakes happen. I will certainly make mistakes in class. Admit mistakes, and then move on.\n\n\n\nUsing the proper pronouns for our students is foundational to a safe, respectful classroom environment that creates a culture of trust. For information on pronouns and usage, please see the Office of Equity and Inclusion link here: Pronouns\n\n\n\n\n\n\n\nAccommodationsStudent Well-beingTrigger Warnings\n\n\nIt is my goal for everyone to succeed in this course. If you have personal circumstances that may impact your experience of our classroom, I encourage you to contact Accessibility Resource Center in College Hall 104 or at arc@smith.edu. The Center will generate a letter that indicates to me what kind of support you need and how I can make your classroom experience more accommodating. Once you have this letter, you are welcome to visit my office hours or email me to discuss ideas about how we can tailor the course accordingly. While you can request accommodations at any time, the sooner we start this conversation, the better. If you have concerns about the course that are not addressed through ARC, please contact me. At no point will I ask you to divulge details about your personal circumstances to me.\n\n\nCollege life is stressful, and life outside of college can be overwhelming. It is my position that attending to your physical and mental health and well-being should be a top priority. I will remind you of this often throughout the semester. I encourage you to schedule a time to talk with me if you are struggling with this course. If you, or anyone you know, is experiencing distress, there are numerous campus resources that can provide support via the Schacht Center. I can point you to these resources at any time throughout the semester.\n\n\nA trigger is a topic or image that can precipitate an intense emotional response. When common triggering topics are to be covered in this course, I will do my best to provide a trigger warning in advance of the discussion. However, I can’t always anticipate triggers. With this in mind I’ve set up an anonymous form, available on Moodle, where you can indicate topics for which you would like me to provide a warning.\n\n\n\n\n\n\n\nMoodlePerusallGoogle DriveSlack\n\n\nGrades, forms, and handouts will be available on the course Moodle.\n\n\nAll course readings and recorded lectures will be available on Perusall. You can access Perusall via our course Moodle page.\n\n\nThis semester, you will submit all of your assignments in a Google Drive folder that you will create on the first day of class. You can write your assignments in Google Docs, or you can work on your own computer and upload files to Google Drive. I just need to be able to open the files on my computer.\n\n\nOutside of class almost all of our communication will happen via Slack. You can use the following channels\n\n#general: Course announcements (only I can post)\n#sds-237-discussions: Share news articles and relevant opportunities\n#sds-237-questions: Ask and answer questions about our course\n#sds-237-class-notes: Find notes that a designated student took for our class on a given day\nYou can also create private Slack channels with your project group members."
  },
  {
    "objectID": "syllabus_pdf.html#course-instructor",
    "href": "syllabus_pdf.html#course-instructor",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "Lindsay Poirier, she/her/hers.\n\n\n\n\n\nI am a cultural anthropologist that studies how civic data gets produced, how communities think about and interface with data, and how data infrastructure can be designed more equitably. My Ph.D. is in an interdisciplinary discipline called Science and Technology Studies - a field that studies the intricate ways science, technology, culture, and politics all co-constitute each other. I work on a number of collaborative research projects that leverage public data to deepen understanding of social and environmental inequities in the US, while also qualitatively studying the politics behind data gaps and inconsistencies. As an instructor, I prioritize active learning and often structure courses as flipped classrooms. You can expect in-class time to predominantly involve group activities and live problem-solving exercises.\n\n\n\nSlackMeeting with Me\n\n\nI can best support students in this course when I can readily keep tabs on our course-related communication. Because of this, I ask that you please don’t email me regarding course-related questions or issues. The best way to get in touch with me is via our course Slack. If you have course-related questions, I encourage you to ask them in the #sds-237-questions channel. When discretion is needed, feel free to DM. Please reserve more formal concerns like grades or accommodation requests for an in-person (or in-person virtual) conversation.\nDuring the week, I will try my best to answer all Slack messages within 24 hours of receiving them. Please note that to maintain my own work-life balance, I don’t answer Slack messages late in the evenings or on the weekends. It’s important that you plan when you start your assignments accordingly.\n\n\nMeeting with me outside of class is a great opportunity for us to chat about what you’re learning in the course, clarify expectations on assignments, and review work in progress. I also love when students drop in to office hours to request book recommendations, discuss career or research paths, or just to say hi!\nThere are two ways to meet with me. If you would like to have a one-on-one private conversation, I ask that you schedule an appointment with me via the booking form on Moodle. For support on class topics, you may drop-in during my regularly scheduled office hours.\n\nTuesday, 1:30PM-2:30PM, McConnell 214\nThursday, 4:15-5:15PM, McConnell 214"
  },
  {
    "objectID": "syllabus_pdf.html#course-texts",
    "href": "syllabus_pdf.html#course-texts",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "I will make all course readings available on Perusall, which can be accessed through our course Moodle page."
  },
  {
    "objectID": "syllabus_pdf.html#assessment",
    "href": "syllabus_pdf.html#assessment",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "This course will be graded via a grading contract.\n\n\n\n\n\n\nJacobson Center\n\n\n\nSmith’s Jacobson Center asserts that all students can improve their communication and learning skills. I encourage all students to take advantage of their writing support services, workshops, and tutors."
  },
  {
    "objectID": "syllabus_pdf.html#policies",
    "href": "syllabus_pdf.html#policies",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "PreparationAttendanceExtensionsAcademic HonestyGenerative AI\n\n\nThis is a 4-credit course with 3 hours per week of in-classroom instructions. Smith expects students to devote 9 out-of-class hours per week to 4-credit classes. I have designed the course assignments and selected the course readings with this target in mind.\n\n\nBecause this course aims to hone certain critical thinking skills, much of the learning happens in class, in collaboration with peers. Attendance in class is both important for your own learning, and for your peers’ learning, as we will spend a good chunk of time in class engaging in group activities and discussions.\nAttendance will be taken each class period. That said, we all have reasons we can’t be available from time-to-time. You may miss three classes with no penalty. You do not need to inform me that you will be absent in these cases, but you should let your group members know. After the third unexcused absence, your grade may drop by a modifier for each class missed. I understand that you may need to be absent beyond these three sessions. Additional absences may be excused due to family/personal difficulties, sickness, or school or career-related activities; however, I will require some form of documentation for these absences. Please speak with your class dean or the Accessibility Resource Center so that we can get documentation of your need.\nI also ask that you make every effort to arrive to class on time. This is a large course, and when students arrive late, it can be distracting for me as the instructor, and it can be distracting to other students in the course. It also makes it difficult for me to plan group activities. Students arriving more than 10 minutes late for class without having informed me ahead of time will be marked as absent.\nIf you must miss a class entirely, you should contact a peer to discuss what was missed. Please note that the SDS Program has adopted a shared policy regarding in-person attendance:\n\nIn keeping with Smith’s core identity and mission as an in-person, residential college, SDS affirms College policy (as per the Provost and Dean of the College) that students will attend class in person. SDS courses will not provide options for remote attendance. Students who have been determined to require a remote attendance accommodation by the Accessibility Resource Center will be the only exceptions to this policy. As with any other kind of ADA accommodations, please notify your instructor during the first week of classes to discuss how we can meet your accommodations.\n\n\n\nThis course aims to support you in learning to become more perceptive to data environments surrounding you. Because of this I’ve deliberately spread course assignments out over the semester in order to help you gain practice in consistently paying attention to these environments. This critical mode of thinking can’t be cultivated effectively if the work is being crammed into later parts of the semester. Further, because this course emphasizes student growth over achievement, I also want to ensure that I can provide feedback on your writing in time for you to incorporate it into future assignments.\nThere is a 24-hour grace period on all written assignments. There will be no penalties for submitting the written assignment within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any written assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted.\nNote that this policy does not apply to reading assignments/Perusall annotations. Reading assignments/Perusall annotations need to be completed by the due date for credit.\n\n\n\nSmith College expects all students to be honest and committed to the principles of academic and intellectual integrity in their preparation and submission of course work and examinations. Students and faculty at Smith are part of an academic community defined by its commitment to scholarship, which depends on scrupulous and attentive acknowledgement of all sources of information, and honest and respectful use of college resources. Any cases of dishonesty or plagiarism will be reported to the Academic Honor Board. Examples of dishonesty or plagiarism include:\n\n\nSubmitting work completed by another student as your own.\nCopying and pasting words from sources without quoting and citing the author.\nParaphrasing material from another source without citing the author.\nFailing to cite your sources correctly.\nFalsifying or misrepresenting information in submitted work.\nPaying another student or service to complete assignments for you.\nSubmitting work generated by artificially intelligent tools such as chatGPT\n\n\n\nThe use of generative artificial intelligence tools is becoming increasingly ubiquitous in a number of domains of work. Despite the urge to push forward with this innovative technology, there are both pedagogical and social reasons that I do not allow students to use generative AI in the context of this course.\nThis course aims to facilitate the development of your ability to think critically and express your ideas with care. The depth of your learning in this course cannot be assessed if Generative AI is aiding or doing this work for you. This course also encourages reflexivity - i.e. I will ask you often throughout the semester to reflect on and communicate your personal standpoint and how it impacts the way you write about cultural phenomenon. Since many generative AI algorithms act as “black boxes” (i.e. we don’t fully know what data they were trained on and how they operate internally) it’s not possible to be fully account for the standpoint from which generative AI text was generated.\nFurther, there are countless unresolved social concerns that stem from the use of generative AI - from the loss of cultural heritage, to the limiting of human aspirations and viewpoints, to the lack of formal mechanisms for regulating its potential social harms. Generative AI models often undermine scholarly values of credit by training on data harvested from across the internet that content creators have not provided express permission to use for these purposes. As an example, several academic journal articles that I have written were recently sold to Microsoft AI; I did not have an opportunity to opt out of this, and I will not receive any compensation for my contributions. Copyright laws have not been able to keep pace with the technology. This course will teach you how to interrogate how these kinds of social harms can emerge from data-based systems and imagine opportunities for mitigating them; the course itself suggests taking a precautionary approach in the absence of regulation and other safeguards.\nAccordingly, any use of generative artificial intelligence in this course will be considered a case of academic dishonesty/plagiarism and will be reported to the Academic Honor Board."
  },
  {
    "objectID": "syllabus_pdf.html#community",
    "href": "syllabus_pdf.html#community",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "Code of ConductPrinciples of CommunityPronouns\n\n\nAs the instructor for this course, I am committed to making participation in this course a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants in this course include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.\nAs the instructor I have the right and responsibility to point out and stop behavior that is not aligned to this Code of Conduct. Participants who do not follow the Code of Conduct may be reprimanded for such behavior. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the instructor.\nAll students and the instructor are expected to adhere to this Code of Conduct in all settings for this course: seminars, office hours, and over Slack.\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.0.0, available here.\n\n\nI hope that we can foster a collaborative and caring environment in this classroom: one that celebrates successes, respects individual strengths and weaknesses, demonstrates compassion for each other’s struggles, and affirms diverse identities. Here are some ideas that I have for creating this environment in our course:\n\nCheck-in with colleagues before starting collaborative work. “What three words describe how you’re feeling?” “Name one challenge and one success from this week.” “What are you doing for self-care right now?” Thank each other for sharing where they’re at.\nConsider when to step up and when to step back in class discussions, creating space for others to contribute. Listening is just as important to community-building as speaking.\nAcknowledge that there is much we don’t know about how our colleagues experience the world. …but don’t ask colleagues to speak on behalf of a social group you perceive them to be a part of.\nCheer on colleagues as they give presentations or try something out for the first time.\nAsk questions often in our #sds-237-questions channel. Help each other out by answering questions when you can.\nMistakes happen. I will certainly make mistakes in class. Admit mistakes, and then move on.\n\n\n\nUsing the proper pronouns for our students is foundational to a safe, respectful classroom environment that creates a culture of trust. For information on pronouns and usage, please see the Office of Equity and Inclusion link here: Pronouns"
  },
  {
    "objectID": "syllabus_pdf.html#support",
    "href": "syllabus_pdf.html#support",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "AccommodationsStudent Well-beingTrigger Warnings\n\n\nIt is my goal for everyone to succeed in this course. If you have personal circumstances that may impact your experience of our classroom, I encourage you to contact Accessibility Resource Center in College Hall 104 or at arc@smith.edu. The Center will generate a letter that indicates to me what kind of support you need and how I can make your classroom experience more accommodating. Once you have this letter, you are welcome to visit my office hours or email me to discuss ideas about how we can tailor the course accordingly. While you can request accommodations at any time, the sooner we start this conversation, the better. If you have concerns about the course that are not addressed through ARC, please contact me. At no point will I ask you to divulge details about your personal circumstances to me.\n\n\nCollege life is stressful, and life outside of college can be overwhelming. It is my position that attending to your physical and mental health and well-being should be a top priority. I will remind you of this often throughout the semester. I encourage you to schedule a time to talk with me if you are struggling with this course. If you, or anyone you know, is experiencing distress, there are numerous campus resources that can provide support via the Schacht Center. I can point you to these resources at any time throughout the semester.\n\n\nA trigger is a topic or image that can precipitate an intense emotional response. When common triggering topics are to be covered in this course, I will do my best to provide a trigger warning in advance of the discussion. However, I can’t always anticipate triggers. With this in mind I’ve set up an anonymous form, available on Moodle, where you can indicate topics for which you would like me to provide a warning."
  },
  {
    "objectID": "syllabus_pdf.html#infrastructure",
    "href": "syllabus_pdf.html#infrastructure",
    "title": "SDS 237: Data Ethnography",
    "section": "",
    "text": "MoodlePerusallGoogle DriveSlack\n\n\nGrades, forms, and handouts will be available on the course Moodle.\n\n\nAll course readings and recorded lectures will be available on Perusall. You can access Perusall via our course Moodle page.\n\n\nThis semester, you will submit all of your assignments in a Google Drive folder that you will create on the first day of class. You can write your assignments in Google Docs, or you can work on your own computer and upload files to Google Drive. I just need to be able to open the files on my computer.\n\n\nOutside of class almost all of our communication will happen via Slack. You can use the following channels\n\n#general: Course announcements (only I can post)\n#sds-237-discussions: Share news articles and relevant opportunities\n#sds-237-questions: Ask and answer questions about our course\n#sds-237-class-notes: Find notes that a designated student took for our class on a given day\nYou can also create private Slack channels with your project group members."
  },
  {
    "objectID": "syllabus_pdf.html#grade-breakdown",
    "href": "syllabus_pdf.html#grade-breakdown",
    "title": "SDS 237: Data Ethnography",
    "section": "Grade Breakdown",
    "text": "Grade Breakdown\nEach row indicates what labor you need to complete in the course to earn the grade indicated in the first column of that row. Note that to earn a particular grade all minimum labor criteria in the corresponding row must be met.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrade\nJournal Entries (out of 5)\nMini-projects\nFinal Project Checkpoints\nReading Annotations\nCommunity Labor Points\nCourse Absences/Late Arrivals\nAdvanced Assignments\n\n\n\n\nA\n4 or more\n2 + 1 substantive revision\n1 group contract + 1 draft + 1 final project\n13 or more\n8 or more\n&lt;=3\n3\n\n\nB\n4 or more\n2 + 1 substantive revision\n1 group contract + 1 draft + 1 final project\n13 or more\n8 or more\n&lt;=6\n0\n\n\nC\n3 or more\n1 + 1 substantive revision\n1 group contract + 1 draft + 1 final project\n10 or more\n6 or more\n&lt;=9\n0\n\n\nD\n2 or more\n1, no revision\n1 group contract + 1 draft + 1 final project\n7 or more\n4 or more\n&lt;=12\n0\n\n\nE\n1\n0\n0\n7 or fewer\n3 or fewer\n&lt;=15\n0\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAt the start of the course, I will assign a syllabus quiz. If you receive an 80% or better on that quiz, it can replace one of your reading annotation assignments. If you receive a 100% on that quiz, it can replace two of your reading annotation assignments.\n\n\n\nDeadlines\nAssignments are due before class (2:45PM EST) on the designated due date. Any granted extensions are similarly due before 2:45PM EST on the date the deadline is extended to.\n\n\nEarning an A\nYou’ll notice that, in most categories, there’s no difference in the quantity of labor you need to complete to earn an A vs. a B. Earning a B in this course demonstrates that you have completed enough work to meet the course’s learning goals. An A grade in this course indicates that you have exceeded expectations of the course’s learning goals. Please note that this does not mean that you need to do more labor (i.e. more assignments) to earn A. Instead, you need to consistently demonstrate levels of critical thinking on existing course assignments that surpass the minimum requirements.\nTo earn an A in this course, you will need to complete 3 course assignments at an advanced level. You will select the assignments that you want to complete at this level from the labor log, giving you an opportunity to choose where you want to engage deeper. The following chart helps breakdown some of the differences between meeting expectations and exceeding expectations:\n\n\n\n\n\n\n\nMeeting Expectations\nExceeding Expectations\n\n\n\n\nDefining and applying course concepts\nDrawing connections between course concepts, critiquing them, or extending them (i.e. offering new ways to think about them)\n\n\nCiting arguments from a course reading to support interpretation and ethnographic arguments\nEngaging more deeply with a course reading - unpacking how its arguments are supported and evaluating its relevance when applied to other ethnographic data\n\n\nImplementing an ethnographic research method\nDefending the selection of a research method and evaluating its strengths/limitations\n\n\nWriting up research findings to convey an argument\nExperimenting with the prose and form of argumentative writing with a clear analytic purpose\n\n\nReporting ethnographic findings in writing\nCritically situating the standpoints of your research findings throughout the writing\n\n\nAnalyzing how a social force shapes the constitution of data\nAnalyzing how multiple, at times competing, social forces shape the constitution of data\n\n\nClearly communicating the contexts that produced data in a format appropriate for a given audience\nCreatively playing with the style and medium of communication in order to better reach and engage an audience\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you may complete a reading annotation assignment at an advanced level by signing up to lead a 15-minute classroom discussion of that reading. I will remind you of this option throughout the first few weeks of the course. This is the only option for completing reading annotation assignments at an advanced level.\n\n\nThe labor log designates which assignments you may attempt to complete at an advanced level. You will be asked to justify why you consider your work on the assignment as advanced in the labor log. Please note that, when justifying why I should consider the work advanced in the labor log, you should clearly explain how the cognitive effort on the assignment surpasses minimum expectations. While you may end up writing more or spending more time on these assignments, exceeding the minimum word count or spending x hours on the assignment would not be considered adequate justification for advanced level work. Being able to justify why your work is advanced demonstrates to me that you have gained a deep understanding of the course’s learning objectives and that you have the ability to communicate what you’ve learned. While it is ultimately up to you to determine what constitutes advanced level work, I am happy to brainstorm ideas with you in office hours.\nPlease note that I reserve the right to deny advanced level designations after reviewing your work, and I also reserve the right to deem an assignment as having been completed at an advanced level, even when you haven’t designated it as such. If ultimately I deny a designation, you may attempt to earn the advanced level designation again. Note that this is a good reason not to wait until the end of the semester to attempt advanced level work.\nAttendance/late arrivals also separate A grades from B grades. Keep in mind that the final grade will be reduced by one grade modifier for each unexcused absence/late arrival beyond the number allowed for your contracted grade. This means that if you completed the work for an A, and you have 4 absences, the final grade will be an A-, and with 5 absences, it would be a B+. I chose to use grade modifiers here so that if you miss just one class beyond the first 3 allotted (i.e. 4 absences), it doesn’t drop your grade so drastically from an A to a B."
  },
  {
    "objectID": "syllabus_pdf.html#assignments",
    "href": "syllabus_pdf.html#assignments",
    "title": "SDS 237: Data Ethnography",
    "section": "Assignments",
    "text": "Assignments\nDescriptions of all written and project assignments are available here. For an assignment to be considered complete, it must meet the minimum criteria outlined in the assignment description. It also must be completed “in good faith” - meaning in a way that demonstrates integrity to the spirit of the assignment."
  },
  {
    "objectID": "syllabus_pdf.html#faq",
    "href": "syllabus_pdf.html#faq",
    "title": "SDS 237: Data Ethnography",
    "section": "FAQ",
    "text": "FAQ\n\nWhat kind of feedback will I receive regarding my work?\nYou will receive extensive and timely feedback on all of your submissions (from me and from your colleagues), and you will have opportunities to revise. I will also provide rubrics for most assignments to indicate how I will evaluate your work. However, you will not receive points or an A, B, C, D, etc. on individual assignments.\n\n\nThere appear to be a significant number of assignments and deadlines in this course. Why is this the case?\nI’ve been very deliberate in pacing your assignments for this course for a number of reasons. Developing the critical thinking skills required of ethnography demands building and consistently practicing observational habits. I want you to leave this class being more mindful of the cultural forces shaping the numbers you see everyday. You are being asked to complete several short reflective writing assignments for this course so that you can start to build those habits. I am aware that this will mean that, sometimes, the writing you produce for this course will not be the very best quality you can produce. Especially for the field note assignments, I have prioritized consistency over depth of reflection because the primary goal there is to encourage you to become more habitually observant. This is also why you have the opportunity to select assignments you want to invest more time on when aiming for an A. If you are struggling with this course’s workload, I recommend coming to talk with me in office hours.\n\n\nHow can I best keep track of my labor in this course?\nThroughout the semester, you will be asked to keep track of your labor via a log that you will share with me. I will reference this log to calculate certain aspects of your final grade. Instructions for using the labor log will be provided in the second week of the course.\n\n\nCan I earn +/- grades in this course?\n+/- will be assigned to final grades at my discretion in cases where a student’s work consistently exceeds the expectations (+) of their contracted grade or is in some way insufficient (-). Students can track their progress towards a grade modifier in feedback that I provide throughout the semester. Your grade may also be reduced by a grade modifier for each missed class beyond the number associated with your contracted grade.\n\n\nWhat if I need an extension on an assignment?\nThere is a 24-hour grace period on all written assignments, except for reading annotations. There will be no penalties for submitting the written assignment within this 24-hour period, and you do not need to inform me that you intend to take the extra time. You can also request up to a 72-hour extension on any written assignment, as long as you make that request at least 48 hours before the original assignment due date. You can request an extension by filling out the Extension Request form on Moodle, and I will confirm your extension on Slack. Beyond this, late assignments will not be accepted.\n\n\nCan I ask for extensions or use the grace period for reading annotations?\nReading assignments/Perusall annotations prepare you to participate in class discussions. For this reason, they need to be completed by the due date for credit. I’ve provided considerable leeway to miss a reading annotation assignment from time-to-time in order to accommodate flexibility in this regard.\n\n\nWhat if I don’t know how to complete an assignment at an advanced level?\nIf you are not sure how to complete assignments at an advanced level, you should plan to come talk with me in office hours. I’m more than happy to help you brainstorm ideas!\n\n\nAn assignment deadline is approaching, and I’m unhappy with the quality of my work. What should I do?\nWhile I always encourage students to strive to submit the best work they can, this course’s grading contract, coupled with the course’s revision assignments, permits you to submit work that you know you want to continue to improve upon without penalty. Not submitting assignments at all has the potentially to significantly lower your grade in this course, whereas submitting an assignment that could benefit from more revision will not be detrimental to your grade as long as the minimum submission requirements are met."
  },
  {
    "objectID": "slides/Day1-Intro.html#very-quick-syllabus-review",
    "href": "slides/Day1-Intro.html#very-quick-syllabus-review",
    "title": "Day One: Introductions",
    "section": "Very quick syllabus review",
    "text": "Very quick syllabus review\n\nAttendance and Late Arrivals\nSlack\nAcademic Honesty\nGrading Contract\n\nDeadlines\n\nCommunity Labor\n\nClass Notes\n\nWell-being\nSeating"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#interrogating-epistemic-beliefs-about-data",
    "href": "slides/Day2-Epstemologies.html#interrogating-epistemic-beliefs-about-data",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "Interrogating Epistemic Beliefs about Data",
    "text": "Interrogating Epistemic Beliefs about Data\n\nHow does a community understand the relationship between numbers and truth?\nHow does a community characterize data origins or originary forms? Where do they suggest that data comes from?\nHow does a community understand the means by which knowledge and meaning emerges from patterns in data?\nWhat analogies or metaphors does this community engage to characterize what data is or what data is like?\nHow does this community frame the role of culture, human judgment, and bias when evaluating data?"
  },
  {
    "objectID": "slides/Day2-Epstemologies.html#group-exercise",
    "href": "slides/Day2-Epstemologies.html#group-exercise",
    "title": "Day Two: Epistemologies of Big Data",
    "section": "Group Exercise",
    "text": "Group Exercise\n\nThere are large sheets of paper and markers on each table.\nIn one section of the paper, write down phrases that capture the hype/imaginaries/mythologies of Big Data/AI.\nIn one section of the paper, write down phrases that capture the actual work of machine learning.\nSelect one question from the previous slide and discuss how epistemic beliefs about Big Data/AI differ from each perspective."
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#pull-out-a-piece-of-paper-and-draw-a-line-down-the-center.-on-the-left-side-list-adjectives-that-people-use-to-describe-good-data.-on-the-right-side-write-the-opposite-of-each-word-you-wrote-on-the-left-side.",
    "href": "slides/Day3-BigDataDiscourse.html#pull-out-a-piece-of-paper-and-draw-a-line-down-the-center.-on-the-left-side-list-adjectives-that-people-use-to-describe-good-data.-on-the-right-side-write-the-opposite-of-each-word-you-wrote-on-the-left-side.",
    "title": "Day Three: Big Data Discourse",
    "section": "Pull out a piece of paper, and draw a line down the center. On the left side, list adjectives that people use to describe “good” data. On the right side, write the opposite of each word you wrote on the left side.",
    "text": "Pull out a piece of paper, and draw a line down the center. On the left side, list adjectives that people use to describe “good” data. On the right side, write the opposite of each word you wrote on the left side."
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#binary-oppositions",
    "href": "slides/Day3-BigDataDiscourse.html#binary-oppositions",
    "title": "Day Three: Big Data Discourse",
    "section": "Binary Oppositions",
    "text": "Binary Oppositions\n\nLooking at the world through pairs of terms that we consider to have the opposite meaning\nExamples include:\n\nReal/fake\nObjective/subjective\nNature/culture\n\nBinary oppositions are reductionist, or oversimplify complexity\nBinary oppositions are rooted in ideologies and disseminated through discourse"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#hierarchies-in-binary-oppositions",
    "href": "slides/Day3-BigDataDiscourse.html#hierarchies-in-binary-oppositions",
    "title": "Day Three: Big Data Discourse",
    "section": "Hierarchies in Binary Oppositions",
    "text": "Hierarchies in Binary Oppositions\n\nIn dominant discourse, one half of a binary opposition tends to be positioned as superior than the other\nOne half tends to get treated as normal or pure, and other as a deviation from the normal, or tainted\nBinary oppositions can reinforce privilege\nWhat are some examples of some hiearchical binary oppositions?"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#natureculture",
    "href": "slides/Day3-BigDataDiscourse.html#natureculture",
    "title": "Day Three: Big Data Discourse",
    "section": "Nature/Culture",
    "text": "Nature/Culture\n\nCountless domains (disciplines, newspaper headings, etc.) organized around the divisions between nature and culture\nNature is often associated with purity, innateness, biology, or rawness.\nCulture is seen as ‘Other’ to what is natural\n\ne.g. human judgments bias science and decision-making\ne.g. human cultures destroy the Earth’s purity\n\nFeminist critiques:\n\nShows how purity is political\nArgues that we can’t tell where nature stops and culture starts\nShows how the divisions justify treating certain social groups as superior and others as inferior\nRefers to natureculture: hybrids reverse the logic of binary oppositions"
  },
  {
    "objectID": "slides/Day3-BigDataDiscourse.html#reminders",
    "href": "slides/Day3-BigDataDiscourse.html#reminders",
    "title": "Day Three: Big Data Discourse",
    "section": "Reminders",
    "text": "Reminders\n\nBe sure to record your work in the labor log!\nLet me know if you’d like to lead a classroom reading discussion\nTonight’s Debate"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#discourse-analysis",
    "href": "slides/Day4-BinaryOppositions.html#discourse-analysis",
    "title": "Day Four: Discourse Analysis",
    "section": "Discourse Analysis",
    "text": "Discourse Analysis\n\nEstablish the context\nConsider the medium\nDiscern the intended audience\nAssess assumptions\nIdentify cultural cues and references\nEvaluate rhetorical strategies and methods of delivery\nConsider the social structures the discourse operates within\nAssess how the discourse disseminates\nReflect on what is not said or who is not included"
  },
  {
    "objectID": "slides/Day4-BinaryOppositions.html#reminders",
    "href": "slides/Day4-BinaryOppositions.html#reminders",
    "title": "Day Four: Discourse Analysis",
    "section": "Reminders",
    "text": "Reminders\n\nTuesday’s reading\nDM me to lead reading discussion\nFill out CATME Survey\nWe will discuss upcoming assignments (Fieldnote 1 and Team Contract) in class on Tuesday."
  },
  {
    "objectID": "slides/Day6-Reflexivity.html#project-groups",
    "href": "slides/Day6-Reflexivity.html#project-groups",
    "title": "Day Six: Reflexivity",
    "section": "Project Groups",
    "text": "Project Groups\n\nIn your groups, be sure to introduce yourselves to each other (pronouns, majors, etc.)"
  },
  {
    "objectID": "assignments.html#advanced-lead-a-class-reading-discussion",
    "href": "assignments.html#advanced-lead-a-class-reading-discussion",
    "title": "Assignments",
    "section": "Advanced: Lead a Class Reading Discussion",
    "text": "Advanced: Lead a Class Reading Discussion\nLeading a class reading discussion is one way to demonstrate advanced level work. Reading discussions should be 15 minutes maximum. Because we have other activities planned, I will need to cut off discussions if they go too long. Students leading a discussion should start by taking 2-3 minutes to summarize the main arguments of the reading, dicussing how the author supports those arguments. After this, it is up to the student(s) presenting how they wish to structure the remaining time. I’ve had students pose discussion questions to the whole class. I’ve also had students pose questions for small groups to discuss first (note that if you take this option, there is really only time to discuss at most 2 questions; trust me, the time goes by fast). I’ve also had students do more experimental things like create Kahoots or present case studies for the class to think through. It’s up to you! Just get us reflecting on the reading! Often students create slides to help structure the discussion, but this is not necessarily a requirement."
  },
  {
    "objectID": "slides/Day7-Datasets.html#we-are-about-to-make-a-transition-in-this-course.-for-the-next-3-minutes-free-write-about-your-biggest-takeaways-from-the-course-so-far.-what-have-you-been-finding-yourself-thinking-about-more-what-concepts-and-theories-are-new-to-you-what-new-questions-have-come-up-for-you",
    "href": "slides/Day7-Datasets.html#we-are-about-to-make-a-transition-in-this-course.-for-the-next-3-minutes-free-write-about-your-biggest-takeaways-from-the-course-so-far.-what-have-you-been-finding-yourself-thinking-about-more-what-concepts-and-theories-are-new-to-you-what-new-questions-have-come-up-for-you",
    "title": "Day Seven: Genealogies of Datasets",
    "section": "We are about to make a transition in this course. For the next 3 minutes, free-write about your biggest takeaways from the course so far. What have you been finding yourself thinking about more? What concepts and theories are new to you? What new questions have come up for you?",
    "text": "We are about to make a transition in this course. For the next 3 minutes, free-write about your biggest takeaways from the course so far. What have you been finding yourself thinking about more? What concepts and theories are new to you? What new questions have come up for you?"
  },
  {
    "objectID": "slides/Day7-Datasets.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day7-Datasets.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Seven: Genealogies of Datasets",
    "section": "Turn to your neighbor and discuss!",
    "text": "Turn to your neighbor and discuss!"
  },
  {
    "objectID": "slides/Day7-Datasets.html#fairness-in-machine-learning",
    "href": "slides/Day7-Datasets.html#fairness-in-machine-learning",
    "title": "Day Seven: Genealogies of Datasets",
    "section": "Fairness in Machine Learning",
    "text": "Fairness in Machine Learning"
  },
  {
    "objectID": "slides/Day7-Datasets.html#overview-of-course-format-from-here",
    "href": "slides/Day7-Datasets.html#overview-of-course-format-from-here",
    "title": "Day Seven: Genealogies of Datasets",
    "section": "Overview of Course Format from Here",
    "text": "Overview of Course Format from Here\n\n\n\n\n\n\n\n\nFocus\nMethod\nConcepts\n\n\n\n\nMeaning-Making\nInfrastructural Analysis\nInfrastructure, Master Narratives\n\n\nLabor\nInterviewing\nInvisible Labor, Boundary Work\n\n\nCollective Action\nParticipant Observation\nRituals, Sensing\n\n\nInstitutions\n\nIncentives\n\n\nDiscourse\nRevisit Discourse Analysis\nMobilization, Meaning-Making, Ignorance"
  },
  {
    "objectID": "slides/Day7-Datasets.html#final-project",
    "href": "slides/Day7-Datasets.html#final-project",
    "title": "Day Seven: Genealogies of Datasets",
    "section": "Final Project",
    "text": "Final Project\n\nAssignment Overview\nPossible Datasets"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#ws-of-metadata",
    "href": "slides/DayX-DataDocumentation.html#ws-of-metadata",
    "title": "Day Nine: Data Documentation",
    "section": "5 W’s of Metadata",
    "text": "5 W’s of Metadata"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#example-library-catalog",
    "href": "slides/DayX-DataDocumentation.html#example-library-catalog",
    "title": "Day Nine: Data Documentation",
    "section": "Example: Library Catalog",
    "text": "Example: Library Catalog"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#metadata-schemas",
    "href": "slides/DayX-DataDocumentation.html#metadata-schemas",
    "title": "Day Nine: Data Documentation",
    "section": "Metadata Schemas",
    "text": "Metadata Schemas\n\nA standardized labeling system for cataloging or describing data\nEnables search engines to index data by certain criteria\nExamples:\n\nSort by “date created”\nRetrieve all results from a specific “author/creator”\nFilter results to a specific “subject”\nExclude results from a specific “publisher”"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#example-citation-manager",
    "href": "slides/DayX-DataDocumentation.html#example-citation-manager",
    "title": "Day Nine: Data Documentation",
    "section": "Example: Citation Manager",
    "text": "Example: Citation Manager"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#whats-the-difference-between-administrative-and-descriptive-metadata",
    "href": "slides/DayX-DataDocumentation.html#whats-the-difference-between-administrative-and-descriptive-metadata",
    "title": "Day Nine: Data Documentation",
    "section": "What’s the difference between administrative and descriptive metadata?",
    "text": "What’s the difference between administrative and descriptive metadata?"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#data-dictionaries",
    "href": "slides/DayX-DataDocumentation.html#data-dictionaries",
    "title": "Day Nine: Data Documentation",
    "section": "Data Dictionaries",
    "text": "Data Dictionaries\n\nDocuments for holding descriptive metadata\nDefine the variables in a dataset and the values that may fill in those variables\nAre not always as descriptive as we’d like them to be"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#example-nyc-metadata-for-all",
    "href": "slides/DayX-DataDocumentation.html#example-nyc-metadata-for-all",
    "title": "Day Nine: Data Documentation",
    "section": "Example: NYC Metadata for All",
    "text": "Example: NYC Metadata for All"
  },
  {
    "objectID": "slides/DayX-DataDocumentation.html#discussion-of-final-project",
    "href": "slides/DayX-DataDocumentation.html#discussion-of-final-project",
    "title": "Day Nine: Data Documentation",
    "section": "Discussion of Final Project",
    "text": "Discussion of Final Project"
  },
  {
    "objectID": "slides/DayX-ResearchEthics.html#i-encourage-you-to-come-to-office-hours-if",
    "href": "slides/DayX-ResearchEthics.html#i-encourage-you-to-come-to-office-hours-if",
    "title": "Day Eight: Research Ethics",
    "section": "I encourage you to come to office hours if…",
    "text": "I encourage you to come to office hours if…\n\nYou’d like to discuss strategies for managing the reading/workload.\nYou’d like clarifications on any of the assignment expectations/course infrastructure.\nYou’d like to review certain concepts discussed in lecture or the readings.\nYou’d like to chat about opportunities for exploring more about data ethnography.\nYou’re stressed and need some positive affirmation."
  },
  {
    "objectID": "slides/DayX-Looping.html",
    "href": "slides/DayX-Looping.html",
    "title": "Day Seven: Data Looping Effects",
    "section": "",
    "text": "Office hours for help with labor log.\nGroup contract and first field note due next Tuesday."
  },
  {
    "objectID": "slides/DayX-Looping.html#announcements",
    "href": "slides/DayX-Looping.html#announcements",
    "title": "Day Seven: Data Looping Effects",
    "section": "",
    "text": "Office hours for help with labor log.\nGroup contract and first field note due next Tuesday."
  },
  {
    "objectID": "slides/DayX-Looping.html#think-about-a-category-or-label-that-institutions-assign-to-a-group-of-people.-free-write-for-about-2-minutes-on-the-following-prompt-how-does-the-existence-of-this-category-or-label-impact-certain-social-groups",
    "href": "slides/DayX-Looping.html#think-about-a-category-or-label-that-institutions-assign-to-a-group-of-people.-free-write-for-about-2-minutes-on-the-following-prompt-how-does-the-existence-of-this-category-or-label-impact-certain-social-groups",
    "title": "Day Seven: Data Looping Effects",
    "section": "Think about a category or label that institutions assign to a group of people. Free-write for about 2 minutes on the following prompt: How does the existence of this category or label impact certain social groups?",
    "text": "Think about a category or label that institutions assign to a group of people. Free-write for about 2 minutes on the following prompt: How does the existence of this category or label impact certain social groups?"
  },
  {
    "objectID": "slides/DayX-Looping.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/DayX-Looping.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Seven: Data Looping Effects",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhy does this category or label exist?\nIn what ways has this category or label been normalized in society?\nWhat happens when folks don’t agree with the assignment? Can they contest it?"
  },
  {
    "objectID": "slides/DayX-Looping.html#assemblages",
    "href": "slides/DayX-Looping.html#assemblages",
    "title": "Day Seven: Data Looping Effects",
    "section": "Assemblages",
    "text": "Assemblages\n\nContests the idea that data are hard, shiny objects that can be held, acquired, etc.\nWhen we talk about data, we are actually referring to many interrelated things:\n\nMaterialities, practices, subjectivities, places, governmentalities, etc.\nSee table in this week’s reading\n\n“Data” emerges in the wake of these interrelated assemblages"
  },
  {
    "objectID": "slides/DayX-Looping.html#hackings-looping-effect",
    "href": "slides/DayX-Looping.html#hackings-looping-effect",
    "title": "Day Seven: Data Looping Effects",
    "section": "Hacking’s Looping Effect",
    "text": "Hacking’s Looping Effect\n &gt; Tekin, Serife. 2014. “The Missing Self in Hacking’s Looping Effect.” In Classifying Psychopathology: Mental Kinds and Natural Kinds, edited by Harold Kincaid and Jacqueline A. Sullivan. MIT Press."
  },
  {
    "objectID": "slides/Day11-Infrastructure.html#women-made-up-47-of-the-u.s.-civilian-labor-force-in-2023-up-from-30-in-1950",
    "href": "slides/Day11-Infrastructure.html#women-made-up-47-of-the-u.s.-civilian-labor-force-in-2023-up-from-30-in-1950",
    "title": "Day Eight: Ethnographies of Infrastructure",
    "section": "“Women made up 47% of the U.S. civilian labor force in 2023, up from 30% in 1950…”",
    "text": "“Women made up 47% of the U.S. civilian labor force in 2023, up from 30% in 1950…”\n\n\nSchaeffer, Katherine. 2024. “For Women’s History Month, a Look at Gender Gains – and Gaps – in the U.S.” Pew Research Center (blog). February 27, 2024. https://www.pewresearch.org/short-reads/2024/02/27/for-womens-history-month-a-look-at-gender-gains-and-gaps-in-the-us/."
  },
  {
    "objectID": "slides/Day13-Interviewing.html#thursdays-reading",
    "href": "slides/Day13-Interviewing.html#thursdays-reading",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Thursday’s Reading",
    "text": "Thursday’s Reading\n\nConsidering “what is data science” in universities\nInterested in the work that people do to draw the boundaries of data science\nOutlines two visions of what kind of discipline data science is\nTransdiscipline - Boundary work by university leaders\nExtradiscipline - Boundary work by the people actually doing data science\nPay attention to how these two different visions of data science get enacted!"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#practice",
    "href": "slides/Day13-Interviewing.html#practice",
    "title": "Day Ten: Labor and Interviewing",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "slides/Day13-Interviewing.html#you-are-scheduled-to-interview-an-ai-ghost-worker.-what-is-one-question-you-might-ask-them-to-deepen-understanding-of-the-social-configurations-of-their-work",
    "href": "slides/Day13-Interviewing.html#you-are-scheduled-to-interview-an-ai-ghost-worker.-what-is-one-question-you-might-ask-them-to-deepen-understanding-of-the-social-configurations-of-their-work",
    "title": "Day Ten: Labor and Interviewing",
    "section": "You are scheduled to interview an AI “ghost worker.” What is one question you might ask them to deepen understanding of the social configurations of their work?",
    "text": "You are scheduled to interview an AI “ghost worker.” What is one question you might ask them to deepen understanding of the social configurations of their work?"
  },
  {
    "objectID": "slides/Day14-Expertise.html#draw-a-map-of-data-science---sort-of-like-you-would-draw-a-map-of-the-world.-whats-at-the-center-of-your-map-what-features-are-at-the-periphery-what-are-the-far-off-lands-outside-of-its-borders-where-are-the-mountains-rivers-forbidden-areas",
    "href": "slides/Day14-Expertise.html#draw-a-map-of-data-science---sort-of-like-you-would-draw-a-map-of-the-world.-whats-at-the-center-of-your-map-what-features-are-at-the-periphery-what-are-the-far-off-lands-outside-of-its-borders-where-are-the-mountains-rivers-forbidden-areas",
    "title": "Day Eleven: Boundary Work",
    "section": "Draw a “map of data science” - sort of like you would draw a map of the world. What’s at the center of your map? What features are at the periphery? What are the far-off lands outside of its borders? Where are the mountains? Rivers? Forbidden areas?",
    "text": "Draw a “map of data science” - sort of like you would draw a map of the world. What’s at the center of your map? What features are at the periphery? What are the far-off lands outside of its borders? Where are the mountains? Rivers? Forbidden areas?"
  },
  {
    "objectID": "slides/Day14-Expertise.html#how-do-we-decide-what-is-pseudo-science",
    "href": "slides/Day14-Expertise.html#how-do-we-decide-what-is-pseudo-science",
    "title": "Day Eleven: Boundary Work",
    "section": "How do we decide what is pseudo-science?",
    "text": "How do we decide what is pseudo-science?\nhttps://en.wikipedia.org/wiki/Talk:Ufology"
  },
  {
    "objectID": "slides/Day14-Expertise.html#positioning-data-science-on-the-map",
    "href": "slides/Day14-Expertise.html#positioning-data-science-on-the-map",
    "title": "Day Eleven: Boundary Work",
    "section": "Positioning Data Science on the Map",
    "text": "Positioning Data Science on the Map\n\nWhere we place data science in relation to other disciplines matters\nDoes data science sit separately from domains? Does it sit outside them? Above them? What does this convey about data science?"
  },
  {
    "objectID": "slides/Day14-Expertise.html#example",
    "href": "slides/Day14-Expertise.html#example",
    "title": "Day Eleven: Boundary Work",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/Day14-Expertise.html#example-1",
    "href": "slides/Day14-Expertise.html#example-1",
    "title": "Day Eleven: Boundary Work",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/Day14-Expertise.html#example-2",
    "href": "slides/Day14-Expertise.html#example-2",
    "title": "Day Eleven: Boundary Work",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#think-about-a-measurement-you-took-in-the-past-two-weeks.-how-did-you-engage-your-body-when-taking-that-measurement",
    "href": "slides/Day15-ParticipantObservation.html#think-about-a-measurement-you-took-in-the-past-two-weeks.-how-did-you-engage-your-body-when-taking-that-measurement",
    "title": "Day Twelve: Participant Observation",
    "section": "Think about a measurement you took in the past two weeks. How did you engage your body when taking that measurement?",
    "text": "Think about a measurement you took in the past two weeks. How did you engage your body when taking that measurement?"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#cartesian-dualism",
    "href": "slides/Day15-ParticipantObservation.html#cartesian-dualism",
    "title": "Day Twelve: Participant Observation",
    "section": "Cartesian Dualism",
    "text": "Cartesian Dualism\n\nWestern philosophy conceptualizes body as essentially biological\n\nOften perceived as closer to nature (further from culture)\n\nRene Descartes: conceptualized mind/body distinct and separate\n\nMind: non-physical or non-material, associated with consciousness, thought, and self-awareness\nSeparate from brain: material human sensory organ"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#feminist-critiques-of-cartesian-dualism",
    "href": "slides/Day15-ParticipantObservation.html#feminist-critiques-of-cartesian-dualism",
    "title": "Day Twelve: Participant Observation",
    "section": "Feminist Critiques of Cartesian Dualism",
    "text": "Feminist Critiques of Cartesian Dualism\n\nMind and mental capacity often gets associated with masculinity\nPhysical and natural body often gets associated with femininity\nFemale bodies often perceived as closer to Nature\n\n“Mother Earth” vs. Man\n“virgin water” or “virgin forests”\nNature itself is often feminized"
  },
  {
    "objectID": "slides/Day15-ParticipantObservation.html#embodiment",
    "href": "slides/Day15-ParticipantObservation.html#embodiment",
    "title": "Day Twelve: Participant Observation",
    "section": "Embodiment",
    "text": "Embodiment\n\nAround 1980s, feminist scholarship in anthropology, sociology, etc shifted\n\nNot just studies of the body,\nStudies of how experiences living in bodies shape how we move through the world and what we perceive\nAlso study the symbolic meaning we inscribe on bodies\n\nBody not just biological but experiences are embodied\nCritiqued essentialism about bodies - the idea that a set of characteristics define the “essence” of a body\n\nNo singular characteristics to separate a “normal” body from an abnormal body\nNo singular characteristics to separate a body from machines"
  },
  {
    "objectID": "slides/Day16-Rituals.html#what-are-rituals",
    "href": "slides/Day16-Rituals.html#what-are-rituals",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "What are rituals?",
    "text": "What are rituals?\n\nStylized repetitive activities engaged in different cultural contexts\nMay involve words, gestures, movements, exchanges\nCan sometimes be taken-for-granted and other times front and center\nMark the shared beliefs of a social group and membership in a community"
  },
  {
    "objectID": "slides/Day16-Rituals.html#turn-to-your-neighbor-and-discuss",
    "href": "slides/Day16-Rituals.html#turn-to-your-neighbor-and-discuss",
    "title": "Day Sixteen: Rituals of Data Collection",
    "section": "Turn to your neighbor and discuss:",
    "text": "Turn to your neighbor and discuss:\n\nWhy do people try to do this?\nWhat meaning does it have for the data?\nHow does it shape the environments/social worlds for the people that engage in this “ritual”?"
  },
  {
    "objectID": "slides/Day17-Incentives.html#take-some-time-to-free-write-on-where-your-heart-is-at-and-what-energy-you-are-bringing-into-class-today.",
    "href": "slides/Day17-Incentives.html#take-some-time-to-free-write-on-where-your-heart-is-at-and-what-energy-you-are-bringing-into-class-today.",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Take some time to free write on where your heart is at, and what energy you are bringing into class today.",
    "text": "Take some time to free write on where your heart is at, and what energy you are bringing into class today."
  },
  {
    "objectID": "slides/Day17-Incentives.html#audit-culture",
    "href": "slides/Day17-Incentives.html#audit-culture",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Audit Culture",
    "text": "Audit Culture\n\nCultural shift often understood to emerge/expand around the 1990s\nReliance on metrics to measure and rank performance\nReliance on metrics to monitor and oversee individuals/institutions\nRedefines what accountability and governance mean"
  },
  {
    "objectID": "slides/Day17-Incentives.html#example-nyc-compstat",
    "href": "slides/Day17-Incentives.html#example-nyc-compstat",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Example: NYC CompStat",
    "text": "Example: NYC CompStat\n\nCrime reduction strategy instituted in NYC in the 1990s\nUsed crime and deployment data as performance metrics\nOfficers penalized for failing to reduce crimes"
  },
  {
    "objectID": "slides/Day17-Incentives.html#example-toxic-release-inventory",
    "href": "slides/Day17-Incentives.html#example-toxic-release-inventory",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Example: Toxic Release Inventory",
    "text": "Example: Toxic Release Inventory\n\nUS industrial facilities required to report amounts of toxic emissions they release into environment or transfer off-site\nUsed to environmental burdens and injustices\nUsed in journalism, activism, and legal proceedings"
  },
  {
    "objectID": "slides/Day17-Incentives.html#metric-fixation",
    "href": "slides/Day17-Incentives.html#metric-fixation",
    "title": "Day Seventeen: Institutional Incentives",
    "section": "Metric Fixation",
    "text": "Metric Fixation\n\nTendencies to focus more on improving numbers than on actual performance/change\nCan incentivize individuals to engage in practices contrary to their goals\n\n“Teaching to the test”\nSurgeons not taking risky cases\n\nCan incentivize individuals to engage in deceptive reporting\n\nJuking the Stats\nCooking the Books\nPhantom Emissions"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#final-project-reminders",
    "href": "slides/Day20-MobilizationOtherwise.html#final-project-reminders",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Final Project Reminders",
    "text": "Final Project Reminders\n\nFirst Draft due on Tuesday\nUser guide demonstrates having completed substantial research across all five project exercises\nLanguage, visuals, and formats are tailored towards a particular audience\nNo minimum word count, but example user guide is about 4000 words long"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#final-project-scoring",
    "href": "slides/Day20-MobilizationOtherwise.html#final-project-scoring",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Final Project Scoring",
    "text": "Final Project Scoring\n\nMeets Criteria: Clearly communicating the contexts that produced data in a format appropriate for a given audience\nExceeds Criteria: Creatively playing with the style and medium of communication in order to better reach and engage an audience"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#free-writing-consider-a-numeric-metric-you-use-to-measure-something-about-yourself.-pick-something-that-has-special-meaning-for-you.-is-there-a-number-that-you-strive-for-how-do-you-know-when-your-score-is-good",
    "href": "slides/Day20-MobilizationOtherwise.html#free-writing-consider-a-numeric-metric-you-use-to-measure-something-about-yourself.-pick-something-that-has-special-meaning-for-you.-is-there-a-number-that-you-strive-for-how-do-you-know-when-your-score-is-good",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Free-writing: Consider a numeric metric you use to measure something about yourself. Pick something that has special meaning for you. Is there a number that you strive for? How do you know when your score is “good”?",
    "text": "Free-writing: Consider a numeric metric you use to measure something about yourself. Pick something that has special meaning for you. Is there a number that you strive for? How do you know when your score is “good”?"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#principles-for-active-listening",
    "href": "slides/Day20-MobilizationOtherwise.html#principles-for-active-listening",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Principles for Active Listening",
    "text": "Principles for Active Listening\n\nSuspend judgment\nBe curious\nHalt internal commentary\nListen with intention\nExpress gratitude for sharing"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#structured-story-tellling",
    "href": "slides/Day20-MobilizationOtherwise.html#structured-story-tellling",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Structured Story-tellling",
    "text": "Structured Story-tellling\n\nFind a partner, and decide who will speak first and who will speak second.\nTwo minutes:\n\n\nFirst speaker: Provides uninterrupted narration about this measure. What does it mean to you? How did it come to have its meaning? How do you decide what success looks like?\nSecond speaker: Listens with intention: What does this number mean to this person?\n\n\nOne minute:\n\n\nSecond speaker: Reflect back on what you heard/ask questions, etc. (e.g. “From what I understand…”)\nFirst speaker: Affirm, correct, clarify, but also listen for what they heard\n\n\nSwap and repeat.\n\n\n\nThrough what mechanisms did people decide what was good?\nDid you identify social structures at play in making this number meaningful?\nDid you hear your partner talk about ways this number might mean different things to different people?"
  },
  {
    "objectID": "slides/Day20-MobilizationOtherwise.html#mobilizing-data-narratives-1",
    "href": "slides/Day20-MobilizationOtherwise.html#mobilizing-data-narratives-1",
    "title": "Day Twenty: Mobilizing Meaning Otherwise",
    "section": "Mobilizing Data Narratives",
    "text": "Mobilizing Data Narratives\n\nMobilization refers to the processes by which people prepare something to be put to use or into action\nStakeholders strategically engage in meaning-making activities around data\nShapes societal interpretations of data"
  },
  {
    "objectID": "slides/Day22-Credibility.html#mobilizing-data-narratives",
    "href": "slides/Day22-Credibility.html#mobilizing-data-narratives",
    "title": "Day Twenty-One: Data Activism",
    "section": "Mobilizing Data Narratives",
    "text": "Mobilizing Data Narratives\n\nMobilization refers to the processes by which people prepare something to be put to use or into action\nStakeholders strategically engage in meaning-making activities around data\nShapes societal interpretations of data"
  },
  {
    "objectID": "slides/Day22-Credibility.html#narrating-data",
    "href": "slides/Day22-Credibility.html#narrating-data",
    "title": "Day Twenty-One: Data Activism",
    "section": "Narrating Data",
    "text": "Narrating Data\n\nCurating plot details and trajectories\n\nChoosing variables to report on\nChoosing how to sequence statistics\n\nEngaging literary or rhetorical devices\n\nTechniques of communication designed to evoke a reaction from the listener\n\nIdentifying the sense-making tools\n\nVisuals\nStatistical terminology (e.g. “statistically significant”)"
  },
  {
    "objectID": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life",
    "href": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life",
    "title": "Day Twenty-One: Data Activism",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-1",
    "href": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-1",
    "title": "Day Twenty-One: Data Activism",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-2",
    "href": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-2",
    "title": "Day Twenty-One: Data Activism",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?"
  },
  {
    "objectID": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-3",
    "href": "slides/Day22-Credibility.html#what-story-telling-elements-bring-the-data-in-this-ad-to-life-3",
    "title": "Day Twenty-One: Data Activism",
    "section": "What story-telling elements bring the data in this ad to life?",
    "text": "What story-telling elements bring the data in this ad to life?\n\n\n\n\nAd banned in 2023\nEmphasized the renewables at the expense of high-carbon initiatives (which make up bulk of Shell production)\n“We strongly disagree with the ASA’s decision, which could slow the UK’s drive towards renewable energy,” said a spokesperson for Shell. “People are already well aware that Shell produces the oil and gas they depend on today. When customers fill up at our petrol stations across the UK, it’s under the instantly recognisable Shell logo.”\n“However, they [are] unlikely to be aware of the details of this in relation to specific companies,” the ASA said. “Ads [are] therefore likely to mislead consumers if they [have] misrepresented the contribution that lower-carbon initiatives played, or would play in the near future, as part of the overall balance of a company’s activities.”"
  },
  {
    "objectID": "index.html#rstudiorstudio-server",
    "href": "index.html#rstudiorstudio-server",
    "title": "SDS 192: Introduction to Data Science",
    "section": "",
    "text": "This class will use the R statistical software package. If you haven’t already, you will install and configure R and RStudio in SDS 100. You should let me know in the first week of the course if you are using a Chromebook or tablet."
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "This lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation.\n\n\n\nRead a data dictionary\nReference data documentation\nIdentify unique observations in a dataset\nUnderstand different variable types\nLook up value codes and recode a variable\nDetermine the number of missing values in a variable and why they are missing"
  },
  {
    "objectID": "labs/lab1.html#introduction",
    "href": "labs/lab1.html#introduction",
    "title": "Lab 1: Understanding Datasets",
    "section": "",
    "text": "This lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation.\n\n\n\nRead a data dictionary\nReference data documentation\nIdentify unique observations in a dataset\nUnderstand different variable types\nLook up value codes and recode a variable\nDetermine the number of missing values in a variable and why they are missing"
  },
  {
    "objectID": "labs/lab1.html#review-of-key-terms",
    "href": "labs/lab1.html#review-of-key-terms",
    "title": "Lab 1: Understanding Datasets",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRectangular Datasets\n\ndatasets in which all rows are the same length, and all columns are the same length\n\nObservations\n\nrows in a dataset; represent discrete entities we observe in the world\n\nVariables\n\ncolumns in a dataset; describe something about an observation\n\nVector\n\none-dimensional set of values that are all of the same type\n\nData Frame\n\na list of vectors of equal lengths; typically organizes data into a two-dimensional table composed of columns (the vectors) and rows\n\nUnique Key\n\nvariable (column) in the dataset that can be used to uniquely identify each row\n\nNominal categorical variables\n\nvariables that identify something else; sometimes, numbers are considered nominal categorical variables (e.g. zip code)\n\nOrdinal categorical variables\n\ncategorical variables that can be ranked or placed in a particular order (e.g. High, Medium, Low)\n\nDiscrete numeric variables\n\nnumeric variables that represent something that is countable (e.g. the number of students in a classroom, the number pages in a book)\n\nContinuous numeric variables are variables\n\nvariables in which it is always possible to measure the value more precisely (e.g. time can be measured with infinite amount of specificity - hours &gt; minutes &gt; seconds &gt; milliseconds &gt; microseconds &gt; nanoseconds …)"
  },
  {
    "objectID": "labs/lab1.html#scorecard-dataset",
    "href": "labs/lab1.html#scorecard-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Scorecard Dataset",
    "text": "Scorecard Dataset\nIn his 2013 State of the Union Address, President Barack Obama announced his plans to create a “college scorecard” that would allow prospective students and parents to compare schools in terms of cost, offerings, diversity, completion rates, and post-graduate earnings. This data was first published in 2015 and since has undergone several improvements and revisions.\nThe College Scorecard dataset is massive. In fact, I thought long and hard about whether this was really the first dataset I wanted to introduce to you in a lab. It includes information about over 6500 institutions in the U.S., and has more than 3000 columns documenting information about those institutions. I chose this dataset for this lab because, if you can learn to read this data dictionary, you will be leaps and bounds ahead of the game in learning to read other data dictionaries. (It’s also just a super cool dataset, and hint, hint: you will get a chance to dive into it in much more detail in a few weeks). While the full data is available online, we are only going to work with a small subset of the data today."
  },
  {
    "objectID": "labs/lab1.html#setting-up-your-environment",
    "href": "labs/lab1.html#setting-up-your-environment",
    "title": "Lab 1: Understanding Datasets",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nOpen the sds-192-labs project in RStudio. Double check the upper right hand corner of RStudio to ensure that you are in the correct project!!\nDownload the template file for this lab from GitHub and move it to the computer folder associated with the sds-192-labs project. Open the template file in RStudio.\nInstall the RScorecard package by entering the following into your Console: install.packages(\"rscorecard\")\nCreate a Scorecard API Key at this link. Shortly after you fill out the form, you will be emailed a key. Copy that key into code chunk below, replacing all of the following text in sc_key(): Sys.getenv(“SCORECARD_KEY”). Be sure to wrap the key in quotation marks.\nDownload the Scorecard Data Dictionary and Technical Documentation for Institution-Level Data Files here.\nRun the code below to the import 2022 Scorecard data for Massachusetts into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(rscorecard)\nsc_key(Sys.getenv(\"SCORECARD_KEY\")) # Replace Sys.getenv(\"SCORECARD_KEY\") here with your API Key in quotation marks\n\nscorecard &lt;- sc_init() |&gt;\n  sc_year(2022) |&gt;                 #Note how we are looking at only 2022 data here!\n  sc_filter(stabbr == \"MA\") |&gt;     #Note how we are looking at only Massachusetts data here!\n  sc_select(unitid, instnm, city, highdeg, control, ugds, adm_rate, costt4_a, costt4_p, pcip27, pctfloan, admcon7, wdraw_orig_yr2_rt, cdr3) |&gt;\n  sc_get()"
  },
  {
    "objectID": "labs/lab1.html#glimpsing-the-data",
    "href": "labs/lab1.html#glimpsing-the-data",
    "title": "Lab 1: Understanding Datasets",
    "section": "Glimpsing the Data",
    "text": "Glimpsing the Data\nWhen working with very large datasets, we need tools to help us get a sense of the dataset without having to load the entire data frame. For instance, we can view the first 6 rows of the dataset by calling head().\n\nhead(scorecard)\n\n# A tibble: 6 × 15\n  unitid instnm    city  highdeg control  ugds adm_rate costt4_a costt4_p pcip27\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 164368 Hult Int… Camb…       4       2   682   0.477     74000       NA  0    \n2 164447 American… Spri…       4       2  1142   0.894     53236       NA  0    \n3 164465 Amherst … Amhe…       3       2  1898   0.0726    80060       NA  0.095\n4 164492 Anna Mar… Paxt…       4       2   991   0.947     55594       NA  0    \n5 164535 Assabet … Marl…       1       1    54   0.474        NA    42983  0    \n6 164562 Assumpti… Worc…       4       2  1676   0.823     60262       NA  0.015\n# ℹ 5 more variables: pctfloan &lt;dbl&gt;, admcon7 &lt;int&gt;, wdraw_orig_yr2_rt &lt;lgl&gt;,\n#   cdr3 &lt;dbl&gt;, year &lt;dbl&gt;\n\n\nstr() provides a great deal of information about the observations in the data frame, including the number of variables, the number of observations, the column names, their data types, and a list of observations.\n\nstr(scorecard)\n\ntibble [147 × 15] (S3: tbl_df/tbl/data.frame)\n $ unitid           : int [1:147] 164368 164447 164465 164492 164535 164562 164580 164599 164614 164632 ...\n $ instnm           : chr [1:147] \"Hult International Business School\" \"American International College\" \"Amherst College\" \"Anna Maria College\" ...\n $ city             : chr [1:147] \"Cambridge\" \"Springfield\" \"Amherst\" \"Paxton\" ...\n $ highdeg          : int [1:147] 4 4 3 4 1 4 4 1 3 4 ...\n $ control          : int [1:147] 2 2 2 2 1 2 2 3 2 2 ...\n $ ugds             : int [1:147] 682 1142 1898 991 54 1676 2709 26 39 1263 ...\n $ adm_rate         : num [1:147] 0.4774 0.8936 0.0726 0.9466 0.4737 ...\n $ costt4_a         : int [1:147] 74000 53236 80060 55594 NA 60262 76360 NA 26691 46775 ...\n $ costt4_p         : int [1:147] NA NA NA NA 42983 NA NA NA NA NA ...\n $ pcip27           : num [1:147] 0 0 0.095 0 0 0.015 0 0 0 0 ...\n $ pctfloan         : num [1:147] 0.0956 0.8012 0.1123 0.6997 0.54 ...\n $ admcon7          : int [1:147] 3 5 5 3 3 5 5 NA 5 5 ...\n $ wdraw_orig_yr2_rt: logi [1:147] NA NA NA NA NA NA ...\n $ cdr3             : num [1:147] 0 0 0 0 0 0 0 0.029 0.017 0 ...\n $ year             : num [1:147] 2022 2022 2022 2022 2022 ...\n\n\nYou can also click on the name of your data frame in your Environment panel in RStudio, and it will open a new tab in RStudio that displays the data in a tabular format. Try clicking on scorecard in your Environment panel.\n\n\n\n\n\n\nTip\n\n\n\nThis is the same as calling View(scorecard) in your Console."
  },
  {
    "objectID": "labs/lab1.html#getting-to-know-this-dataset",
    "href": "labs/lab1.html#getting-to-know-this-dataset",
    "title": "Lab 1: Understanding Datasets",
    "section": "Getting to Know this Dataset",
    "text": "Getting to Know this Dataset\n\nObservations (Rows)\nIn starting our data analysis, we need to have a good sense of what each observation in our dataset refers to - or its observational unit. Think of it this way. If you were to count the number rows in your dataset, what would that number refer to? A unique key is a variable (or set of variables) that uniquely identifies an observation in the dataset. Think of a unique key as a unique way to identify a row and all of the values in it. There should never be more than one row in the dataset with the same unique key. A unique key tells us what each row in the dataset refers to.\n\nQuestion\n\nSee if you can identify a unique key for this dataset. Write some lines of code to determine whether the column you’ve identified can act as unique key for the data. Hint: You need to check whether the values in the column ever repeat.\n\n\n# Write code to calculate number of rows in scorecard\n\n# Write code to calculate the number of unique values in the column you've identified as a unique key \n\n# Do these numbers match?\n\n\n\n\n\n\n\n\n\nWhy not use name as a unique key?\n\n\n\nNote that NAME is typically not an appropriate variable to use as a unique key. Let me provide an example to demonstrate this. When I worked for BetaNYC, I was trying to build a map of vacant storefronts in NYC by mapping all commercially zoned properties in the city, and then filtering out those properties where a business was licensed or permitted. This way the map would only include properties where there wasn’t a business operating. One set of businesses I was filtering out was restaurants. The only dataset that the city had made publicly available for restaurant permits was broken. It was operating on an automated process to update whenever there was a change in the permit; however, whenever a permit was updated, rather than updating the appropriate fields in the existing dataset, it was creating a new row in the dataset that only included the permit holder (the restaurant name), the permit type, and the updated fields. Notably the unique permit ID was not being included in this new row. We pointed this issue out to city officials, but fixing something like this can be slow and time-consuming, so in the meantime, we looked into whether we could clean the data ourselves by aggregating the rows that referred to the same restaurant. However, without the permit ID it was impossible to uniquely identify the restaurants in the dataset. Sure, we had the restaurant name, but do you know how many Wendy’s there are in NYC?\n\n\nAnytime we count something in the world, we are not only engaging in a process of tabulation; we are also engaging in a process of defining. If I count the number of students in a class, I first have to define what counts as a student. If someone is auditing the class, do they count? If I, as the instructor, am learning from my students, do I count myself as a student? As I make decisions about how I’m going to define “student,” those decisions impact the numbers that I produce. When I change my definition of “student,” how I go about tabulating students also changes. Thus, as we prepare to count observations in a dataset, it is important to know how those observations are defined.\n\n\n\nQuestion\n\nAt this point, you’ve probably figured out that each row in this dataset is a higher education institution. …but there are many different ways that we can define higher education institutions, and that will impact what gets included and excluded in our data. Referencing the Technical Documentation, locate a definition for the unit of observation in this dataset. What institutions are included, and what institutions are excluded? Summarize a definition below.\n\n\n\n\nVariables (Columns)\nNote the column names for this dataframe, and the kinds of values that appear in those columns. Some of them (like city and year) might make sense to you immediately. Others (like pcip27 and highdeg) might be much more confusing. To figure out what we are looking at, we are going to need to refer to the dataset’s data dictionary.\nOpen the data dictionary you downloaded in an earlier step. It will open as an Excel file. Click on the tab labeled “Institution_Data_Dictionary”. There are thousands of variables in this dataset, falling into the broader categories of school, completion, admissions, cost, etc. Note how the file is organized, and specifically draw your attention to:\n\nColumn 1 (NAME OF DATA ELEMENT): This is a long description of the variable and gives you clues as to what is represented in it.\nColumn 6 (VARIABLE NAME): This is the column name for the variable. This is how you will reference the variable in R.\nColumn 7 (VALUE): These are the possible values for that variable. Note that for many categorical variables, the values are numbers. We are going to have to associate the numbers with their corresponding labels.\nColumn 8 (LABEL): These are the labels associated with the values recorded for the variable.\nColumn 11 (NOTES): This provides notes about the variable, including whether it is currently in use and what missing values indicate.\n\n\nQuestion\n\nFor each of the variable names in the scorecard data frame, look up the associated name in the data dictionary. You will need to search for the variable name in the sixth column of the data dictionary (I recommend using Ctrl-F/Cmd-F to quickly locate the variable name in the spreadsheet.) Once you’ve found the variable name, reference column 1 to determine what this variable means, and reference columns 7 and 8 to see what possible values will appear in that column.\nIdentify one nominal variable, one ordinal variable, one discrete variable, and one continuous variable in the scorecard data frame and list their variable names below. Then uncomment the lines below and use the typeof() function to see how R determined their data types. Did any surprise you?\n\n\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n#typeof(scorecard$_____)\n\n\n\n\nValues (Cells)\nYou may have noticed that several categorical variables are coded as numbers in the imported dataset. For instance, look at the column control which designates the institution’s ownership. Running the code below, we see that the distinct values in that column are 1, 2, and 3.\n\nunique(scorecard$control)\n\n[1] 2 1 3\n\n\nWhen we reference that column in the data dictionary (row 27), we see that a 1 in that column designates that the institution is Public, a 2 that the institution is Private nonprofit, and a 3 that the institution is Private for-profit. While I can always look that up, sometimes it is helpful to have that information in our dataset. For instance, let’s say I create a bar plot that’s supposed to show how many higher education institutions have each type of ownership in MA (which you will learn how to do soon!). The plot can be confusing when control is a series of numbers.\n\nggplot(scorecard, aes(x = control)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWith this in mind, sometimes it can be helpful to recode the values in a column. Recoding data involves replacing the values in a vector according to criteria that we provide. Remember how all columns in a data frame are technically vectors? We can use the recode() function to recode all of the values in the control vector. We are going to store the recoded values in a new column in our dataset called control_text. Check out the code below to see how we do this. Reference the help pages for recode (i.e. ?recode) to help you interpret the code.\n\nscorecard$control_text &lt;-\n  recode(\n    scorecard$control, \n    \"1\" = \"Public\", \n    \"2\" = \"Private nonprofit\", \n    \"3\" = \"Private for-profit\",\n    .default = NA_character_\n  )\n\nCheck out our barplot now!\n\nggplot(scorecard, aes(x = control_text)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nQuestion\n\nWrite code below to recode the admcon7 variable and store the results in a new variable in scorecard called admcon7_text. You’ll need to look up the values in the data dictionary. If you’ve done this correctly, running this code should produce a barplot that displays multiple bars.\n\n\nscorecard$admcon7_text &lt;-\n  recode(\n    scorecard$admcon7, \n    #Fill replacements here\n    .default = NA_character_\n  )\n\nggplot(scorecard, aes(x = admcon7_text)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\nMissing Values\nWhen we have missing values in a rectangular dataset, we have to provide a placeholder for the missing value in order for the dataset to remain rectangular. If we just skipped the value, then our dataset wouldn’t necessarily have rows of all equal lengths and columns of all equal lengths. In R, NA serves as that placeholder. Before we start analyzing data, it can be important to note how many NA values we have in a column so that we can determine if the data is representative.\nThe function is.na() checks whether a value is an NA value and returns TRUE if it is and FALSE if it isn’t. Providing a vector to is.na() will check this for every value in the vector and return a logical vector indicating TRUE/FALSE for every original value in the vector.\n\nis.na(scorecard$wdraw_orig_yr2_rt)\n\n  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [76] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [91] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[106] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[121] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[136] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nWhen we sum() across a logical vector, R will calculate the number of TRUE values in the vector.\n\nsum(is.na(scorecard$wdraw_orig_yr2_rt))\n\n[1] 147\n\n# Note that this is the same as:\n\nscorecard$wdraw_orig_yr2_rt |&gt; is.na() |&gt; sum()\n\n[1] 147\n\n\n\nQuestion\n\nIn a code chunk below, calculate the number of missing values in both the costt4_a and the costt4_p columns. Reference the NOTES column in the data dictionary to determine why there are so many NA values in these columns. Add a comment to the code chunk below, explaining the reason for missing values in these columns.\n\n\n# Write code here\n\n# Add comment here\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nReferencing the College Scorecard data documentation, see if you can determine which students are included in calculations of earnings and debt. How might the data’s coverage bias the values that get reported? What might be the social consequences of these biases? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab1.html#submission",
    "href": "labs/lab1.html#submission",
    "title": "Lab 1: Understanding Datasets",
    "section": "Submission",
    "text": "Submission\n\nWhen you are done, save your .qmd file in RStudio.\nStage, commit, and push your file in the Git pane. Refer to step 12 in our Course Infrastructure setup for a reminder of how to do this.\nNavigate back to GitHub.com and click on the .qmd files to make sure you see your changes to the files there. If you don’t see the changes there, I won’t see them either!"
  },
  {
    "objectID": "schedule.html#september-05-2025",
    "href": "schedule.html#september-05-2025",
    "title": "Schedule",
    "section": "September 05, 2025",
    "text": "September 05, 2025\n\nWhat is Data Science?\n\nDue TodayFurther Resources\n\n\n\n\n\n Spinelli Center\n Getting Started with Slack"
  },
  {
    "objectID": "schedule.html#september-08-2025",
    "href": "schedule.html#september-08-2025",
    "title": "Schedule",
    "section": "September 08, 2025",
    "text": "September 08, 2025\n\nData Fundamentals\n\nDue TodayFurther Resources\n\n\n Fill out First Day of Class Questionnaire in Moodle\n Contact me if you will be using a Chromebook\n Create a GitHub account if you don’t have one\n Accept SDS 192 Labs Assignment"
  },
  {
    "objectID": "schedule.html#september-10-2025",
    "href": "schedule.html#september-10-2025",
    "title": "Schedule",
    "section": "September 10, 2025",
    "text": "September 10, 2025\n\nIntroduction to R\n\nDue TodayFurther Resources\n\n\n Getting Started , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n Complete Syllabus Quiz in Perusall"
  },
  {
    "objectID": "schedule.html#september-12-2025",
    "href": "schedule.html#september-12-2025",
    "title": "Schedule",
    "section": "September 12, 2025",
    "text": "September 12, 2025\n\nLab: Understanding Datasets\n\nDue TodayFurther Resources\n\n\n Complete Course Infrastructure Set-Up\n\n\n Lab 1 Template\n Lab 1 Instructions"
  },
  {
    "objectID": "schedule.html#september-15-2025",
    "href": "schedule.html#september-15-2025",
    "title": "Schedule",
    "section": "September 15, 2025",
    "text": "September 15, 2025\n\nGrammar of Graphics\n\nDue TodayFurther Resources\n\n\n 2. Data Visualization , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n\n\n [ggplot2 cheatsheet](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf"
  },
  {
    "objectID": "schedule.html#september-17-2025",
    "href": "schedule.html#september-17-2025",
    "title": "Schedule",
    "section": "September 17, 2025",
    "text": "September 17, 2025\n\nVisualization Conventions\n\nDue TodayFurther Resources\n\n\n Lab 1 Due\n\n\n [ggplot2 cheatsheet](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf"
  },
  {
    "objectID": "schedule.html#september-19-2025",
    "href": "schedule.html#september-19-2025",
    "title": "Schedule",
    "section": "September 19, 2025",
    "text": "September 19, 2025\n\nLab: Designing Effective Data Visualizations\n\nDue TodayFurther Resources\n\n\n\n\n\n [ggplot2 cheatsheet](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf\n Lab 2 Instructions"
  },
  {
    "objectID": "schedule.html#september-22-2025",
    "href": "schedule.html#september-22-2025",
    "title": "Schedule",
    "section": "September 22, 2025",
    "text": "September 22, 2025\n\nFrequency Plots\n\nDue TodayFurther Resources\n\n\n 2. Data Visualization , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n Project 1 Assigned"
  },
  {
    "objectID": "schedule.html#september-24-2025",
    "href": "schedule.html#september-24-2025",
    "title": "Schedule",
    "section": "September 24, 2025",
    "text": "September 24, 2025\n\nBoxplots\n\nDue TodayFurther Resources\n\n\n Lab 2 Due\n\n\n Lab 2 Templates\n [ggplot2 cheatsheet](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf"
  },
  {
    "objectID": "schedule.html#september-26-2025",
    "href": "schedule.html#september-26-2025",
    "title": "Schedule",
    "section": "September 26, 2025",
    "text": "September 26, 2025\n\nLab: Visualizing Data\n\nDue TodayFurther Resources\n\n\n Group Contract Due\n\n\n [ggplot2 cheatsheet](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf"
  },
  {
    "objectID": "schedule.html#september-29-2025",
    "href": "schedule.html#september-29-2025",
    "title": "Schedule",
    "section": "September 29, 2025",
    "text": "September 29, 2025\n\nIntroduction to Open Source\n\nDue TodayFurther Resources\n\n\n Bryan, Jennifer (2018). “Excuse Me, Do You Have a Moment to Talk About Version Control?” In: The American Statistician 72.1. Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/00031305.2017.1399928, pp. 20-27. (Visited on Jan. 14, 2022). Read in Perusall\n\n\n [ggplot2 cheatsheet](https://www.maths.usyd.edu.au/u/UG/SM/STAT3022/r/current/Misc/data-visualization-2.1.pdf\n [Lab 3 Instructions]"
  },
  {
    "objectID": "schedule.html#october-01-2025",
    "href": "schedule.html#october-01-2025",
    "title": "Schedule",
    "section": "October 01, 2025",
    "text": "October 01, 2025\n\nIntroduction to git and GitHub\n\nDue TodayFurther Resources\n\n\n [Lab 3] Due\n\n\n Lab 3 Template"
  },
  {
    "objectID": "schedule.html#october-03-2025",
    "href": "schedule.html#october-03-2025",
    "title": "Schedule",
    "section": "October 03, 2025",
    "text": "October 03, 2025\n\nLab: Collaborating via GitHub\n\nDue TodayFurther Resources\n\n\n\n\n\n Lab 4 Instructions"
  },
  {
    "objectID": "schedule.html#october-06-2025",
    "href": "schedule.html#october-06-2025",
    "title": "Schedule",
    "section": "October 06, 2025",
    "text": "October 06, 2025\n\nSubsetting Data\n\nDue TodayFurther Resources\n\n\n 3. Data Wrangling , Ismay, Chester and Albert Y. Kim (2021). Modern Dive: Statistical Inference via Data Science. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n\n\n dplyr Cheatsheet"
  },
  {
    "objectID": "schedule.html#october-08-2025",
    "href": "schedule.html#october-08-2025",
    "title": "Schedule",
    "section": "October 08, 2025",
    "text": "October 08, 2025\n\nAggregating and Summarizing Data\n\nDue TodayFurther Resources\n\n\n Lab 4 Due\n\n\n dplyr Cheatsheet"
  },
  {
    "objectID": "schedule.html#october-10-2025",
    "href": "schedule.html#october-10-2025",
    "title": "Schedule",
    "section": "October 10, 2025",
    "text": "October 10, 2025\n\nLab: Data Wrangling\n\nDue TodayFurther Resources\n\n\n Project 1 Due\n\n\n dplyr Cheatsheet\n Lab 5 Instructions"
  },
  {
    "objectID": "schedule.html#october-13-2025",
    "href": "schedule.html#october-13-2025",
    "title": "Schedule",
    "section": "October 13, 2025",
    "text": "October 13, 2025\n\nNo Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#october-15-2025",
    "href": "schedule.html#october-15-2025",
    "title": "Schedule",
    "section": "October 15, 2025",
    "text": "October 15, 2025\n\nReview\n\nDue TodayFurther Resources\n\n\n Lab 5 Due\n Self-Scheduled Mid-Term Exams\n\n\n Lab 5 Template"
  },
  {
    "objectID": "schedule.html#october-17-2025",
    "href": "schedule.html#october-17-2025",
    "title": "Schedule",
    "section": "October 17, 2025",
    "text": "October 17, 2025\n\nIntroduction to FEC Data\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#october-20-2025",
    "href": "schedule.html#october-20-2025",
    "title": "Schedule",
    "section": "October 20, 2025",
    "text": "October 20, 2025\n\nJoining Datasets\n\nDue TodayFurther Resources\n\n\n 5. Data wrangling on multiple tables , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n Project 2 Assigned\n Watch Project Intro Video in Perusall"
  },
  {
    "objectID": "schedule.html#october-22-2025",
    "href": "schedule.html#october-22-2025",
    "title": "Schedule",
    "section": "October 22, 2025",
    "text": "October 22, 2025\n\nEthics of Data Joining\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#october-24-2025",
    "href": "schedule.html#october-24-2025",
    "title": "Schedule",
    "section": "October 24, 2025",
    "text": "October 24, 2025\n\nLab: Joining Datasets\n\nDue TodayFurther Resources\n\n\n Group Contract Due\n\n\n Lab 6 Instructions"
  },
  {
    "objectID": "schedule.html#october-27-2025",
    "href": "schedule.html#october-27-2025",
    "title": "Schedule",
    "section": "October 27, 2025",
    "text": "October 27, 2025\n\nTidying Datasets\n\nDue TodayFurther Resources\n\n\n 6. Tidy Data , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n 26. Parsing dates and times , list() Read in Perusall\n\n\n tidyr Cheatsheet"
  },
  {
    "objectID": "schedule.html#october-29-2025",
    "href": "schedule.html#october-29-2025",
    "title": "Schedule",
    "section": "October 29, 2025",
    "text": "October 29, 2025\n\nPivoting Datasets\n\nDue TodayFurther Resources\n\n\n Lab 6 Due\n\n\n Lab 6 Template\n tidyr Cheatsheet"
  },
  {
    "objectID": "schedule.html#october-31-2025",
    "href": "schedule.html#october-31-2025",
    "title": "Schedule",
    "section": "October 31, 2025",
    "text": "October 31, 2025\n\nLab: Pivoting and Tidying Datasets\n\nDue TodayFurther Resources\n\n\n\n\n\n tidyr Cheatsheet\n Lab 7 Instructions"
  },
  {
    "objectID": "schedule.html#november-03-2025",
    "href": "schedule.html#november-03-2025",
    "title": "Schedule",
    "section": "November 03, 2025",
    "text": "November 03, 2025\n\nWriting Functions\n\nDue TodayFurther Resources\n\n\n 7. Iteration , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall"
  },
  {
    "objectID": "schedule.html#november-05-2025",
    "href": "schedule.html#november-05-2025",
    "title": "Schedule",
    "section": "November 05, 2025",
    "text": "November 05, 2025\n\nIteration\n\nDue TodayFurther Resources\n\n\n Lab 7 Due\n\n\n Lab 7 Template\n purrr cheatsheet"
  },
  {
    "objectID": "schedule.html#november-07-2025",
    "href": "schedule.html#november-07-2025",
    "title": "Schedule",
    "section": "November 07, 2025",
    "text": "November 07, 2025\n\nLab: Programming in R\n\nDue TodayFurther Resources\n\n\n Project 2 Due\n\n\n purrr cheatsheet\n Lab 8 Instructions"
  },
  {
    "objectID": "schedule.html#november-10-2025",
    "href": "schedule.html#november-10-2025",
    "title": "Schedule",
    "section": "November 10, 2025",
    "text": "November 10, 2025\n\nMap Projections and Spatial Thinking\n\nDue TodayFurther Resources\n\n\n 17. Working with geospatial data (17.1-17.3) , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall"
  },
  {
    "objectID": "schedule.html#november-12-2025",
    "href": "schedule.html#november-12-2025",
    "title": "Schedule",
    "section": "November 12, 2025",
    "text": "November 12, 2025\n\nIntroduction to Leaflet\n\nDue TodayFurther Resources\n\n\n Lab 8 Due\n\n\n Lab 8 Template\n Leaflet Guide"
  },
  {
    "objectID": "schedule.html#november-14-2025",
    "href": "schedule.html#november-14-2025",
    "title": "Schedule",
    "section": "November 14, 2025",
    "text": "November 14, 2025\n\nLab: Mapping Point Data in Leaflet\n\nDue TodayFurther Resources\n\n\n\n\n\n Leaflet Guide\n Lab 9 Instructions"
  },
  {
    "objectID": "schedule.html#november-17-2025",
    "href": "schedule.html#november-17-2025",
    "title": "Schedule",
    "section": "November 17, 2025",
    "text": "November 17, 2025\n\nPolygon Mapping in Leaflet\n\nDue TodayFurther Resources\n\n\n 17. Working with geospatial data (17.4-17.8) , Baumer, Benjamin S., Daniel T. Kaplan, and Nicholas J. Horton (2021). Modern Data Science with R. 2nd. CRC Press. (Visited on Jan. 14, 2022). Read in Perusall\n Project 3 Assigned\n\n\n MP3 Template Your team name should be a string with your team members’ last names.\n Leaflet Guide"
  },
  {
    "objectID": "schedule.html#november-19-2025",
    "href": "schedule.html#november-19-2025",
    "title": "Schedule",
    "section": "November 19, 2025",
    "text": "November 19, 2025\n\nHow to Lie with Maps\n\nDue TodayFurther Resources\n\n\n Lab 9 Due\n\n\n Lab 9 Template\n Leaflet Guide"
  },
  {
    "objectID": "schedule.html#november-21-2025",
    "href": "schedule.html#november-21-2025",
    "title": "Schedule",
    "section": "November 21, 2025",
    "text": "November 21, 2025\n\nLab: Mapping Polygon Data with Leaflet\n\nDue TodayFurther Resources\n\n\n Group Contract Due\n\n\n Leaflet Guide\n Lab 10 Instructions"
  },
  {
    "objectID": "schedule.html#november-24-2025",
    "href": "schedule.html#november-24-2025",
    "title": "Schedule",
    "section": "November 24, 2025",
    "text": "November 24, 2025\n\nReview\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#november-26-2025",
    "href": "schedule.html#november-26-2025",
    "title": "Schedule",
    "section": "November 26, 2025",
    "text": "November 26, 2025\n\nNo Class\n\nDue TodayFurther Resources\n\n\n Lab 10 Due\n\n\n Lab 10 Template"
  },
  {
    "objectID": "schedule.html#november-28-2025",
    "href": "schedule.html#november-28-2025",
    "title": "Schedule",
    "section": "November 28, 2025",
    "text": "November 28, 2025\n\nNo Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-01-2025",
    "href": "schedule.html#december-01-2025",
    "title": "Schedule",
    "section": "December 01, 2025",
    "text": "December 01, 2025\n\nWorking with APIs\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-03-2025",
    "href": "schedule.html#december-03-2025",
    "title": "Schedule",
    "section": "December 03, 2025",
    "text": "December 03, 2025\n\nAdvanced APIs\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-05-2025",
    "href": "schedule.html#december-05-2025",
    "title": "Schedule",
    "section": "December 05, 2025",
    "text": "December 05, 2025\n\nLab: Working with APIs\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-08-2025",
    "href": "schedule.html#december-08-2025",
    "title": "Schedule",
    "section": "December 08, 2025",
    "text": "December 08, 2025\n\nWork on Group Projects in Class\n\nDue TodayFurther Resources"
  },
  {
    "objectID": "schedule.html#december-10-2025",
    "href": "schedule.html#december-10-2025",
    "title": "Schedule",
    "section": "December 10, 2025",
    "text": "December 10, 2025\n\nWrap-up\n\nDue TodayFurther Resources\n\n\n Project 3 Due"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Problem Solving\nNote: We will not complete this lab this semester since this material is now covered in SDS 100. However, you may wish to refer back to this lab if you are looking fore resources to get help in R. This lab will introduce you to resources and techniques for problem solving in R. You should reference this lab often throughout the semester for reminders on best practices for addressing errors and getting help.\n\nProblem Solving Lab Instructions\nTemplate\n\n\n\nLab 1: Understanding Datasets\nThis lab is all about learning to understand the context and parts of a dataset by referencing and interpreting data dictionaries and technical data documentation. We will get to know the U.S. Department of Education’s College Scorecard data, which includes over 3000 variables characterizing colleges in the U.S.\n\nLab 1 Instructions\nTemplate\n\n\n\nLab 2: Visualization Aesthetics\nThis week we will practice mapping variables in the U.S. National Bridge Inventory onto different plot aesthetics in order to tell different stories with the data. We’re going to look at what kinds of variables might contribute to poor bridge conditions, where there are poor bridge conditions, and which entities are responsible for maintaining them. We will only be creating one type of plot today - a scatterplot. However, we are going to show how we can use different visual cues to plot a number of different variables onto a scatterplot.\n\nLab 2 Instructions\nTemplate\n\n\n\nLab 3: Plotting Freqencies\nIn this lab, you will practice plotting both frequencies and distributions by analyzing data about racial disparities in home mortgage denial rates in Mississippi in 2024.\n\nLab 3 Instructions\nTemplate\n\n\n\nLab 4: GitHub\nThis lab is designed to help you get acquainted with the concepts behind Git and GitHub, suggested workflows for collaborating on projects in this course, and error resolution strategies.\n\nLab 4 Instructions\n\n\n\nLab 5: Data Wrangling\nIn this lab, you will apply 6 data wrangling verbs in order to analyze data regarding NYPD stop, question, and frisk. Specifically, we will replicate data analysis performed by the NYCLU in 2011 to demonstrate how the practice was being carried out unconstitutionally in New York.\n\nLab 5 Instructions\nTemplate\n\n\n\nLab 6: Joining Datasets\nIn terms of data analysis, this lab has one goal: to determine the number of industrial facilities that are currently in violation of both the Clean Air Act and the Clean Water Act in California. To achieve this goal, we’re going to have to do some data wrangling and join together some datasets published by the EPA. We’re going to practice applying different types of joins to this data and consider what we learn with each.\n\nLab 6 Instructions\nTemplate\n\n\n\nLab 7: Tidying Datasets\nIn this lab, we will create a few data visualizations documenting point-in-time counts of homelessness in the United States. Specifically, we are going visualize data collected in 2020 through various Continuums of Care (CoCs) programs. In order to produce these data visualizations, you will need to join homelessness data with census population data and develop and execute a plan for how to wrangle the dataset into a “tidy” format.\n\nLab 7 Instructions\nTemplate\n\n\n\nLab 8: Programming with Data\nIn this lab, we will program some custom R functions that allow us to analyze data related to medical conflicts of interest. Specifically, we will determine which ten Massachusetts-based doctors received the most money from pharmaceutical or medical device manufacturers in 2021. Then we will leverage our custom functions to produce a number of tables and plots documenting information about the payments made to each of these doctors. In doing so, we will update a similar analysis produced by ProPublica in 2018 called Dollars for Docs.\n\nLab 8 Instructions\nTemplate\n\n\n\nLab 9: Point Mapping with Leaflet\nIn this lab, we will build a map that visualizes the extent of toxic emissions in Louisiana’s Cancer Alley, using Toxic Release Inventory data. In doing so, we will gain practice in producing point maps in Leaflet.\n\nLab 9 Instructions\nTemplate\n\n\n\nLab 10: Polygon Mapping with Leaflet\nIn this lab, we will extend the analysis we did in lab 9 to consider the environmental injustices along Louisiana’s Cancer Alley. We will map census data of race demographics along cancer alley to understand the disparate impact of pollution in this region.\n\nLab 10 Instructions\nTemplate\n\n\n\nReview + Working with APIs\nIn this lab, we will write queries to access subsets of a very large dataset on the NYC Open Data Portal. We will practice all of the standards we have learned in the course so far in visualizing and wrangling the resulting data.\n\nReview Lab Instructions\nTemplate"
  },
  {
    "objectID": "labs/lab8.html",
    "href": "labs/lab8.html",
    "title": "Lab 8: Programming with Data",
    "section": "",
    "text": "In this lab, we will program some custom R functions that allow us to analyze data related to medical conflicts of interest. Specifically, we will determine which ten Massachusetts-based doctors received the most money from pharmaceutical or medical device manufacturers in 2024. Then we will leverage our custom functions to produce a number of tables and plots documenting information about the payments made to each of these doctors. In doing so, we will update a similar analysis produced by ProPublica in 2018 called Dollars for Docs.\n\n\n\nWrite custom R functions\nIterate a function over the values in a vector, using the family of map functions\nPractice data cleaning and wrangling"
  },
  {
    "objectID": "labs/lab8.html#introduction",
    "href": "labs/lab8.html#introduction",
    "title": "Lab 8: Programming with Data",
    "section": "",
    "text": "In this lab, we will program some custom R functions that allow us to analyze data related to medical conflicts of interest. Specifically, we will determine which ten Massachusetts-based doctors received the most money from pharmaceutical or medical device manufacturers in 2024. Then we will leverage our custom functions to produce a number of tables and plots documenting information about the payments made to each of these doctors. In doing so, we will update a similar analysis produced by ProPublica in 2018 called Dollars for Docs.\n\n\n\nWrite custom R functions\nIterate a function over the values in a vector, using the family of map functions\nPractice data cleaning and wrangling"
  },
  {
    "objectID": "labs/lab8.html#review-of-key-terms",
    "href": "labs/lab8.html#review-of-key-terms",
    "title": "Lab 8: Programming with Data",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this purrr Cheatsheet when completing this lab.\n\n\n\nFunction\n\na series of statements that returns a value or performs a task\n\nIteration\n\nrepeating a task over a series of values, vectors, or lists"
  },
  {
    "objectID": "labs/lab8.html#cmss-open-payments-dataset",
    "href": "labs/lab8.html#cmss-open-payments-dataset",
    "title": "Lab 8: Programming with Data",
    "section": "CMS’s Open Payments Dataset",
    "text": "CMS’s Open Payments Dataset\nIn 2010, the the Physician Payments Sunshine Act (2010) was passed, requiring medical drug and device manufacturers to disclose payments and other transfers of value made to physicians, non-physician practitioners, and teaching hospitals. This law was put into place to promote transparency in our medical system - enabling the U.S. government and citizens to monitor for potential medical conflicts of interest. Today, every time a drug or medical device manufacturer makes a payment to a covered recipient, they must disclose the nature of that payment and the amount to the U.S. Centers for Medicare & Medicaid. Data about payments is then aggregated, reviewed by (and sometimes disputed by) recipients, corrected, and then published as an open government dataset.\nDefinitions for what counts as a reporting entity, a covered recipient, and a reportable activity have been expanding since the passing of the Physician Payments Sunshine Act as legislators have raised concerns over the degree of transparency of diversifying financial arrangements in the healthcare system. In 2020, the first settlement for violations to the Sunshine Act was announced, requiring Medtronic Inc. to pay $9.2 million to resolve allegations for failure to report. This served as a signal that enforcement is ramping up in the coming years. In 2022, the state of California passed a law requiring that medical practitioners disclose to patients that this data resource exists."
  },
  {
    "objectID": "labs/lab8.html#setting-up-your-environment",
    "href": "labs/lab8.html#setting-up-your-environment",
    "title": "Lab 8: Programming with Data",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(RColorBrewer)\nopen_payments_original &lt;- read_csv(\"https://openpaymentsdata.cms.gov/api/1/datastore/query/e6b17c6a-2534-4207-a4a1-6746a14911ff/0/download?conditions%5B0%5D%5Bproperty%5D=recipient_state&conditions%5B0%5D%5Bvalue%5D=MA&conditions%5B0%5D%5Boperator%5D=%3D&format=csv\") |&gt;\n  select(Covered_Recipient_NPI,\n         Covered_Recipient_First_Name:Covered_Recipient_Last_Name,\n         Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_ID,\n         Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name,\n         Recipient_City,\n         Recipient_State,\n         Covered_Recipient_Specialty_1,\n         Total_Amount_of_Payment_USDollars,\n         Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_1,\n         Product_Category_or_Therapeutic_Area_1,\n         Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_1,\n         Date_of_Payment,\n         Nature_of_Payment_or_Transfer_of_Value,\n         Number_of_Payments_Included_in_Total_Amount,\n         Form_of_Payment_or_Transfer_of_Value,\n         Dispute_Status_for_Publication,\n         Payment_Publication_Date) |&gt;\n  filter(!is.na(Covered_Recipient_NPI))"
  },
  {
    "objectID": "labs/lab8.html#cleaning-up-this-data-frame",
    "href": "labs/lab8.html#cleaning-up-this-data-frame",
    "title": "Lab 8: Programming with Data",
    "section": "Cleaning up this Data Frame",
    "text": "Cleaning up this Data Frame\nEventually we are going to plot some timelines of payments to specific doctors, and we will need the Date_of_Payment column to be in a date-time format to do so. Right now however, these columns are strings. To get started with cleaning up this dataset, let’s convert the date columns in open_payments_original to a date-time format.\n\nQuestion\n\nWrite code to convert the Date_of_Payment and the Payment_Publication_Date column to date-time format. You should first determine the format of the date in Date_of_Payment and Payment_Publication_Date and then reference the lubridate to determine the corresponding function for parsing that date format. Finally, you will mutate the two columns.\n\nOptional challenge: Rather than mutating each column, see if you can mutate() across() the two columns to complete this step.\n\n\n\n# Uncomment below to write code to convert to date-time format here. \n\n# open_payments_dates_cleaned &lt;- open_payments_original |&gt;\n\n\n\nTo confirm that we’ve done this right, we can check whether both of the dates are in the date-time format.\n\n\n\nQuestion\n\nThe is.Date function returns TRUE if a vector is in date-time format, and FALSE if it is not. Below, I’ve selected the two columns in open_payments that contain the word “date” in the column header. Determine which map() function to use in order to return a vector that indicates whether these columns are in a date-time format. If you’ve done everything correctly, you should get the output below.\n\n\n# Select the appropriate map function below\n\nopen_payments_dates_cleaned |&gt;\n  select(contains(\"date\")) |&gt;\n  _____(is.Date)\n\n\n\n         Date_of_Payment Payment_Publication_Date \n                    TRUE                     TRUE \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out what happens if you swap out your map function above for map, map_chr, or map_int. Can you figure out the relationship between these values and the original values?\n\n\nIt’s important to note that the unit of observation in this dataset is not one medical practitioner, and it is not one manufacturer. Instead it is one payment from a manufacturer to a medical practitioner. That means that a medical practitioner can appear multiple times in the dataset if they’ve received multiple payments, and a medical drug or device manufacturer can appear multiple times in the dataset if they’ve disbursed multiple payments. We can identify medical practitioners with the Covered_Recipient_NPI column and manufacturers with the Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_ID column.\n…but we want to know more than just the ID of a medical practitioner. To identify which doctors are receiving the most money, we also want to know that practitioner’s name, location, specialty, etc. Because practitioners’ names are manually entered into a database every time a payment is made to them, sometimes the formatting of a practitioner’s name entered for one payment can differ from how that same practitioner’s name is formatted when entered for another payment. The same goes for other variables related to that practitioner. For instance, check out how the capitalization differs for the practitioner below. In some cases, there is a middle initial, while in others, there is a full middle name; in some cases, the practitioner’s name is all capitalized, and in other cases it is not.\n\nopen_payments_dates_cleaned |&gt;\n  filter(Covered_Recipient_NPI == 1003047473) |&gt;\n  select(Covered_Recipient_First_Name, Covered_Recipient_Middle_Name, Covered_Recipient_Last_Name)\n\n\n  \n\n\n\nThis issue with formatting exists across this entire dataset. To ensure that similar entities appear in the right buckets when we aggregate the data, we are going to standardize capitalization across the whole dataset. We’re also going to leave out the practitioner’s middle initial since it is not always included (or included in the same way).\n\n\n\nQuestion\n\nWrite code to mutate across all character columns such that strings in these columns are converted to title case. Title case refers to casing where the first letter in each word is capitalized and all other letters are lowercase. Strings can be converted to title case with the function str_to_title.\nAfter you’ve done this, mutate a new column called Covered_Recipient_Full_Name that concatenates (hint: i.e. paste()) together Covered_Recipient_First_Name and Covered_Recipient_Last_Name.\nStore the resulting data frame in open_payments.\n\n\n# Uncomment below to clean up strings\n\n# open_payments &lt;- open_payments_dates_cleaned |&gt;\n\n\n\nAs we saw before, one Covered_Recipient_NPI was associated with multiple names if the names were capitalized in some cases and not others. Now that we’ve standardized the formatting of these names, there ideally should be one full name associated with every Covered_Recipient_NPI. Let’s compare the length of unique Covered_Recipient_NPI values to the length of unique Covered_Recipient_Full_Name values to check whether this is the case.\n\n\n\nQuestion\n\nWrite a function below called num_unique. The function should calculate the length of unique values in the vector passed to the argument x.\nBelow, I’ve selected the two columns in open_payments that we want to iterate this function over. Determine which map() function to use in order to return a numeric vector that indicates the length of unique values in each of these columns. If you’ve done everything correctly, you should get the output below.\n\n\nnum_unique &lt;- function(x) {\n # Write function here.\n}\n\nopen_payments |&gt;\n  select(Covered_Recipient_NPI, Covered_Recipient_Full_Name) |&gt;\n  _____(_____) # Determine which map function to call here.\n\n\n\n      Covered_Recipient_NPI Covered_Recipient_Full_Name \n                      22244                       22316 \n\n\n\n\nNotice that there are still more full names than Covered_Recipient_NPIs, which means that certain doctors have multiple names in this dataset. Below I’ve written some code to calculate the number unique full names listed for each Covered_Recipient_NPI and filter to the rows with more than one name. Can you identify some reasons why we might have multiple names listed for this same medical practitioner in this data frame?\n\nopen_payments |&gt;\n  group_by(Covered_Recipient_NPI) |&gt;\n  mutate(num_names = length(unique(Covered_Recipient_Full_Name))) |&gt;\n  ungroup() |&gt;\n  filter(num_names &gt; 1) |&gt;\n  select(Covered_Recipient_NPI, Covered_Recipient_Full_Name) |&gt;\n  distinct() |&gt;\n  arrange(desc(Covered_Recipient_NPI))\n\n\n  \n\n\n\nBecause of these issues, it is important that we use the Covered_Recipient_NPI to identify doctors vs. the full name.\nNow that we have a final cleaned up open_payments data frame, let’s remove the other data frames from our environment.\n\nrm(open_payments_original, open_payments_dates_cleaned)\n\n…and on to analysis."
  },
  {
    "objectID": "labs/lab8.html#data-analysis",
    "href": "labs/lab8.html#data-analysis",
    "title": "Lab 8: Programming with Data",
    "section": "Data Analysis",
    "text": "Data Analysis\nUltimately, our aim is to produce a number of tables and plots for each of the ten MA-based doctors that received the most money from medical drug and device manufacturers in 2024. This means that one of our first analysis steps is to identify those 10 medical practitioners.\n\nQuestion\n\nWrite code to determine the 10 medical practitioners that received the most money from drug and device manufacturers in 2024, and store your results in top_10_doctors. Your final data frame should have 10 rows and columns for Covered_Recipient_NPI and sum_total_payments.\n\n\n# Uncomment below and write data wrangling code\n\n#top_10_doctors &lt;- open_payments |&gt;\n\n\n\nRight now the values that we will eventually want to iterate over in our analysis are stored as columns in a dataframe. …but remember that the family of purrr functions allows us to apply a function to each element of a vector or list. We want to create a series of vectors from these columns that we can iterate over. We will use the pull() function to do this.\n\n\n\nQuestion\n\nCreate a vector of top_10_doctors_ids from top_10_doctors, using the pull() function.\n\n\n# Uncomment and write code below to pull the top 10 doctor IDs into a vector\n\n# top_10_doctors_ids &lt;- top_10_doctors |&gt;\n\n\n\nWe also want a vector of doctor names associated with each of these IDs, but remember that there can be multiple names for a single doctor in this dataset. With this in mind, we are going to create a vector of the first listed name for a given Covered_Recipient_NPI in the dataset. Taking the first listed name as the doctor’s name is an imperfect solution. The first listed name could be a misspelling. It could be a doctor’s maiden name that they have since changed. This is a temporary solution, and we would want to confirm that we have the correct name for each doctor before publishing any of these findings.\n\n\n\nQuestion\n\nCreate a vector containing the names of the doctors associated with the IDs in top_10_doctors_ids. First, define the function get_doctor_name. This function will:\n\ntake a doctor_id as an argument,\nfilter open_payments to that ID,\nsummarize the first() Covered_Recipient_Full_Name listed for that ID,\npull() the name value\n\nOnce this function has been defined, select the appropriate map() function to iterate top_10_doctors_ids through get_doctor_name and store the resulting character vector in top_10_doctors_names.\n\n\nget_doctor_name &lt;- function(doctor_id){\n  # Write function code here\n}\n\n# Iterate the top_10_doctors_ids vector through get_doctor_name and store the results in a character vector\n\n# top_10_doctors_names &lt;- \n\n\n\n\nNow that we have the vectors we want to iterate over, we are ready to start defining our first functions.\n\n\nWhat kind of payments did MA-based doctors receive in 2024?\nTo get started, let’s define a function that filters open_payments to a given doctor ID, and then calculates how much of each kind of payment has been paid to that doctor. Here is an example of what that data wrangling code would look like for a specific Covered_Recipient_NPI:\n\nopen_payments |&gt;\n  filter(Covered_Recipient_NPI == 1003801903) |&gt;\n  group_by(Nature_of_Payment_or_Transfer_of_Value) |&gt;\n  summarize(num_payments = \n              sum(Number_of_Payments_Included_in_Total_Amount),\n            total_payments = sum(Total_Amount_of_Payment_USDollars))\n\n\n  \n\n\n\n\nQuestion\n\nWrap the above code in a function named calculate_payment_type_amts. Rather than filtering to 1003801903, filter based on the value passed to an argument named doctor_id.\nThen, use the map() function to apply calculate_payment_type_amts to each element in the top_10_doctors_ids vector. Running this code should return a list of 10 data frames.\nFinally, pipe in set_names(top_10_doctors_names) to set the names for each data frame in the list to the doctor’s name.\n\n\n# Write calculate_payment_type_amts function here\n\n# Iterate calculate_payment_type_amts over top_10_doctors_ids and set names to top_10_doctors_names\n\n\n\n\n\n\n\nWhen were payments made to each of these doctors in 2024?\nHere’s an example of a plot we could create to answer this question for one doctor.\n\n  open_payments |&gt;\n    filter(Covered_Recipient_NPI == 1003801903) |&gt;\n    ggplot(aes(x = day(Date_of_Payment), \n               y = \"\", \n               fill = Total_Amount_of_Payment_USDollars)) +\n    geom_jitter(pch = 21, size = 2, color = \"black\") +\n    theme_minimal() +\n    labs(title = \"Roger Hajjar\", \n         y = \"\", \n         x = \"Day\",\n         fill = \"Payment Amount\") +\n    scale_y_discrete(limits = rev) +\n    scale_fill_distiller(palette = \"BuPu\", direction = 1, labels = scales::comma) +\n    facet_wrap(vars(month(Date_of_Payment, label = TRUE)), nrow = 4) \n\n\n\n\n\n\n\n\n\nQuestion\n\nWrite a function named payments_calendar. The function should:\n\nTake a doctor_id and doctor_name as arguments\nFilter open_payments to the doctor’s ID\nCreate payment calendar plot modeled after the one above.\nSet the title of the plot to the doctor’s name\n\nAfter you’ve written this function, select the appropriate map function to apply payments_calendar to each element in the top_10_doctors_ids vector and top_10_doctors_names vector.\n\nOptional Challenge: Extend your code to include the first listed specialty for each top 10 doctor as a subtitle in each plot.\n\n\n\n# Write payments_calendar function here\n\n# Iterate payments_calendar over top_10_doctors_ids and top_10_doctors_ids to create 10 plots\n\n\n\n\n\n\n\nWhich manufacturers paid MA-based doctors in 2024, and through what forms of payment?\nFinally, let’s define a function that filters open_payments to a given doctor ID and determines how much the doctor received in compensation from different manufacturers, along with the forms of payment from each manufacturer. To do so, we will need to aggregate the data by Covered_Recipient_NPI, Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name, and Form_of_Payment_or_Transfer_of_Value and calculate the total payments associated with each grouping.\n\nQuestion\n\nWrite a function named calculate_manufacturer_payments. The function should:\n\nTake a doctor_id as an argument\nFilter open_payments to that ID\nAggregate the filtered data by Covered_Recipient_NPI, Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name, and Form_of_Payment_or_Transfer_of_Value\nCalculate the total amount of payments for each grouping\nSort the resulting data frame in descending order by the total amount of payments.\n\nAfter you’ve written this function, use the map_df() function to apply calculate_manufacturer_payments to each element in the top_10_doctors_ids vector. Note how this returns one data frame rather than a list of 10 data frames.\nPlot your resulting data frame as a column plot, attempting (to the best of your ability) to match the formatting of the plot below.\n\nOptional Challenge: List the doctor’s full name in each facet band, rather than the the doctor’s ID.\n\n\n\n# Write calculate_manufacturer_payments function here\n\n# Iterate calculate_manufacturer_payments over top_10_doctors_ids here\n\n# Plot resulting data frame here\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtra Credit (+5 points on lab)\nWrite a function named drugs_and_devices. The function should create a plot that shows how much money a given doctor was paid for each drug or biological device for which they received payment in 2024, grouped by the nature of payment.\nAfter you’ve written this function, use a map function to apply drugs_and_devices to each element in the top_10_doctors_ids vector. You should produce 10 plots - one for each doctor.\n\n\n# Create function here\n\n# Apply function here\n\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n[[6]]\n\n\n\n\n\n\n\n\n\n\n[[7]]\n\n\n\n\n\n\n\n\n\n\n[[8]]\n\n\n\n\n\n\n\n\n\n\n[[9]]\n\n\n\n\n\n\n\n\n\n\n[[10]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nOn September 29, 2022, California Governor Gavin Newsom signed a bill requiring that all physicians and surgeons notify patients about the Open Payments database on their initial visit. Specifically, patients will be given the following notice: “The Open Payments database is a federal tool used to search payments made by drug and device companies to physicians and teaching hospitals. It can be found at https://openpaymentsdata.cms.gov,” and will be prompted to sign and date that they have received the notice. This policy was developed in response to concerns that, while the Open Payments database includes a great deal of information that might impact how citizens make decisions about their healthcare, very few people knew of the database. Do you think this is a good solution to this problem? Should other states follow suit? Do you see any drawbacks to this policy? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4: Git and GitHub",
    "section": "",
    "text": "There are many reasons that Git and GitHub are essential infrastructures for collaborative coding projects. For one Git saves snapshots of a code repository at different stages of a project so that we can track how it has changed over time and revert back to an older version if we discover a more recent error. We call this version control. Certain Git features also facilitate many people working on a coding project at once, by providing a number of tools to help prevent collaborators from over-writing each other’s work. These features also make it possible for developers to simultaneously modify, extend, and test components of the code without jeopardizing the project’s current functionality. I use GitHub for assignment submission in this course because it offers a number of features for commenting on and making suggestions in your code, which will be super helpful when reviewing your group projects. Further, GitHub supports code publication; by publishing code on GitHub, you contribute to an open access/free software community, enabling others to learn from and build off of your work.\nDespite all of these awesome benefits, there can be a significant learning curve when getting started with Git and GitHub. There are new vocabularies, workflows, and error mitigation strategies to learn when getting started. This lab is designed to help you get acquainted with the concepts behind Git and GitHub, suggested workflows for collaborating on projects in this course, and error resolution strategies.\n\n\n\nCreate, update, and close issues\nBranch a repo\nIssue pull requests\nAddress common push/pull errors\nAddress merge conflicts"
  },
  {
    "objectID": "labs/lab4.html#introduction",
    "href": "labs/lab4.html#introduction",
    "title": "Lab 4: Git and GitHub",
    "section": "",
    "text": "There are many reasons that Git and GitHub are essential infrastructures for collaborative coding projects. For one Git saves snapshots of a code repository at different stages of a project so that we can track how it has changed over time and revert back to an older version if we discover a more recent error. We call this version control. Certain Git features also facilitate many people working on a coding project at once, by providing a number of tools to help prevent collaborators from over-writing each other’s work. These features also make it possible for developers to simultaneously modify, extend, and test components of the code without jeopardizing the project’s current functionality. I use GitHub for assignment submission in this course because it offers a number of features for commenting on and making suggestions in your code, which will be super helpful when reviewing your group projects. Further, GitHub supports code publication; by publishing code on GitHub, you contribute to an open access/free software community, enabling others to learn from and build off of your work.\nDespite all of these awesome benefits, there can be a significant learning curve when getting started with Git and GitHub. There are new vocabularies, workflows, and error mitigation strategies to learn when getting started. This lab is designed to help you get acquainted with the concepts behind Git and GitHub, suggested workflows for collaborating on projects in this course, and error resolution strategies.\n\n\n\nCreate, update, and close issues\nBranch a repo\nIssue pull requests\nAddress common push/pull errors\nAddress merge conflicts"
  },
  {
    "objectID": "labs/lab4.html#review-of-key-terms",
    "href": "labs/lab4.html#review-of-key-terms",
    "title": "Lab 4: Git and GitHub",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nRepo\n\nCollaborative storage space for folders, documents, data, and code\n\nBranch\n\nAn isolated version of a repo that can be modified without affecting the main branch\n\nClone\n\nCreates a copy of a repo stored in a remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nPull\n\nDownloads the latest version of a repo from remote space (e.g. GitHub) to your local machine (e.g. your computer)\n\nStage\n\nThe process of marking which changes of the code are ready to be saved\n\nCommit\n\nA stored snapshot of a repo at a particular moment in time\n\nPush\n\nUploads commits from your local machine (e.g. your computer) to a remote space (e.g. GitHub)\n\nPull Request\n\nA request for modified code to be integrated with a different branch\n\nMerge\n\nThe process of integrating code modifications from one branch into another branch"
  },
  {
    "objectID": "labs/lab4.html#github-flow",
    "href": "labs/lab4.html#github-flow",
    "title": "Lab 4: Git and GitHub",
    "section": "GitHub Flow",
    "text": "GitHub Flow\n\ngit vs. GitHub.com\nAt the beginning of this semester, many of you installed git on your computers. git is a series of commands available in our computers to save snapshots of files at different moments in time. On the other hand, GitHub.com is a website where we can store projects that have been configured with git commands. As you’ve been accepting lab assignments for this course, project folders (what we will call repositories) have been created for you at GitHub.com. If you navigate to GitHub.com right now, you will see a list of these project folders, and you can click through each of the them to see changes that have been made. However, when you created projects for each of these repositories in RStudio, you were using git commands on your computer to copy the project from GitHub.com to your computer. Similarly, you were using git commands when you staged, committed, and pushed your changes back to GitHub.com. In sum, git is a series of command line tools to manage changes to files. GitHub.com is a Web hosting platform for storing git projects.\n\n\nRemote vs. Local\nWe can store, edit, and publish files on GitHub.com without ever copying them to our local computers. Files at GitHub.com are technically stored on a server somewhere, so we will refer to the projects stored at GitHub.com as remote (i.e. distant, far-off, etc). Similarly, we can use git commands to save snapshots of different versions of files on our local computers without ever pushing the changes to GitHub.com, so we will refer to the projects stored on on our computers as local. In this course, we need the ability to move changes to files between our local and remote repositories.\nWhy can’t we just do everything at GitHub.com or everything on our local machines?? Well GitHub.com doesn’t have the nice environment for writing and running code that you have in RStudio, so we need to be able to move things to your local computers so that you can work on the code in RStudio. On the other hand, if all the work was done on your local computer and never pushed to GitHub.com, I (your instructor) would never see it! So the first important consideration in the GitHub Flow is how do we move changes made on our local machines to GitHub.com and vice versa.\nThe great news is that you’ve already been doing this! The primary way that we move changes between remote and local is through two git commands: pull and push. Pull copies changes from a remote repository to our local machines. Push pushes changes from our local machines to a remote repository. It looks like this:\n\nTypically, we want to make sure that we always follow this flow:\n\nPull changes from GitHub.com (remote) to local.\nMake changes locally,\nPush changes to GitHub.com.\n\nAs we are going to see later in the lab, the most frustrating issues with GitHub emerge when we break this flow.\n\n\nTo Branch or Not to Branch?\nIn my opinion, there are two kinds of workflows for GitHub. There’s the quick and dirty version, and there’s the long and elegant version. We just went over the Quick and Dirty version, and you’ve already had practice in it when submitting all of your labs for the course. Below are the differences between these two workflows (don’t worry about if you don’t understand what the steps mean right now; we will learn all of them in the lab).\n\n\n\n\n\n\n\nQuick and Dirty Version\nLong and Elegant Version\n\n\n\n\n\nPull recent changes from GitHub to local machine.\nMake edits and save them\nStage and commit changes.\nPush changes from local machine to GitHub.\n\nIn the quick and dirty version, all of this occurs in the main branch.\n\nCreate an issue at GitHub.com\nBranch the repo at GitHub.com.\nPull recent changes from GitHub.com to local machine.\nMake edits in the new branch and save them.\nStage and commit changes.\nPush changes from local machine to GitHub.com.\nCreate a pull request at GitHub.com\nAssign a reviewer to review the proposed changes and wait for their approval.\nMerge changes, while also closing the issue and deleting the branch.\n\n\n\n\nTypically I recommend the long and elegant version for group work as it is designed to avoid errors and ensure that collaborators are all on the same page regarding changes to files. However, occasionally when you have to make small, quick changes to a file, and it won’t impact your teammate’s work, it will make more sense to follow the quick and dirty workflow. The goal for today is to get practice in the long and elegant version.\n\n\nRepo\n\nQuestion\n\nNavigate to GitHub Classroom to accept the assignment. For this assignment, you will work with your Project 1 team members. You can find your group number at CATME.org.\n\n\n\nThis will create a repository at GitHub.com called github-practice. You’ll notice that the repository has a few files - README.md, github-practice.Rmd, .gitingore. You’ll also notice in the repo’s right sidebar that your group members are listed as collaborators. This means that you all have access to read and write to this repository.\n\n\n\n\nClone\nCloning is a git command that copies a remote repository to your local machine. In other words, it will copy all of the project files in the remote GitHub.com folder to your local computer.\n\nQuestion\n\nAll group members should clone the repo to their RStudio environment. To do so, copy the repo’s URL at GitHub.com. You will want to make sure you are in the main project folder when you copy this URL; it won’t work if you’ve clicked through to any of the files. Then in RStudio click on File &gt; New Project &gt; Version Control &gt; Git, and paste the copied URL into the window that appears. Note what you see in the RStudio files pane after cloning the repo.\n\n\n\nRemember that cloning creates a copy of a remote repo on a local machine. In creating this project, you’ve copied all of the files that make up the github-practice repo at GitHub.com to your local computer. This means that you will find all of the files associated with this repo by navigating to the folder where you created the project on your computer.\nIt’s important to note that this is not just any old folder on your computer though. By cloning, you’ve created a git folder. This means that the folder has been set up in a way where git can track the changes that you make to it over time, and it knows that there is a remote version of the repository somewhere that you might want to keep it consistent with. If you tried to just create a new folder on your computer, it wouldn’t necessarily have these nice features.\n\n\n\n\nIssues\n\nQuestion\n\nNavigate back to GitHub.com, and click on the repo’s Issues tab. Each member of your team should create an issue by clicking the ‘New Issue’ button. Title the issue: “Adding &lt;your name&gt; to the assignment.” Submit the issue, and to the left of the screen, assign the issue to yourself.\n\n\n\n\nIssues support project planning by allowing you to track changes you hope to make to your project over time. By assigning issues to certain collaborators on your project team, you can have clear documentation of who is responsible for what.\n\n\n\n\n\n\nTip\n\n\n\nIn my own projects, I use Issues for a number of purposes. Sometimes I use Issues to bugs that I notice in my code that need to be fixed. Other times I use them to track features that I would like to add to my code down the road. Oftentimes, in my public repositories, I encourage others that are using my code to submit issues to ask questions about how something works, to report bugs, or to request features.\n\n\n\n\n\n\nBranch\nWhen you first create a repository, all of the code will be stored in the main branch of the repository. If you were to think of a project like a tree growing up from the ground, then the main branch would be the like the trunk of the tree. We don’t want the main branch to break because the whole tree could come down. One goal of a branching workflow in GitHub is to keep the most stable and polished versions of code in the main branch.\nSo what do we do in the meantime - when we’re editing code, potentially breaking things, trying to sort out its bugs, and it’s not quite in that stable and polished state yet? That’s where branching comes in.\nWhen we create a branch of our repo, GitHub creates a separate copy of the repo where you can make changes without impacting what’s in the main branch. Later, once we’re done making changes and things are stable and polished, we will have the opportunity to merge those changes back into the main branch.\nThink of it another way: You write a rockstar first draft of a final paper, and you decide to send it to a few friends to review. You could just share the original document with them and tell them “Have at it! Edit away!” The problem is that, if they are making changes directly to the original document, you could lose some of that awesome original text. A better option would be to create separate copies for each teammate to edit, send them those copies, and then figure out how to layer in their edits later. This is like branching. The original document would be the main branch, and each copy sent to a friend would be a branch off of main. Friends can make as many edits as they want in their branch because you still have the original stable and polished copy. Later, we can merge their changes back into the main branch.\n\n\n\n\n\n\n\nTip\n\n\n\nBranching can get pretty wild in GitHub. You can have branches of branches of branches. I don’t recommend this. A good workflow is to create a branch for making specific changes, merge those changes back into main, delete the branch, and then create a new branch for the next batch of changes.\n\n\n\nQuestion\n\nClick on the Code tab on your repo’s page at GitHub.com. Directly below this tab, you will see a dropdown that is currently labeled “main.” This means that you are in the main branch. Each member of your team should click the down arrow, and create a branch by entering their first name into the textbox that appears, and then clicking “Create branch.”\n\n\n\n\n\nPull\nAs of right now, the branches that were created in the previous step only exist on GitHub.com, they don’t exist yet on your local machine. To get these changes to your local machine, you need to Pull the changes. Remember how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? When we pull changes to our local machines, we are basically saying, check that remote version for changes, and then pull them into the repo on my computer.\n\nQuestion\n\nHead back to RStudio. In the Environment pane, you will notice a tab labeled “Git.” It’s important to note that this tab will only appear in projects that are built from super fancy git folders. This is your RStudio command center for Git and GitHub. When you click on this tab, you will see a few buttons in the navigation bar. To pull changes, you should click the blue downward arrow. Click this button to pull the branches created remotely to your local machine.\n\n\n\n\n\nSwitch Branches\nEven though you pulled the new branches to your local machine, you are still currently working in the main branch. Remember that we always want to keep the main branch stable and polished. This is not where we are going to make edits. Instead, you will make edits in the branch that you just created. Later, we will merge those changes back into the main branch.\n\nQuestion\n\nIn the top right hand corner of the Git tab, you will see a dropdown currently set to “main”. Click the downward arrow, and switch to your branch by selecting the appropriate branch.\n\n\n\n\n\nMake Changes\n\nQuestion\n\nOnce in your branch, open github-practice.Rmd from the files pane. Decide within your group who will be Group member 1, 2, 3, and so on. Each group member should edit this file on their own local machines by adding their name and only their name to the appropriate location in the document (line 5, 7, or 9) based on their assigned number. It’s very important that this be the only section of the document you edit. Save the file by clicking File&gt;Save.\n\n\n\n\nStage\nSometimes we make a changes to a few files, save them, and we’re ready to create a snapshot of our repo (i.e. create a commit) with some of those changes. Remember that creating this snapshot is almost like taking a photo of the repo at this particular moment, allowing us to later go back to that photo to see what the repo looked like in that moment. To let Git know which changes we want to include in that snapshot, we need to stage the files. Staging basically says, “these files are ready to be included in the snapshot.”\n\nQuestion\n\nOnce you save the file you’ll notice in the RStudio Git pane that the file name appears after a blue square labeled “M” (which stands for Modified). This means that the file is ready for staging. Stage the file - indicating that it’s ready for committing - by clicking the checkbox in front of the file name.\n\n\n\n\n\nCommit\nAs we just noted, committing changes basically means taking a snapshot of a repo at a particular moment in time. Commits are given unique hashes - sort of like a unique identifier that enables us to access the snapshot of the repo at a later date. In collaborative projects, it is typically recommended to commit often - after any major changes are made to a file. This ensures that we can eventually go back to look at very specific changes. It’s also important to label commits with descriptive titles so that we can recall what changes within that commit. Descriptive titles should detail what changes were made in the last round of edits.\nTo help put this into context, think back to our photograph metaphor. Let’s say that we are a photographer assigned to document how a baby develops in the first year of its life. If the photographer only took one photograph when the baby was 1 year old, we wouldn’t have a lot of documentation regarding how the baby developed! …so instead, let’s say that the photographer took a snapshot of the baby after every major milestone - their first laugh, their first solid food, their first crawl, their first word. We would have a lot more to go by when trying to understand how the baby developed. Same goes for committing code often.\nNow let’s say the photographer handed the batch of photos to the parents, and said - “look, here’s how your baby developed over time.” The parents might not remember which photograph was taken after which milestone. …but if the photographer were to label each photograph with things like “baby had first laugh,” the parents would be able to easily go back to specific moments in their baby’s development. This is why we want descriptive commit messages. We want to later be able to go back and scan through what changes were made after each commit.\nTo get a sense of what it means to be able to look back on these changes, check out the latest commits that I made to the GitHub repo for our course website. While I’m not going to claim to be the most diligent commit-er, you do get a basic sense of what changes were made to the repo following each commit from these commit messages. I could click through any of these links to see what my repo looked like at this moment in time.\n\n\nQuestion\n\nCommit your changes by clicking the ‘Commit’ button in the Git pane. When you click this button, a new window will open showcasing the changes that have been made to the staged file. You should enter a commit message in the window that appears. Remember that commit messages should be descriptive. In this case, something like “added &lt;your-name&gt;’s name” would work. Click commit. Now a snapshot of this version of the code repo has been taken.\n\n\n\n\nPush\nOnly your local machine knows that a change has been made to the code. Remember again how we said that the super fancy git folder knows that there is a remote version of the repository somewhere that you might want to keep it consistent with? Now we want to do the opposite of pulling changes from GitHub to our local machines. Instead, we want to push the changes on our local machines to GitHub.\n\nQuestion\n\nClick the Green upward arrow in the Git pane to push your changes to GitHub.\n\nOnce all group members have pushed their changes, head back over to GitHub. On the main code page, switch between branches and check out the contents of github-practice.Rmd in each branch. What differences do you notice?\n\n\n\n\nPull Request\nNote that now we have a few versions of our repo in separate branches on GitHub.com, and in each of those versions of the repo, the github-practice.Rmd file looks a little bit different. Now that we’ve made our changes and things are stable and polished, we want to move all of those changes into the main branch. To do this, we are going to issue a Pull Request. This is a request that signals to all of our collaborators that we are ready to move our changes back into the main branch.\n\nQuestion\n\nOn your repo’s page in GitHub.com, click the “Pull Requests” tab, and then click the green “New Pull Request” button. You’re requesting to pull the changes from your personal branch into the main branch. This means that the base branch should be main, and the compare branch should be your personal branch.\n\nYou’ll see a screen where you can compare your branch to the main branch. Click the button to “Create Pull Request,” enter a descriptive title of the changes made, and then click “Create Pull Request” again.\n\n\n\n\nReview Pull Requests\nI recommend that you get in the habit of reviewing your collaborator’s work before merging their changes into the main branch. By creating pull requests, we scaffold an opportunity to review each other’s work before fully integrating the changes.\nNow there should be a pull request for all members of your team. Assign one team member to review one other team member’s code. All team members should have one reviewer.\n\nQuestion\n\nOpen your own pull request in GitHub.com, and in the right sidebar, assign the team member responsible for reviewing your changes as a “Reviewer.”\n\nThen navigate to the pull request you are responsible for reviewing. Click on the “Files Changed” tab. Note that the left side of the screen shows the previous version of the file, and the right side of the screen shows the new version of the file. Lines in red have been deleted, and lines in green have been added.\n\nAfter looking through the changes, click the green button “Review changes.” Leave a note for your collaborator, indicating your evaluation of their changes. If everything looks good, check the radio button for “Approve.” If there are issues, check the radio button for “Request Changes.” Then click the button to “Submit Review.”\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf your reviewer requested changes, you should go back to RStudio, and make sure you are in your own branch. Then make the requested changes, save the file, stage the file, commit the changes, and push again. The changes to your file will be tracked in your pull request. After this, you may move on to the Merge step. See here for further options on dismissing or re-requesting reviews.\n\n\n\n\n\n\nMerge\nOnce all reviewers have approved changes, we are ready to merge those changes into the main branch. Open each pull request. If everything is good and ready to merge, you will see a green checkmark that says “This branch has no conflicts with the base branch.”\n\n(If you get a message that there are conflicts, call myself or one of the Data Assistants over.)\n\nQuestion\n\nClick the button to “Merge Pull Request”. In the comment box that appears, enter the text “closes #”. When you enter this text, you will see a dropdown of issues and pull requests currently in the repo. Issues will have an icon that appears as a circle with a dot in the center.\n\nSelect the issue associated with this pull request, and then click “Confirm Merge.” This will both merge the changes into the main branch and simultaneously close the issue you opened earlier. Finally, click the button to the delete the branch. Once this has been completed for all pull requests, head back over to the “Code” tab at GitHub.com, and check out github-practice.Rmd. What has happened to the file since merging the code? Navigate to the “Issues” tab. What has happened to the issues since confirming the merge?\n\n\n\n\n\n\n\n\n\nBefore moving on to the next section…\n\n\n\nYou’ve now deleted branches at GitHub.com that your local machines don’t know have been deleted. Before moving on to the next step, you should navigate back to RStudio and pull these changes to your local machine by clicking the blue downward arrow. To streamline things in the next section of the lab, we are going to work entirely in the main branch (something that I would otherwise not recommend)."
  },
  {
    "objectID": "labs/lab4.html#error-resolution",
    "href": "labs/lab4.html#error-resolution",
    "title": "Lab 4: Git and GitHub",
    "section": "Error Resolution",
    "text": "Error Resolution\nThe workflow presented above seems to work all fine and dandy. …but there are a number of factors that can impede this seamless workflow. In this final section of the lab, we will go over three kinds of errors that you might come across in the workflow above, and talk about how you would resolve them. I can almost guarantee that you will deal with some of these issues when working on your group projects, so I would encourage you to keep this lab handy when engaging in project work.\n\nPush error\nA push error occurs when we make changes to files on our local machines, and go to commit and push them to GitHub.com, but other changes had already been made to the file at GitHub.com that were not yet pulled into our local environments. We get an error because our local repo is inconsistent with the remote repo. To fix this error, we need to pull changes to our local machine, and try committing and pushing again. Let’s replicate this error:\n\nQuestion\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file README.md. Click the pencil icon to edit the file. Replace the text: ADD NAME 1 HERE ADD NAME 2 HERE, and so on with your names. Scroll to the bottom of the page and commit changes noting in the message that your names were added.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. On line 40 change the ncol() function to dim(). Save the file. Stage and commit your changes. Click the green upward arrow to push your changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\n\nAn easy way to avoid a push error is to always click the blue downward arrow to pull remote changes before starting to edit files on your local machine.\n\n\n\n\nPull error\nA pull error occurs when changes have already been made to the same location (the same line number) in both a remote file and a local file, and then we try to pull the changes from the remote repo to our local machines. As far as Git can tell, there are two options for what this line is supposed to look like, and it can’t tell which to prioritize. So Git recommends that, as a first step, we commit the changes that we made locally. It’s basically saying, let’s take a snapshot of your local repo as it looks right now, so that later we can figure out what to do about this conflict.\nIf this seems confusing imagine this: let’s say you write a paper, and you share it with one of your classmates to review. The classmate reads through it, makes suggested changes to the opening sentence, and sends it back you. …but, while your classmate was reviewing the paper, you were getting antsy about the paper deadline and started making your own edits to the paper, including edits to the opening sentence. Now you’re trying to incorporate the changes from your classmate’s review, and you’re not sure what to do about that opening sentence. As a first step, you have two options you can scrap your recent changes (maybe your classmate’s suggestions were better!) or you can save a separate copy of the file with your recent changes and figure out later how to resolve the differences. That’s exactly what we are going to do here:\nTo fix this error, you should stage and commit your local changes and then try pulling again. Let’s replicate this error:\n\nQuestion\n\nOne of your partners should navigate to the GitHub.com repository. Click on the file github-practice.Rmd. Click the pencil icon to edit the file. Replace the code on line 47 with the following: colnames(pioneer_valley_2013).\nScroll to the bottom of the page and commit changes noting in the message how you updated the function.\nOther partner: Return to RStudio. Do not pull changes yet. Open github-practice.Rmd. Replace the code on line 47 with the following: ncol(pioneer_valley_2013)\nSave the file. Click the blue downward button to Pull changes. You should get an error that looks like this.\n\nFollow the steps above to resolve the error.\n\n\n\n\nMerge conflict\nSo now we have these two snapshots of github-practice.Rmd, and they are in conflict with one another. If we try to push our changes back to GitHub.com, Git is not going to know what to do. Should the file at GitHub.com look like the version currently at GitHub.com, should it look like the snapshot that we just commit to our local machines, or should it look like something else entirely?\nLet’s return to the example of trying to incorporate a peer’s edits to a paper that you have recently made changes to. We have to figure out what to do about that opening sentence. Do we want our version, their version, or some combination of the two? This is what it is like to fix a merge conflict.\nTo fix this error, open the file with conflicts and edit the lines with conflict.\n\nQuestion\n\nOne of your partners should try to pull changes by clicking the blue downward arrow in RStudio. You will get an error that looks like this:\n{fig-alt=“This is the window that we see when we get a merge conflict. It says: CONFLICT (content): Merge conflict in README.md}\nTo fix this one your partners should open the file with the conflict. In this case it will be github-practice.Rmd. Scroll to the section of the file with the conflict. It will now look something like this:\n    &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n    ncol(pioneer_valley_2013)\n\n    =======\n\n    colnames(pioneer_valley_2013)\n\n    &gt;&gt;&gt;&gt;&gt;&gt;&gt; ee175895783b64e0e1f696d9456be4c4c7c3f3bf\nThe code following HEAD represents the recent changes you made on your local machine, and the code right before the long string of characters represents the changes that were made in an earlier commit (the long string of characters is the commit hash). Decide what that line should look like and delete all other content. This means you must delete “&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD”, “=======”, and “&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;long-hash&gt;”, and you likely should delete at least one other line. Save the file, stage the file by clicking the checkbox next to the file in the Git page, and then commit your changes, and push them to GitHub.com.\n\n\n\n\n\n\n\n\n\nAvoiding Merge Conflicts\n\n\n\nYou may have noticed that the most frustrating merge conflicts tend to emerge when we have two people working on the same line of a repo’s file. The most effective way to avoid merge conflicts is to ensure that collaborators are working on different documents or different lines in a file. One way you might do this when starting to work on your group project is to open a file that you all plan to work on and having one of your team mates section off space of that file for different people to work. It might look something like this:\n\nOnce this change has been made, that group mate should stage, commit, and push the file to GitHub, and all other group mates should pull the change to their local machines.\n\n\n\n\n\n\nPath of Least Resistance\nI have been working with GitHub for years, and even to this day, I run into instances where things become so inconsistent between my local machine and the repo at GitHub.com that the fastest way to fix things is just to save local copies of the files that I’ve changed to somewhere else on my machine, delete the super fancy Git folder from its current location, and then re-clone the most up-to-date remote version to my local machine. Then I can figure out how I want to edit the most up-to-date version with my changes. This comic from XKCD captures this widely acknowledged solution beautifully:\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIncreasingly, when data science researchers publish a paper in a journal, they are making the code they used to reach certain results freely available on GitHub.com for other researchers and the public to review. This is in part a response to the reproducibility crisis that you learned about in SDS 100. What do you see as the social benefits to making the code behind a data science finding publicly available online? What might be some of the social consequences of making this code freely available? How might we mitigate these consequences? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab7.html",
    "href": "labs/lab7.html",
    "title": "Lab 7: Tidying Data",
    "section": "",
    "text": "In this lab, we will create a few data visualizations documenting point-in-time counts of homelessness in the United States. Specifically, we are going visualize data collected in 2020 through various Continuums of Care (CoCs) programs. In order to produce these data visualizations, you will need to join homelessness data with census population data and develop and execute a plan for how to wrangle the dataset into a “tidy” format.\n\n\n\nRecognize the differences between tidy and non-tidy data\nPivot datasets both longer and wider\nSeparate and unite columns\nConsider the ethical implications of analyzing homeless counts"
  },
  {
    "objectID": "labs/lab7.html#introduction",
    "href": "labs/lab7.html#introduction",
    "title": "Lab 7: Tidying Data",
    "section": "",
    "text": "In this lab, we will create a few data visualizations documenting point-in-time counts of homelessness in the United States. Specifically, we are going visualize data collected in 2020 through various Continuums of Care (CoCs) programs. In order to produce these data visualizations, you will need to join homelessness data with census population data and develop and execute a plan for how to wrangle the dataset into a “tidy” format.\n\n\n\nRecognize the differences between tidy and non-tidy data\nPivot datasets both longer and wider\nSeparate and unite columns\nConsider the ethical implications of analyzing homeless counts"
  },
  {
    "objectID": "labs/lab7.html#review-of-key-terms",
    "href": "labs/lab7.html#review-of-key-terms",
    "title": "Lab 7: Tidying Data",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Tidy R Cheatsheet when completing this lab.\n\n\n\nTidy data\n\nA rectangular data table in which every row is an observation and every column is a variable describing something about that observation\n\nPivoting\n\nRotating data columns so that they are presented as rows or rotating data rows so that they are presented as columns"
  },
  {
    "objectID": "labs/lab7.html#huds-point-in-time-counts",
    "href": "labs/lab7.html#huds-point-in-time-counts",
    "title": "Lab 7: Tidying Data",
    "section": "HUD’s Point-in Time Counts",
    "text": "HUD’s Point-in Time Counts\nThe U.S. Department of Housing and Urban Development is responsible for monitoring and addressing housing affordability and homelessness throughout the country. One initiative that they oversee towards this end is the Continuum of Care Program. Continuums of Care (CoCs) are local planning organizations responsible for allocating resources and coordinating services to address homelessness in the United States. Every state has a number of CoCs that report to the Department of Housing and Urban Development.\nEvery year, on a single night in the last 10 days of January, CoCs required to conduct a “point-in-time” count of sheltered and unsheltered homeless individuals. To generate the sheltered point-in-time count, CoCs coordinate with transitional housing centers or emergency shelters to record the number of people housed in that location on the selected night. There are a few different approaches to generating an unsheltered point-in-time count:\n\n“Night of” street count: Volunteers are sent out to canvass either the entire geography of a random sample of areas in a CoC. While canvassing, they are expected to record the number of people they see currently residing in spaces not designed for sleeping accommodations.\nService-based count: In the 7 days following the night of the designated PIT count, volunteers are dispatched to food kitchens, shelters, libraries and other services identified as spaces that unhoused individuals frequent. There, they are expected to survey individuals to determine if they were sheltered on the night of the count.\n\nFollowing the night of the count, the collected data points are shipped off to HUD where they are aggregated into the dataset we will be working with today.\nThere are a number of limitations to this approach of estimating homelessness, which I encourage you to consider throughout this analysis and when responding to our Ethical Considerations question."
  },
  {
    "objectID": "labs/lab7.html#setting-up-your-environment",
    "href": "labs/lab7.html#setting-up-your-environment",
    "title": "Lab 7: Tidying Data",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\npit &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/pit_2015_2020.csv\") \n\n#Note for future reference how adding the following could remove unreliable counts:\n# mutate_at(vars(2:97), funs(case_when(. &lt; 20 ~ as.numeric(NA), TRUE ~ .)))\n\ngender &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/gender_state_2015_2020.csv\")\nrace &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/main/website/data/race_state_2015_2020.csv\")"
  },
  {
    "objectID": "labs/lab7.html#our-goal",
    "href": "labs/lab7.html#our-goal",
    "title": "Lab 7: Tidying Data",
    "section": "Our Goal",
    "text": "Our Goal\nToday’s goal is to produce just two plots:\n\nA timeseries of rates of both sheltered and unsheltered homelessness by gender for a given state.\nA timeseries of rates of both sheltered and unsheltered homelessness by race for a given state.\n\nThe first plot will look like this:\n\n…and the second plot will look like this.\n\nTo be able to produce these plots, we are going to have to do considerable amount of data wrangling and cleaning. This lab will walk you through those steps."
  },
  {
    "objectID": "labs/lab7.html#data-cleaning",
    "href": "labs/lab7.html#data-cleaning",
    "title": "Lab 7: Tidying Data",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nFirst, let’s consider each of our data frames. You have pit, which documents homelessness counts from 2015 to 2020 in each state along a number of categories (such as gender, race, sheltered vs. unsheltered, etc.). Note how in this data frame each row is a state, and each variable is a count associated with a particular category.\n\nhead(pit)\n\n\n  \n\n\n\nThis is an example of untidy data. The reason is that there is data we want to visualize (such as race, gender, year, and sheltered vs. unsheltered) that are stored in our column headers, not in cells of the data frame. To be able to visualize this data by race, by gender, or by year we need to pivot our data frames so that the values stored in column headers instead get restored in data cells.\n\nQuestion\n\nWrite code to pivot longer columns 2 through 117 (i.e. all columns except the first state column) in pit. Store the names in column called “Measure” and store the values in a column called “Value.” Store the resulting data frame in in pit_pivoted.\n\n\n# Uncomment below and write code to pivot dataset. Store the results in pit_pivoted\n\n# pit_pivoted &lt;- pit |&gt; pivot_longer(_____)\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_pivoted)\n\n\n  \n\n\n\nNow you’ll notice that that we have some more cleaning to do because many separate variables are all stored in the Measure column. We have: the year, the demographic, whether the count is for Sheltered or Unsheltered individuals all stored in the column. We need to separate all of these distinct variables into different columns. We’re going to do this in a few steps.\n\n\n\nQuestion\n\nWrite code to separate Measure into two columns. One column should be called Shel_Unshel, and the second should be called Demographic. Note what symbol separates these two pieces of data: space dash space (i.e. ” - “). You will need to specify this in the sep argument for separate(). Store the resulting data frame in in pit_separate_1.\n\n\n# Uncomment below and complete the code to separate the Measure column. Store the results in pit_separate_1\n\n# pit_separate_1 &lt;- pit_pivoted |&gt; separate(_____)\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_separate_1)\n\n\n  \n\n\n\nWe still have some cleaning to do because we have demographic data and year data are stored in the same column.\n\n\n\nQuestion\n\nWrite code to separate Demographic into two columns. One column should be called Demographic, and the second should be called Year. Note what symbol separates these two pieces of data: comma space (i.e. “,”). You will need to specify this in the sep argument for separate(). Store the resulting data frame in in pit_separate_2.\n\n\n# Uncomment below and complete the code to separate the Demographic column. Store the results in pit_separate_2\n\n# pit_separate_2 &lt;- pit_separate_1 |&gt; separate(_____)\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_separate_2)\n\n\n  \n\n\n\nAs a final cleaning step, let’s remove some unnecessary characters from the Shel_Unshel column. Specifically, let’s remove the string “Total” since we already know that these represent Total counts for these categories.\n\n\n\nQuestion\n\nRemove the string “Total” from the Shel_Unshel column. To do this you should use the str_replace() function. Replace the string “Total” with the empty string (i.e. ““) to remove these characters. Store the results in pit_str_cleaned.\n\n\n# Uncomment below and complete the code to replace the string \"Total \" in Shel_Unshel with an empty string. Store the results in pit_str_cleaned\n\n# pit_str_cleaned &lt;- pit_separate_2 |&gt; mutate(Shel_Unshel = _______)\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_str_cleaned)\n\n\n  \n\n\n\nThe next issue is that there are multiple kinds of demographics in the demographic column. Specifically, we have both race represented in that column and gender represented in that column. Remember the rules of tidy data, according to Wickham (2014):\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nThis issue violates the third rule. In one table, we have two observational units - a count of unhoused individuals by race and a count of unhoused individuals by gender. To clean this up, we need to separate this into different tables.\n\n\n\nQuestion\n\nCreate two new data frames - one for pit_by_gender and one for pit_by_race. To do this you want to extract the rows with values in Demographic %in% the following vector of values: c(\"Female\", \"Male\", \"Transgender\"), and store the result in pit_by_gender. Then you want to extract the rows with values in Demographic %in% the following vector of values:\nc(\"Black or African American\", \n  \"Asian\", \n  \"American Indian or Alaska Native\", \n  \"Native Hawaiian or Other Pacific Islander\", \n  \"White\", \n  \"Multiple Races\")\nand store the result in pit_by_race.\n\n\n# Uncomment below to create two new tables.\n\n# pit_by_gender &lt;- pit_str_cleaned |&gt;\n# pit_by_race &lt;- pit_str_cleaned |&gt;\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(pit_by_gender)\n\n\n  \n\n\nhead(pit_by_race)\n\n\n  \n\n\n\nNow we have two significantly cleaned data tables. Let’s go ahead and remove some of the data frames we no longer need from our environmennt.\n\nrm(pit, pit_pivoted, pit_separate_1, pit_separate_2, pit_str_cleaned)\n\nWith the final data, we could visualize counts of homelessness per state using each of these tables. Check out the plot below to see an example of what this might look like.\n\n\n\n\n\n\n\n\n\nThe problem now is that we don’t know if higher counts are a result of greater homelessness for that sub-group or a result of there being higher populations of that sub-group in each state. For instance, in the plot above, is the count of homelessness higher for unsheltered white individuals because white individuals are more likely to be homeless or because there is a higher population of white individuals in Florida? We ultimately want to consider the rates of homelessness per x number of people of that demographic in that state. To do that, we’re going to need to join this dataset with some census data documenting population.\nI’ve supplied you with two census tables:\n\ngender: documents population estimates and margin of error (moe) for each gender in each state from 2015-2020\n\n\nhead(gender)\n\n\n  \n\n\n\n\nrace: documents population estimates and margin of error (moe) for each race in each state from 2015-2020\n\n\nhead(race)\n\n\n  \n\n\n\nNote that in these two data frames - just like above - we have values stored in our column headers, and we need to pivot our data longer and then clean it up. We’re going to complete this in three parts.\n\n\n\nQuestion\n\nCreate two new tidy data frames - gender_final and race_final. For each, you should do this in 3 steps:\nStep 1: Pivot the columns estimate_2015:moe_2020 longer, storing the column names in “Measure” and the values in “Values”. Step 2: Separate the values in Measure into two columns: “Measure”, “Year”. Note that these are separated by an underscore (“_“). Step 3: Pivot the Measure column wider, taking the values from the Values column.\n\n\n# Fill in the blanks to clean up these data frames. \n\n# Step 1\n#gender_pivoted_longer &lt;- gender |&gt; pivot_longer(_____)\n\n# Step 2\n#gender_separated &lt;- gender_pivoted |&gt; separate(_____)\n\n# Step 3\n#gender_final &lt;- gender_separated |&gt; pivot_wider(_____)\n\n# Step 1\n#race_pivoted_longer &lt;- race |&gt; pivot_longer(_____)\n\n# Step 2\n#race_separated &lt;- race_pivoted |&gt; separate(_____)\n\n# Step 3\n#race_final &lt;- race_separated |&gt; pivot_wider(_____)\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frame should look like this.\n\nhead(gender_final)\n\n\n  \n\n\nhead(gender_final)\n\n\n  \n\n\n\nNow that we have cleaned up census data, let’s go ahead and remove some data frames that we no longer need in our environments\n\nrm(gender, gender_pivoted_longer, gender_separated, race, race_pivoted_longer, race_separated)\n\nOur next step is to join the census data with our point-in-time count data tables. Let’s just compare pit_by_gender to gender_final to discern the join key.\n\nhead(pit_by_gender)\n\n\n  \n\n\nhead(gender_final)\n\n\n  \n\n\n\nNotice that here, we need to join on three variables: the state, the year, and the demographic. …but you might have noticed a problem. In our census data table, states are written out, and in our point-in-time data table, states are abbreviated. The values need to match for the join to work, so we will need to create a new column with the full state name in our point-in-time count data table. I’ve written the code to do this below. You should run that code before moving on to the next step.\n\npit_by_gender &lt;- \n  pit_by_gender |&gt;\n  mutate(StateName = state.name[match(State, state.abb)])\n  \npit_by_race &lt;- \n  pit_by_race |&gt;\n  mutate(StateName = state.name[match(State, state.abb)])\n\nNow our data frames are formatted in such a way that we can join pit_by_gender to gender_final and pit_by_race to race_final.\n\n\n\nQuestion\n\nJoin pit_by_gender on the left to gender_final on the right and store the results in pit_gender. Join pit_by_race on the left to race_final on the right and store the results in pit_race. In both case, you will be joining by three variables: State, Demographic, and Year. We can set the left key variables to the right key variables by setting the by argument to this in the join: c(\"StateName\" = \"NAME\", \"Demographic\" = \"variable\", \"Year\" = \"Year\")\n\n\n# Uncomment below and write code to join pit_by_gender to gender_final and pit_by_race to race_final. Store the results in pit_gender and pit_race respectively.\n\n# pit_gender &lt;- pit_by_gender |&gt;\n# pit_race &lt;- pit_by_race |&gt;\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frames should look like this.\n\nhead(pit_gender)\n\n\n  \n\n\nhead(pit_race)\n\n\n  \n\n\n\nOne final step before we can create our plots! We need to create a new column that calculates the rate of homelessness for this sub-group per 10,000 population of that sub-group in that state and year. You have all of the pieces you need to do this now.\n\n\n\nQuestion\n\nUse a data wrangling verb to create a new column in both pit_gender and pit_race that calculates the rate of homelessness per 10,000 population. Set that column name to homeless_rate. Hint: When creating that column, you’ll need to divide the homeless count by the population estimate and then multiply by 10000. Store the resulting data frames in\n\n\n# Uncomment below and write code to create a new column for homelessness_rate in both pit_gender and pit_race\n\n# pit_gender_rates &lt;- pit_gender |&gt;\n# pit_race_rates &lt;- pit_race |&gt;\n\n\n\nIf you’ve done this correctly the first six rows of the resulting data frames should look like this.\n\nhead(pit_gender_rates)\n\n\n  \n\n\nhead(pit_race_rates)\n\n\n  \n\n\n\nIf you’ve done everything correctly, you should be able to run the following code to generate the plots presented at the beginning of the lab. Feel free to swap out the State filter for different states to see how the rates compare across the US.\n\noptions(scipen=999)\n\npit_gender_rates |&gt;\n  filter(State == \"FL\") |&gt;\n  ggplot(aes(x = Year, \n             y = homeless_rate, \n             col = Demographic, \n             group = Demographic)) +\n  geom_line() +\n  facet_wrap(vars(Shel_Unshel)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Point in Time Homeless Rates in FL, 2015-2020\",\n       x = \"Year\", \n       y = \"Homeless per 10,000 Population\",\n       col = \"Gender\")\n\n\n\n\n\n\n\npit_race_rates |&gt;\n  filter(State == \"FL\") |&gt;\n  ggplot(aes(x = Year, \n             y = homeless_rate, \n             col = Demographic, \n             group = Demographic)) +\n  geom_line() +\n  facet_wrap(vars(Shel_Unshel)) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3, byrow = TRUE)) +\n  labs(title = \"Point in Time Homeless Rates in FL, 2015-2020\",\n       x = \"Year\", \n       y = \"Homeless per 10,000 Population\",\n       col = \"Race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nWhile this is one of the primary datasets used to direct resources for homelessness in the U.S., there are a number of reasons unhoused individual may go uncounted through point-in-time counts:\n\nOnly individuals that are visible to enumerators get counted. However, other sources (such as surveys in public schools) have shown that the majority of unhoused individuals are not on the streets or in shelters, but instead are temporarily residing with family/friends or are in staying in motels.\nAs living on the streets is increasingly criminalized in cities across the U.S., there are reasons why unhoused individuals may try to avoid being seen on a given night.\nOnly recently have counting protocols started to include separate categories for Transgender and Gender Non-Conforming individuals. When counting PIT data against census population data, however, these sub-groups won’t appear because the census only collects Sex as “Male” and “Female”.\nDifferent CoCs use different methods to generate PIT counts.\n\nFor more information see e National Law Center on Homelessness & Poverty (2017).\nTaking into consideration all of these factors, how should we, as data scientists, think about and present this data? Is there value in PIT count data? How should we go about communicating its shortcomings? How might we go about collecting this data in different ways? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Lab 6: Joining Datasets",
    "section": "",
    "text": "In terms of data analysis, this lab has one goal: to determine the number of industrial facilities that are currently in violation of both the Clean Air Act and the Clean Water Act in California. To achieve this goal, we’re going to have to do some data wrangling and join together some datasets published by the EPA. We’re going to practice applying different types of joins to this data and consider what we learn with each.\n\n\n\nIdentify join keys\nDiscern the differences between types of joins\nDiscern what happens when we perform many-to-many joins\nApply filtering joins to identify missing data\nPractice data wrangling"
  },
  {
    "objectID": "labs/lab6.html#introduction",
    "href": "labs/lab6.html#introduction",
    "title": "Lab 6: Joining Datasets",
    "section": "",
    "text": "In terms of data analysis, this lab has one goal: to determine the number of industrial facilities that are currently in violation of both the Clean Air Act and the Clean Water Act in California. To achieve this goal, we’re going to have to do some data wrangling and join together some datasets published by the EPA. We’re going to practice applying different types of joins to this data and consider what we learn with each.\n\n\n\nIdentify join keys\nDiscern the differences between types of joins\nDiscern what happens when we perform many-to-many joins\nApply filtering joins to identify missing data\nPractice data wrangling"
  },
  {
    "objectID": "labs/lab6.html#review-of-key-terms",
    "href": "labs/lab6.html#review-of-key-terms",
    "title": "Lab 6: Joining Datasets",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\n\nJoin Key\n\nA variable shared across two datasets that identifies the observations that will be merged"
  },
  {
    "objectID": "labs/lab6.html#epas-echo",
    "href": "labs/lab6.html#epas-echo",
    "title": "Lab 6: Joining Datasets",
    "section": "EPA’s ECHO",
    "text": "EPA’s ECHO\nThe U.S. Environmental Protection Agency (EPA) is responsible for monitoring and regulating over 800,000 industrial facilities in the United States. This involves regularly inspecting facilities for their compliance with different environmental laws, issuing notices and taking enforcement actions when facilities are out of compliance, and issuing penalties for failures to address violations. A record of all of this activity is maintained online in a database known as ECHO - Enforcement and Compliance History Online. Specifically, ECHO maintains information about enforcement and compliance actions taken in relation to the Clean Air Act, the Clean Water Act, the Safe Drinking Water Act, and the Resource Conservation and Recovery Act.\nThere is so much information available to us via ECHO, and there is even an R package that makes it really simple to access the data in ECHO. However, because ECHO documents compliance with a number of different environmental laws, when we pull data from ECHO about regulated facilities, it is usually organized into separate tables for each law. Today we are going to pull a dataset documenting facilities’ compliance with the Clean Air Act and then pull a second dataset documenting facilities’ compliance with the Clean Water Act.\nThe Clean Air Act was first enacted in 1963 to regulate emissions from both stationary and mobile sources of air pollution. The Clean Water Act was enacted in 1972 regulate the polluting of U.S. waterways. When a facility violates one of these Acts, it typically means that they have released an excess amount of pollutants, neglected to implement the proper control technologies or standards, or failed to submit reports."
  },
  {
    "objectID": "labs/lab6.html#setting-up-your-environment",
    "href": "labs/lab6.html#setting-up-your-environment",
    "title": "Lab 6: Joining Datasets",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the echor package in your Console.\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\nlibrary(echor)\n\necho_air_ca &lt;- \n  echoAirGetFacilityInfo(p_st = \"CA\", \n                         qcolumns = \"1,4,5,8,107,108,109,110\") |&gt;\n  select(-SourceID)\n\necho_water_ca &lt;- \n  echoWaterGetFacilityInfo(p_st = \"CA\", \n                           qcolumns = \"1,4,5,9,203,204,205,206\") |&gt;\n  select(-SourceID)\n\n\nRun the code below to load the data dictionary for echo_air_ca into your environment.\n\n\necho_air_dd &lt;- echoAirGetMeta() |&gt;\n  filter(ColumnID %in% c(1,4,5,8,107,108,109,110)) |&gt;\n  select(ColumnID, ObjectName, Description)\n\necho_water_dd &lt;- echoWaterGetMeta() |&gt;\n  filter(ColumnID %in% c(1,4,5,9,203,204,205,206)) |&gt;\n  select(ColumnID, ObjectName, Description)\n\n\nOpen both echo_air_dd and echo_water_dd to review the data dictionaries for these datasets."
  },
  {
    "objectID": "labs/lab6.html#data-analysis",
    "href": "labs/lab6.html#data-analysis",
    "title": "Lab 6: Joining Datasets",
    "section": "Data Analysis",
    "text": "Data Analysis\n\nJoin Keys\nWhen joining data frames, we need to determine a join key. This will be a variable that exists in both data frames to uniquely identify the data frames’ units of observation. This key needs to be shared across the two data frames in order to be able to join the data frames together. This is because when joining data frames, R will check the value in that variable for one data frame and look for the corresponding value in the key variable for the other data frame in order to determine which units of observation match across the data frames.\nFor instance, let’s say we have two data frames documenting the same group of students - one documents their current courses, and another documents their living arrangements. For both data frames, the student ID might serve as a join key. This ID uniquely identifies each student, and we expect that that key will be the same in both data frames (i.e. the 99- that represents you in one data frame will be the same as the 99- number that represents you in another data frame.) Because that number matches across the two data frames, we can use that variable to determine which rows (i.e. students) in the first data frame are associated with which rows in the second data frame.\n\nQuestion\n\nReference the data dictionaries for these two data frames to determine the names of the variables that we will join on. Remember that these should be variables that we can use to uniquely identify each unit of observation across the data frames. Write code below to determine the number of unique values for the variables you identify in each of these data frames. (Hint: Remember how we counted unique values in a variable in lab 1?)\n\n\n# Write code here for echo_air_ca\n# Write code here for echo_water_ca\n\n\n\nIf you’ve done this correctly, you’ll learn that there are about 2700 unique values echo_air_ca’s key variable. You’ll also learn that there are about 29000 unique values in echo_water_ca’s key variable.\n…but if you look in your environment, you will notice that the number of unique values does not match the number of rows in echo_air_ca, and the number of rows in echo_water_ca. This means that the key variable must repeat, and that certain facilities are represented more than once in each of these data frames.\nThis happens because even the EPA defines a facility in different ways depending on what laws it is regulated by. What counts as one facility in the Clean Air Act may count as two or three facilities in the Clean Water Act, but there is only one ID to document the relationships between them. This means that the same ID may show up a few times for different parts of the same facility. Check out some of the duplicating rows via my code below. Notice how multiple facilities - sometimes with entirely different names can be associated with the same RegistryID?\n\necho_air_ca |&gt;\n  group_by(RegistryID) |&gt;\n  filter(n() &gt; 1) |&gt;\n  arrange(RegistryID) |&gt;\n  head()\n\n\n  \n\n\n\nLater we will talk about what happens when we join two data frames that both have repeating keys, but for now, we’re going aggregate both of these data frames so that each Registry ID only appears once. Basically, we are going to write code to say: if any of the rows pertaining to this Registry ID document certain kinds of violations, set the violation flag for that facility to 1.\n\n\n\nQuestion\n\nIn the code below, for each of the data frames, you should group the data by RegistryID and then summarize by returning the max() value in CurrSvFlag, ViolFlag, CurrVioFlag, Insp5yrFlag for each group. Store the results in echo_air_ca_agg and echo_water_ca_agg respectively. Note how many rows in are in echo_air_ca_agg and echo_water_ca_agg. It should match the number of unique values in RegistryID that you calculated above.\n\n\necho_air_ca_agg &lt;-\n  echo_air_ca |&gt;\n  ______ |&gt;             #group by RegistryID\n  _____(                #Summarize the following:\n    CurrSvFlag = _____, #Calculate max value in CurrSvFlag\n    ViolFlag = _____,   #Calculate max value in ViolFlag\n    CurrVioFlag = _____,#Calculate max value in CurrVioFlag\n    Insp5yrFlag = _____ #Calculate max value in Insp5yrFlag\n  )\n\necho_water_ca_agg &lt;- #Copy and adjust code above to create echo_water_ca_agg\n\n\n\n\nJoins\nSo now we have two data frames with unique registry IDs and variables indicating whether the facility has a current significant violation, whether it has had a violation in the past three years, whether it has a current violation, and whether it has been inspected in the past 5 years.\nNow we want to join these two data frames together, so that we can check which facilities have violations to both the Clean Air Act and the Clean Water Act.\n\nQuestion\n\nWrite code below to perform four kinds of joins - a left join, a right join, and inner join, and a full join. echo_water_ca_agg should be in the first position, and echo_air_ca_agg should be joined onto it. I’ve started that for you in the commented code below.\n\n\n#joined_left &lt;- echo_water_ca_agg |&gt; \n\n#joined_right &lt;- echo_water_ca_agg |&gt; \n\n#joined_inner &lt;- echo_water_ca_agg |&gt; \n\n#joined_full &lt;- echo_water_ca_agg |&gt; \n\n\n\nCheck out the first six rows of joined_full below and note what happens when a RegistryID is present in one data frame but not in the other. We see data values in the columns associated with the data frame where the RegistryID was present, and NA values in the columns associated with the data frame where the RegistryID was not present. This is how we come to have so many extra rows in joined_full.\n\njoined_full |&gt; head()\n\n\n  \n\n\n\nWith joined_inner, any rows associated with a RegistryID that doesn’t appear in both data frames get dropped. Check out the number of rows in joined_inner.\n\nnrow(joined_inner)\n\n[1] 766\n\n\nThis represents the number of Registry IDs that were present in both echo_water_ca_agg and echo_air_ca_agg.\n\n\n\nQuestion\n\nThe three statements below are incorrect. Correct my statements below about the remaining joins.\n\n\npaste(nrow(joined_full), \"represents the number of Registry IDs present in echo_air_ca_agg\")\n\npaste(nrow(joined_left), \"represents the number of Registry IDs present in either echo_air_ca_agg or echo_water_ca_agg\")\n\npaste(nrow(joined_right), \"represents the number of Registry IDs present in echo_water_ca_agg\")\n\n\n\nFrom here on out, since we are only interested in the facilities with violations to both Acts, we are going to focus on joined_inner. Let’s remove the rest of the joined data frames from our environment.\n\nrm(joined_full, joined_left, joined_right)\n\nYou might note at this point that it can be difficult to tell which columns are associated with which environmental laws. This is because originally in both data frames, the variables we are most interested in (CurrSvFlag, ViolFlag, CurrVioFlag, Insp5yrFlag) shared the same names. When we joined these two data frames together, R opted to distinguish between them by tacking a .x onto the variable names for the data frame in the first position in the join and a .y onto the variable names for the data frame in the second position in the join. Let’s give each of these variables more meaningful names. To do so, we can use the rename() function, which is included in the dplyr package in the tidyverse. When piped to a dataframe, the formula for rename() is simple: rename(new_name = old_name)\n\n\n\nQuestion\n\nIn my code below rename the remaining columns to differentiate between the column names from the two original data frames.\n\n\njoined_inner_renamed &lt;-\n  joined_inner |&gt;\n  rename(\n    CurrSvFlag_water = CurrSvFlag.x,\n    ViolFlag_water = ViolFlag.x \n    #Rename remaining columns here. Be sure to separate by comma. There are six more to go!\n  )\n\n\n\nQuestion\n\nNow that we have a cleaned up data frame, use a data wrangling verb to subset the data frame to the rows where a RegistryID has a current violation to both the Clean Water Act and Clean Air Act. Repeat these steps to determine which RegistryIDs have a significant violation to both Acts. I recommend running this on joined_inner_renamed.\nOnce you’ve run these codes, open both echo_air_ca and echo_water_ca by clicking on the data frames in your environment. Search for a few of the RegistryIDs that appeared in your analysis to identify the names and locations of the facilities.\n\n\n# Write wrangling code here for current violations. \n\n# Write wrangling code here for significant violations. \n\n\n\n\nMany-to-Many Joins\nWe don’t know much about these facilities because we lost a lot of critical information (e.g. the facility’s name and location) when aggregating our data by RegistryID above. At the time, we couldn’t aggregate by name because certain facilities sharing the same RegistryID had different names! Aggregating by name would have meant that we’d still have repeating RegistryIds in the data frame - one for each different version of the facility’s name.\nThe truth is though that we didn’t technically have to ensure that the RegistryID didn’t repeat. We can still join data frames in instances where the join key repeats in both data frames. This is called performing a many-to-many join because we are joining many of the same key in one data frame to many of the same key in another data frame. It’s important to pay attention to what happens when we make this join. Let’s look at an example: the facility with Registry ID 110001181186.\n\necho_air_ca |&gt;\n  filter(RegistryID == 110001181186) \n\n\n  \n\n\necho_water_ca |&gt;\n  filter(RegistryID == 110001181186)\n\n\n  \n\n\n\nIn echo_air_ca, this RegistryID appears three times in the data frame, and in echo_water_ca, this RegistryID appears twice in the data frame. In each instance, the facility has a different name. What happens to facility 110001181186 when we perform a many-to-many join?\n\necho_water_ca |&gt; \n  inner_join(echo_air_ca, by = \"RegistryID\") |&gt; \n  filter(RegistryID == 110001181186) |&gt;\n  select(CWPName, AIRName, RegistryID)\n\n\n  \n\n\n\nThis inner join created six rows for facility 110001181186. This is because R matched each of the three rows for facility 110001181186 in echo_air_ca with the two rows for facility 110001181186 in echo_water_ca (and 3 rows * 2 rows = 6 rows).\nThis is one of the reasons why it is important to understand the context of datasets and their units of observation. Here we’ve joined these rows together as if they represented matching facilities. …but just by looking at this output we know that PLATFORM A is going to be physically different than DOS CUADRAS/SOUTH COUNTY/PLATFORM B, and PLATFORM B is going to be physically different than DOS CUADRAS/SOUTH COUNTY/PLATFORM C. The only thing that they all share is that they are sub-parts of the facility with the RegistryID 110001181186. Ultimately the units of observation don’t match across these two data frames because Clean Air Act and Clean Water Act delineate facilities differently. By aggregating to RegistryID above - something shared across the two Acts - we standardized the unit of observation, which made it possible to perform a more meaningful join.\n\nQuestion\n\nUsing my code as an example, determine how many times facility 110000483619 appears in echo_air_ca and how many times it appears in echo_water_ca. How many times would this facility show up if we were to perform a many-to-many inner join for these two data frames? Write your response as a comment in the code chunk.\n\n\n#Write code here for echo_air_ca!\n\n#Write code here for echo_water_ca!\n\n#Write comment here!\n\n\n\n\nFiltering Joins\nSometimes we want to know which observations that appear in one data frame don’t appear in another data frame. This might tell us where we have missing data. In this case, it will tell us which facilities are regulated by one environmental law and not the other.\n\nQuestion\n\nWrite code below to perform two anti_join()s. The first should tell me which facilities are regulated by the Clean Air Act and not the Clean Water Act, and the second should tell me which facilities are regulated by the Clean Water Act and not the Clean Air Act.\n\n\n#not_clean_water &lt;- Write code here!\n\n#not_clean_air &lt;- Write code here!\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn March 2020, the Environmental Protection Agency published a memo that permitted industrial facilities impacted by the Covid-19 pandemic to temporarily suspend mandated pollution monitoring. Research published by the Environmental Data Governance Initiative (EDGI) (Nost et al. 2020) has indicated that, during this time, reported violations to the Clean Air Act and the Clean Water Act dropped considerably. However, EDGI’s report goes on to argue that violations were likely being under-counted at this time. …first, because facilities were not being required to monitor and report data to the same degree during Covid-19, and second, because the EPA was conducting fewer inspections during Covid-19. In sum, as they say, “the absence of data should not be taken as the absence of pollution.” What social harms emerge in the wake of these data absences? Who benefits from these policies, and who faces the greatest risks? How should we as data scientists think and act as we anaylze and present this data?"
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "In 1968, the Supreme Court case Terry v. Ohio ruled that a police officer could stop an individual without “probable cause” but with “reasonable suspicion” the suspect had committed a crime. Further, an officer could frisk an individual without “probable cause” but with “reasonable suspicion” the suspect was carrying a weapon. In the early 1990s, stop, question, and frisk became a widely prevalent tactic in policing in NYC. However, as the practice expanded in the 2000s, civil rights organizations began calling attention to the ways in which it was fueling the over-policing of NYC neighborhoods, along with racial profiling.\nIn 2011, David Floyd and David Ourlicht filed a class action lawsuit (on behalf of themselves and other minority civilians in NYC) against the City of New York, Police Commissioner Raymond Kelly, Mayor Michael Bloomberg, and a number of NYPD officers. Their argument was that the NYPD had stopped them without “reasonable suspicion” and that the defendants had sanctioned policies for stopping civilians on the basis of race or national origin. Honorable Shira A. Scheindlin, who oversaw the US District Court Case, ultimately ruled that stop and frisk was being carried out in an unconstitutional way. The officer-reported data that we will examine in this lab was leveraged as key evidence throughout this case.\nIn this lab, you will apply the 6 data wrangling verbs we learned this week in order to analyze data regarding NYPD stop, question, and frisk. Specifically, we will replicate data analysis performed by the NYCLU in 2011 to demonstrate how the practice was being carried out unconstitutionally in New York. We are specifically going to look at three issues:\n\nHow many times did the NYPD stop an individual in 2011?\nIn how many 2011 stops was the stopped individual found to be guilty of a crime?\nIn how many 2011 stops that involved a frisk was the individual found to be carrying a weapon?\n\nWe are also going to examine the racial breakdown for each of these three questions to assess the extent to which stop and frisk was being carried out in a discriminatory way in 2011.\n\n\n\nApply data wrangling verbs to subset and aggregate data\nConsider the implications of racial categorization"
  },
  {
    "objectID": "labs/lab5.html#introduction",
    "href": "labs/lab5.html#introduction",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "",
    "text": "In 1968, the Supreme Court case Terry v. Ohio ruled that a police officer could stop an individual without “probable cause” but with “reasonable suspicion” the suspect had committed a crime. Further, an officer could frisk an individual without “probable cause” but with “reasonable suspicion” the suspect was carrying a weapon. In the early 1990s, stop, question, and frisk became a widely prevalent tactic in policing in NYC. However, as the practice expanded in the 2000s, civil rights organizations began calling attention to the ways in which it was fueling the over-policing of NYC neighborhoods, along with racial profiling.\nIn 2011, David Floyd and David Ourlicht filed a class action lawsuit (on behalf of themselves and other minority civilians in NYC) against the City of New York, Police Commissioner Raymond Kelly, Mayor Michael Bloomberg, and a number of NYPD officers. Their argument was that the NYPD had stopped them without “reasonable suspicion” and that the defendants had sanctioned policies for stopping civilians on the basis of race or national origin. Honorable Shira A. Scheindlin, who oversaw the US District Court Case, ultimately ruled that stop and frisk was being carried out in an unconstitutional way. The officer-reported data that we will examine in this lab was leveraged as key evidence throughout this case.\nIn this lab, you will apply the 6 data wrangling verbs we learned this week in order to analyze data regarding NYPD stop, question, and frisk. Specifically, we will replicate data analysis performed by the NYCLU in 2011 to demonstrate how the practice was being carried out unconstitutionally in New York. We are specifically going to look at three issues:\n\nHow many times did the NYPD stop an individual in 2011?\nIn how many 2011 stops was the stopped individual found to be guilty of a crime?\nIn how many 2011 stops that involved a frisk was the individual found to be carrying a weapon?\n\nWe are also going to examine the racial breakdown for each of these three questions to assess the extent to which stop and frisk was being carried out in a discriminatory way in 2011.\n\n\n\nApply data wrangling verbs to subset and aggregate data\nConsider the implications of racial categorization"
  },
  {
    "objectID": "labs/lab5.html#review-of-key-terms",
    "href": "labs/lab5.html#review-of-key-terms",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this Data Wrangling Cheatsheet when completing this lab.\n\n\n\nfilter\n\nfilters to rows (observations) that meet a certain criteria\n\nselect\n\nkeeps only selected variables (columns)\n\narrange\n\nsorts values in a variable (column)\n\nsummarize\n\ncalculates a single value by performing an operation across a variable (column); summarizing by n() calculates the number of observations in the column\n\ngroup_by\n\ngroups rows (observations) by shared values in a variable (column); when paired with summarize(), performs an operation in each group\n\nmutate\n\ncreates a new variable (column) and assigns values according to criteria we provide"
  },
  {
    "objectID": "labs/lab5.html#stop-question-and-frisk-dataset",
    "href": "labs/lab5.html#stop-question-and-frisk-dataset",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Stop, Question, and Frisk Dataset",
    "text": "Stop, Question, and Frisk Dataset\nEvery time an officer stops a civilian in NYC, the officer is supposed to complete a UF-250 form (see image below) outlining the details of the stop.\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\n\n\n\nUF250 Side 2, Dan Nguyen’s Blog\n\n\nAs a result of some high profile police shootings in the late 1990s and mid-2000s, both the New York State Attorney General’s Office and the New York Civil Liberties Union began to examine NYPD stop and frisk activity for racial profiling. With pressure from these organizations, in the mid-2000s, information recorded on UF-250 forms began getting reported in public databases.\nSince then, the NYCLU has been using the data to conduct annual reports investigating the degree of racial profiling in stop and frisk activity in NYC. Through this research, they have been able to show that stops increased almost 700% from 2002 to 2011 - the year that marked the height of stop and frisk activity in NYC.\nWhen the NYPD’s use of stop and frisk went before the US District Court in the early 2010s, these public databases were integral in proving that stop and frisk was being carried out in NYC in an unconstitutional way. The ruling mandated that the NYPD create a policy outlining when stops were authorized, and since the practice has declined. See (Smith 2018) and (Southall and Gold 2019) for more information."
  },
  {
    "objectID": "labs/lab5.html#setting-up-your-environment",
    "href": "labs/lab5.html#setting-up-your-environment",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\n\nsqf_url &lt;- \"https://www1.nyc.gov/assets/nypd/downloads/zip/analysis_and_planning/stop-question-frisk/sqf-2011-csv.zip\"\ntemp &lt;- tempfile()\ndownload.file(sqf_url, temp)\nsqf_zip &lt;- unzip(temp, \"2011.csv\")\nsqf_2011 &lt;- read.csv(sqf_zip, stringsAsFactors = FALSE) \nsqf_2011_race_cat &lt;- read.csv(\"https://raw.githubusercontent.com/lindsaypoirier/STS-101/master/Data/SQF/sqf_race_categories.csv\", stringsAsFactors = FALSE) \nrm(sqf_url)\nrm(temp)\nrm(sqf_zip)\n\n\nWe will analyze data for all stops in 2011. This data can be found here. However, we’re only going to be considering a subset of the columns. Review the data dictionary for the columns we’re using in this analysis.\n\n\n\n\n\n\n\n\n\nVARIABLE\nDEFINITION\nPOSSIBLE VALUES\n\n\n\n\npct\nPrecinct of the stop\n1:123\n\n\narrestsum\nWas an arrest made or summons issued?\n1 = Yes\n0 = No\n\n\nfrisked\nWas suspect frisked?\n1 = Yes\n0 = No\n\n\nwpnfound\nWas a weapon found on suspect?\n1 = Yes\n0 = No\n\n\nrace_cat\nSuspect’s race\nAMERICAN INDIAN/ALASKAN NATIVE\nASIAN/PACIFIC ISLANDER\nBLACK\nBLACK-HISPANIC\nOTHER\nWHITE\nWHITE-HISPANIC\n\n\nage\nSuspect’s age\n999 indicates a missing value\n\n\n\n\nThe original dataset codes each race with a single letter. Run the code below to add a column called race_cat that writes out each racial category in accordance with the data documentation. It also replaces every instance of “Y” in the dataset with 1 and every instance of “N” with 0. This will allow us to sum the Yes’s in the dataset. You’ll learn more about how this code works in coming weeks.\n\n\nsqf_2011 &lt;- \n  sqf_2011 |&gt; \n  select(pct, race, age, frisked, pistol, riflshot, asltweap, knifcuti, machgun, othrweap, sumissue, arstmade) |&gt;\n  left_join(sqf_2011_race_cat, by = \"race\") |&gt;\n  mutate(across(frisked:arstmade, \n         ~ case_when(. == \"Y\" ~ 1, . == \"N\" ~ 0)))\nrm(sqf_2011_race_cat)\n\n\nNavigate to the NYCLU’s webpage documenting statistics from analysis of annual stop and frisk data. Scroll to the data for 2011. In today’s lab, we are going to replicate their analysis in an attempt to produce/verify these numbers."
  },
  {
    "objectID": "labs/lab5.html#analysis",
    "href": "labs/lab5.html#analysis",
    "title": "Lab 5: Exploratory Data Analysis",
    "section": "Analysis",
    "text": "Analysis\nTerry v. Ohio ruled that officers with “reasonable suspicion” that a stopped individual was carrying a weapon may frisk that individual for weapons.\nIn order to assess how “reasonable suspicion” was being applied in NYC in 2011, we are eventually going to analyze how many 2011 stops resulted in a frisk, how many frisks resulted in a weapon found on an individual, and how many stops resulted in either an arrest or summons.\nThe original dataset had separate variables for indicating whether a pistol, rifle, assault weapon, knife, machine gun, or other weapon was found on a suspect. We will create a variable equal to 1 if any of these weapons were found on the suspect. Further, the original dataset had separate variables for indicating whether a stop resulted in an arrest made or summons issued. We will create a variable equal to 1 if either occurred.\n\nQuestion\n\nAdd two new columns. The first should indicate whether a weapon was found, and the second should indicate whether an arrest/summons was made.\n\n\nsqf_2011 &lt;- \n  sqf_2011 |&gt;\n  #Add a variable for weapon found\n  _____(wpnfound = case_when(pistol == 1 |\n                               riflshot == 1 | \n                               asltweap == 1 |\n                               knifcuti == 1 | \n                               machgun == 1 | \n                               othrweap == 1 ~ 1,\n                             TRUE ~ 0))\nsqf_2011 &lt;- \n  sqf_2011 |&gt;\n  #Add a variable for arrest made or summons issued\n  _____(arrestsumm = case_when(sumissue == 1 | \n                                arstmade == 1 ~ 1,\n                               TRUE ~ 0))\n\n\n\n\n\n\n\n\n\nTip\n\n\n\ncase_when() allows us to assign values based on whether certain criteria is met. Here is the basic formula:\ncase_when(\n  &lt;criteria to check&gt; ~ &lt;set to this value if criteria is met&gt;, \n  TRUE ~ &lt;set to this value if criteria is not met&gt;\n  )\nCheck out the code above. In the first line of code, we set the value for wpnfound to 1 if the first criteria is met (i.e. if pistol equals 1 or riflshot equals 1, and so on.) Otherwise, we set the value for wpnfound to 0.\n\n\nNow that you’ve run this code, open up the data frame in the View window. What is the unit of observation in this dataset? What variables describe each row?\n\n\n\n  \n\n\n\nWe don’t need all of these columns in this data frame for this analysis. In fact, to replicate the NYCLU’s 2011 analysis, we only need six columns from this data frame.\n\n\n\nQuestion\n\nSubset the dataset to the six variables listed in the data dictionary above.\n\n\nsqf_2011 &lt;-\n  sqf_2011 |&gt;\n  _____(pct, arrestsumm, _____, wpnfound, race_cat, _____)\n\n\n\nIn order to be able to determine the percentage of stops that resulted in a frisk, weapon found, or arrest/summons, we are going to need to know how many total stops were conducted in NYC in 2011. We are going to store that value in the variable total_stops so that we can refer to it later.\n\n\n\nQuestion\n\nCalculate the number of stops in 2011. If you are not sure which function to use below, you may want to refer to the list of Summary functions in the the Data Wrangling cheatsheet. Remember that each row in the data frame is a stop.\n\n\ntotal_stops &lt;-\n  sqf_2011 |&gt;\n  summarize(Count = _____) |&gt;\n  pull()\n\ntotal_stops\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nsummarize() will return a data frame with the summarized value. Remember a data frame is a two-dimensional table with rows and columns (even if it’s a 1x1 table). If we want just the value, we can call pull() to extract the value from that two–dimensional table.\n\n\nRecall that for stops to be legally justified, officers must have “reasonable suspicion” that an individual committed a crime. This means that officers must be able to offer “specific and articulable facts,” along with rational inferences based on those facts, that an individual had committed a crime, in order to justify a stop. Reasonable suspicion requires more than a ‘hunch’.\nFor this reason, one of first metrics that the NYCLU calculates in their reports is the number and percentage of stops in which in the stopped individuals turned out to be innocent.\n\n\n\nQuestion\n\nHow many stops did not result in an arrest or summons in 2011? What percentage of stops did not result in an arrest or summons?\n\n\nsqf_2011 |&gt;\n  #Subset to rows where suspect innocent\n  _____(arrestsumm _____ 0) |&gt; \n  #Calculate number of observations\n  _____(total_innocent = n(), \n            percent_innocent = _____ / total_stops * 100)\n\n\n\nIn order to assess how stop and frisk may be carried out in a discriminatory way, the NYCLU’s reports go on to look at stops through the lens of a number of demographic indicators, including the age, race, gender, and ethnicity of the individuals stopped. For starters, let’s look at how many young people (14-24) were stopped in NYC in 2011.\n\n\n\nQuestion\n\nIn how many stops were the individuals aged 14-24? In what percentage of stops were the individuals aged 14-24?\n\n\nsqf_2011 |&gt;\n  #Subset to rows where suspect age 14-24\n  _____(age _____ 14 & age _____ 24) |&gt; \n  #Calculate number of observations and percentage of observations\n  _____(total_14_24 = _____, \n            percent_14_24 = n() / total_stops * 100)\n\n\n\nWhy doesn’t this match the values we see on the NYCLU website?\nNote the following from the NYCLU’s 2011 report on Stop, Question, and Frisk data:\n\n“In a negligible number of cases, race and age information is not recorded in the database. Throughout this report, percentages of race and age are percentages of those cases where race and age are recorded, not of all stops.”\n\nNote that it is very common in government datasets for missing values to get coded as 999 or -999. Here we want to exclude those values in the age column.\n\n\n\nQuestion\n\nFix the code below to calculate the currect number of stops for individuals 14-24.\n\n\ntotal_stops_age_recorded &lt;-\n  sqf_2011 |&gt;\n  #Subset to rows where age is not 999\n  _____(age _____ 999) |&gt; \n  summarize(Count = n()) |&gt;\n  pull()\n\nsqf_2011 |&gt;\n  filter(age &gt;= 14 & age &lt;= 24) |&gt;\n  summarize(total_14_24 = n(), \n            percent_14_24 = n() / total_stops_age_recorded * 100)\n\nThis still doesn’t match the values we see on the website, but it does match the values we see in the NYCLU’s 2011 report on Stop, Question, and Frisk data. This is typically when I would reach out to a representative at the NYCLU to inquire about the discrepancy.\n\n\nQuestion\n\nHow many stops were there per race in 2011? What percentage of stops per race in 2011? Arrange by number of stops in descending order.\n\n\ntotal_stops_race_recorded &lt;-\n  sqf_2011 |&gt;\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  filter(_____(race_cat) & race_cat _____ \"OTHER\") |&gt; \n  summarize(Count = n()) |&gt;\n  pull()\n\nsqf_2011 |&gt;\n  #Subset to rows where race_cat is not NA or \"OTHER\"\n  _____(_____(race_cat) & race_cat _____ \"OTHER\") |&gt; \n  #Group by race\n  _____(race_cat) |&gt; \n  #Calculate number of observations\n  _____(stops = n(), \n            percent_stops = n() / total_stops_race_recorded * 100) |&gt;\n  #Sort by stops in descending order\n  _____(_____(stops)) \n\n\n\nNote how this dataset categorizes race. Many different government datasets categorize race in many different ways. How we categorize race matters for how we can talk about discrimination and racial profiling. Imagine if WHITE was not categorized separately from WHITE-HISPANIC. The values in this dataset would appear very differently!\nWhen compiling their report, the NYCLU chose to aggregate two racial categories in this dataset into the one category - Latinx - in order to advance certain claims regarding discrimination. What we should remember is that these racial categories are not reported by those stopped; they are recorded by officers stopping individuals. They may not reflect how individuals identify themselves.\n\n\n\nQuestion\n\nIn how many stops were the individuals identified as Latinx (i.e. “WHITE-HISPANIC” or “BLACK-HISPANIC”)? In what percentage of stops were the individuals identified as Latinx?\n\n\nsqf_2011 |&gt;\n  #Subset to rows where race_cat is \"WHITE-HISPANIC\" or \"BLACK-HISPANIC\"\n  _____(race_cat _____ c(\"WHITE-HISPANIC\", \"BLACK-HISPANIC\")) |&gt; \n  #Calculate number of observations\n  _____(stops_Latinx = _____, \n            percent_Latinx = n() / total_stops_race_recorded * 100)\n\n\n\nAs a final consideration, we are going to calculate some summary statistics to further our understanding of the discriminatory ways in which this practice was being carried out. To so so, we are going to look at a racial breakdown of which stops resulted in an arrest/summons, and which frisks resulted in a weapon found.\n\n\n\nQuestion\n\nWhat percentage of stops in 2011 resulted in a frisk per race? What percentage of stops in 2011 resulted in a weapon found per race? What percentage of stops in 2011 resulted in an arrest or summons per race? In your resulting data table, each row should be a race, and there should be columns for stops, percent_stops, percent_frisked, percent_wpnfound , and percent_arrestsumm.\n\n\n# Write code here. \n\n\n\nQuestion\n\nBelow, in 2-3 sentences, summarize what you learn from reviewing the summary statistics from the code above. What does this tell us about the constitutionality of 2011 stop and frisk activity in NYC?\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nIn her ruling in the 2011 District Court case Floyd vs. the City of New York, Honorable Shira Sheidlen remarked the following regarding the database of stops we analyzed today:\n\nBecause it is impossible to individually analyze each of those stops, plaintiffs’ case was based on the imperfect information contained in the NYPD’s database of forms (“UF-250s”) that officers are required to prepare after each stop. The central flaws in this database all skew toward underestimating the number of unconstitutional stops that occur: the database is incomplete, in that officers do not prepare a UF-250 for every stop they make; it is one-sided, in that the UF250 only records the officer’s version of the story; the UF-250 permits the officer to merely check a series of boxes, rather than requiring the officer to explain the basis for her suspicion; and many of the boxes on the form are inherently subjective and vague (such as “furtive movements”). Nonetheless, the analysis of the UF-250 database reveals that at least 200,000 stops were made without reasonable suspicion.\n\nWhat are some of the consequences of the incompleteness and/or one-sided nature of this dataset? How should we think about and communicate its flaws vs its value? How might this data collection program be redesigned so as to represent more diverse perspectives? Share your ideas on our sds-192-discussions Slack channel.\n\n\n\n#If you finish early, I encourage you to attempt to plot some of this data below using `ggplot()`!"
  },
  {
    "objectID": "labs/lab9.html",
    "href": "labs/lab9.html",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "",
    "text": "In this lab, we will build a map that visualizes toxic emissions in Louisiana’s Cancer Alley - an 85-mile stretch of land from Baton Rouge to New Orleans, where some of the most serious health disparities in the U.S. are reported. In doing so, we will gain practice in producing point maps in Leaflet.\n\n\n\nTransform point data to an appropriate CRS\nMap point data in Leaflet\nCreate palettes for points on maps\nAdd legends and labels to a map"
  },
  {
    "objectID": "labs/lab9.html#introduction",
    "href": "labs/lab9.html#introduction",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "",
    "text": "In this lab, we will build a map that visualizes toxic emissions in Louisiana’s Cancer Alley - an 85-mile stretch of land from Baton Rouge to New Orleans, where some of the most serious health disparities in the U.S. are reported. In doing so, we will gain practice in producing point maps in Leaflet.\n\n\n\nTransform point data to an appropriate CRS\nMap point data in Leaflet\nCreate palettes for points on maps\nAdd legends and labels to a map"
  },
  {
    "objectID": "labs/lab9.html#review-of-key-terms",
    "href": "labs/lab9.html#review-of-key-terms",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this reference guide when completing this lab.\n\n\n\nCoordinate Reference System (CRS)\n\na system used to locate geographic points on different spatial projections"
  },
  {
    "objectID": "labs/lab9.html#nyc-public-advocates-worst-landlords-watchlist",
    "href": "labs/lab9.html#nyc-public-advocates-worst-landlords-watchlist",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "NYC Public Advocate’s Worst Landlords Watchlist",
    "text": "NYC Public Advocate’s Worst Landlords Watchlist\nThe NYC Public Advocate (currently Jumaane D. Willaims) publishes a list of NYC’s worst landlords every year by tracking the number of class B and C Housing Preservation and Development (HPD) violations in buildings owned by various people and companies in the City.\n\n\nFrom the Public Advocate’s website:\n\nExamples of Class B violations include: failing to provide self-closing public doors or adequate lighting in public areas, lack of posted Certificates of Occupancy, or failure to remove vermin. Class C violations include: immediately hazardous violations such as rodents, lead-based paint, and lack of heat, hot water, electricity, or gas.1 1See more here.\n\nI have already constructed a dataset documenting the 100 rental properties with the most average housing violations approved in 2021. If you want to check out the code to construct that dataset, you can review the code in data-import.R in this directory. Run the code below to import the dataset."
  },
  {
    "objectID": "labs/lab9.html#setting-up-your-environment",
    "href": "labs/lab9.html#setting-up-your-environment",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the following packages in your Environment:\n\n\ninstall.packages(\"leaflet\")\ninstall.packages(\"sf\")\n\n\nRun the code below to load today’s data frames into your environment.\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(RColorBrewer)\nlibrary(sf)\n\ntri_la_2023 &lt;- read_csv(\"https://raw.githubusercontent.com/SDS-192-Intro/public-website-fall-25/refs/heads/main/data/2023_la.csv\", name_repair = make.names) |&gt;\n  filter(X50..UNIT.OF.MEASURE == \"Pounds\") |&gt;\n  arrange(desc(X107..TOTAL.RELEASES)) |&gt;\n  mutate(chemical_total_text = paste(X37..CHEMICAL, X107..TOTAL.RELEASES, sep = \": \")) |&gt;\n  group_by(X2..TRIFD) |&gt;\n  summarize(X4..FACILITY.NAME = first(X4..FACILITY.NAME),\n            X12..LATITUDE = first(X12..LATITUDE),\n            X13..LONGITUDE = first(X13..LONGITUDE),\n            X23..INDUSTRY.SECTOR = first(X23..INDUSTRY.SECTOR),\n            X107..TOTAL.RELEASES = sum(X107..TOTAL.RELEASES),\n            X..CHEMICAL.TEXT = paste(chemical_total_text, collapse = \" &lt;br&gt; \"))"
  },
  {
    "objectID": "labs/lab9.html#mapping-in-leaflet",
    "href": "labs/lab9.html#mapping-in-leaflet",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Mapping in Leaflet",
    "text": "Mapping in Leaflet\nWe are going to work with the leaflet() package to design a map of the toxic emissions released in Louisiana in 2023. The map will color and size the points by the total emissions. At any point during these exercises, you can reference the leaflet documentation to help you build out these maps."
  },
  {
    "objectID": "labs/lab9.html#initialize-your-map",
    "href": "labs/lab9.html#initialize-your-map",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Initialize your Map",
    "text": "Initialize your Map\nThere are three steps to initializing a map in leaflet:\n\nCall the leaflet() function to create a map widget.\nCall the setView(lat = 0.00, lng = 0.00, zoom = 0). This function determines where the map will initially focus. We provide a set of coordinates that will be the map’s center point when we load it, and a number (from 1 to 16) to indicate the level at which to zoom in on the map.\n\nCall addProviderTiles() to load up provider tiles that constitute a base map. A number of different map providers have provider tiles that we can reference here. A few examples of the arguments we can supply to this function include include:\n\n\nproviders$OpenStreetMap\nproviders$Stamen.Toner\nproviders$CartoDB.Positron\nproviders$Esri.NatGeoWorldMap\n\nRun the code below to initialize a map.\n\nmap1 &lt;- leaflet(width = \"100%\") %&gt;%\n  setView(lat = 0.00, lng = 0.00, zoom = 0) %&gt;%\n  addProviderTiles(providers$Stamen.Toner)\n\nmap1\n\n\n\n\n\n\nQuestion\n\nAdjust the code below to center the map on Louisiana. You’ll need to look up the coordinates for LA, keeping in mind that South and West coordinates will be negative. Adjust the zoom to keep the whole state in view (setting the zoom level between 1 and 16). When you are happy with the View, switch out the provider tiles to a base map that won’t distract from the data points we will layer on top of this map. Keep in mind our discussions regarding Visualization Aesthetics here.\n\n\nla_map &lt;- leaflet(width = \"100%\") %&gt;%\n  setView(lat = 0.00, lng = 0.00, zoom = 0) %&gt;%\n  addProviderTiles(providers$Stamen.Toner)\n\n# Uncomment below to view the map!\n# la_map"
  },
  {
    "objectID": "labs/lab9.html#set-geometry-and-transform-data-crs",
    "href": "labs/lab9.html#set-geometry-and-transform-data-crs",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Set Geometry and Transform Data CRS",
    "text": "Set Geometry and Transform Data CRS\nWe will be looking to convert our data into an object with coordinate-based geometry so that we can map it. We can use the st_as_sf() function to do this. st_as_sf() takes two arguments:\n\nThe names of the columns in our data frame containing geographic coodinates in the format coords = c(\"&lt;longitude column names&gt;\", \"&lt;latitude column name&gt;\")\nThe coordinate reference system that the dataset currently uses in the format crs = &lt;crs number&gt;.\n\nAfter adding this geometry column with st_as_sf(), we need to make sure that the CRS for our coordinates is consistent with the CRS of the basemap we will be placing the points on. The coordinates in tri_la_2023 data are in NAD 83 (EPSG:4269) and our basemap is in WGS 84 (EPSG:4326).\n\nQuestion\n\nIn the code block below, create a new geometry column with the given coordinates, using the function st_as_sf(). The column for longitude will go in the first coordinate position, and latitude will go in the second. Be sure to set the data’s current CRS (4269) in that function. Then transform the CRS to 4326 using st_transform().\n\n\n# Uncomment and fill in the blanks below. The column for longitude will go in the first coordinate position, and latitude will go in the second. \n\n#tri_la_2023 &lt;- tri_la_2023 %&gt;%\n#  st_as_sf(coords = c(\"____\", \"____\"), crs = ____) %&gt;%\n#  st_transform(____)"
  },
  {
    "objectID": "labs/lab9.html#add-layers",
    "href": "labs/lab9.html#add-layers",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Add Layers",
    "text": "Add Layers\nOnce we have our initial map, we can add layers to the map that display different forms of geospatial data. There are a number of different functions in leaflet that we can use to add layers. For instance, we can add markers to the map at a certain geo-coordinates using the addMarkers() function. We can also add polygons and rectangles to the map using the addRectangles() function or the addPolygons() function. Today we are going to work exclusively with the addCircleMarkers() function. This allows us to add a circle at the latitude and longitude for each row in our dataset. It also allows us to adjust the circle’s color and size according to values in our dataset.\n\nQuestion\n\nIn the code block below, pipe addCircleMarkers() onto the basemap and list data = tri_la_2023 as an argument in that function. It should look like my map below.\n\n\n# Uncomment and add the function\n\n#la_map %&gt;%\n\n\n\n\n\n\n\n\n\nMap isn’t so legible/beautiful at this point, right?"
  },
  {
    "objectID": "labs/lab9.html#styling-the-map",
    "href": "labs/lab9.html#styling-the-map",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Styling the Map",
    "text": "Styling the Map\n\nQuestion\n\nMap isn’t so legible/beautiful at this point, right? Copy the map you just created into the code chunk below, and pipe the addCircleMarkers() function onto your code. Check out the help pages for the addCircleMarkers() function, and add some arguments to help with the map’s legibility. At the very least, you should adjust the radius, weight, color, fillColor, and fillOpacity, and understand how each of these arguments will change the style of the map. For now you can set the color to “black” and the fillColor to “white”. See if you can get your map to match mine below.\n\n\n# Copy previous map here! \n\n\n\n\n\n\n\n\n\nSizing the Circles\n\nQuestion\n\nCopy and paste the last map that you created below. We are going to size each circle by the total chemical releases at that facility. Total releases is stored in the column X107..TOTAL.RELEASES in our dataset, so we could size the circles by setting the radius to ~X107..TOTAL.RELEASES. However, this would lead to some massive circles on our map as certain facilities have released hundreds of thousands of pounds in emissions, and the value we supply to radius will determine the pixels of the circle on our map. To deal with this, we are going to divide the total releases by 1000000. Set the radius to ~X107..TOTAL.RELEASES/1000000 below.\n\n\n# Copy map colored by bins here!\n\n\n\n\n\n\n\n\n\n\nCreating Color Palettes\nNow that our map is looking more legible, let’s color the circles by the type of industry for each facility. Remembering back to our lesson on Understanding Datasets and Visualization Aesthetics, we should keep in mind that X23..INDUSTRY.SECTOR is a categorical variable, and therefore, we will create a qualitative color palette to map it.\nThere is one function in leaflet for creating a categorical color palette:\n\ncolorFactor(): Creates a palette by assigning a color to each unique value\n\nThis function takes the following arguments:\n\npalette: the colors we wish to map to the data. We will use preset palettes from the RColorBrewer. You can call display.brewer.all() to see the list of palettes or reference here: http://applied-r.com/rcolorbrewer-palettes/\ndomain: the values we wish to apply the palette to. Here we reference the column from the data frame we wish to color the points by using the accessor $.\n\n\nQuestion\n\nCreate a categorical palettes below using colorFactor(), setting the palette to “Set2”, and the domain to tri_la_2023$X23..INDUSTRY.SECTOR.\n\n\n#Uncomment below to create the palette\n\n#pal_fact &lt;- \n\n\n\nNow that we have created a palette, we can map the colors to the points in our map.\n\n\n\nQuestion\n\nCopy and paste the last map you created into the code chunk below. We are going to adjust the fillColor by setting it to the variable from the dataset that we wish to color (X23.INDUSTRY.SECTOR), colored by one of the palettes that we created. We can do this by setting the fill color argument equal to ~pal_fact(X23.INDUSTRY.SECTOR).\nWe also need to add a legend to explain the colors. Add a pipe to the end of the addCircleMarkers() and then add the function addLegend(). Consult the help pages for the addLegend() function to determine how to add a legend for the meaning of the colors represented on the map. At the very least, you will need an argument for data, pal, and values.\nYour map should look like mine below.\n\n\n# Copy the map here!\n\n\n\n\n\n\n\n\n\nNow there are 19 unique industry sectors in this dataset, and that may be too many unique values to be able to readily recognize differentiation in color. Let’s change up how we approach coloring this map. Instead, let’s color the circles by the 2023 total releases at each facility on the map. Keep in mind that X107..TOTAL.RELEASES is a numeric variable, and therefore, we will create a sequential color palette to map it.\nThere are three functions in leaflet for creating a sequential color palette:\n\ncolorNumeric(): Creates a palette by assigning numbers to different colors on a spectrum\ncolorBin(): Creates a palette by grouping numbers into a specified number of equally-spaced intervals (e.g. 0-10, &gt;10-20, &gt;20-30)\ncolorQuantile(): Creates a palette by grouping numbers into a specified number of equally-sized quantiles\n\nEach of these functions takes the arguments palette and domain, just as we did when calling colorFactor(). In addition, the colorBin() function takes the argument bins, which indicates the number of color intervals to be created. The colorQuantile() functions takes the argument n, which indicates the number of quantiles to map data into.\n\n\n\nQuestion\n\nCreate three palettes below (one using each function colorNumeric(), colorBin(), and colorQuantile()), setting the palette to “YlOrRd”, and the domain to tri_la_2023$X107..TOTAL.RELEASES. For colorBin(), set the bins to 6, and for colorQuantile(), set n to 4.\n\n\npal_num &lt;- colorNumeric(palette=\"YlOrRd\", \n                        domain = tri_la_2023$X107..TOTAL.RELEASES)\n#Uncomment below to create the other two palettes.\n\n#pal_bin &lt;- \n\n#pal_quant &lt;- \n\n\n\nQuestion\n\nCopy and paste the last map you created into the code chunk below, three times. For each, we are going to adjust the fillColor by setting it to the variable from the dataset that we wish to color (X107..TOTAL.RELEASES), colored by one of the palettes that we created. We can do this by setting the fill color argument equal to:\n\n~pal_num(X107..TOTAL.RELEASES): for coloring the points according the pal_num() palette we created on the X107..TOTAL.RELEASES variable\n~pal_bin(X107..TOTAL.RELEASES): for coloring the points according the pal_bin() palette we created on the X107..TOTAL.RELEASES variable\n~pal_quant(X107..TOTAL.RELEASES): for coloring the points according the pal_quant palette we created on the X107..TOTAL.RELEASES() variable\n\nAdjust the legends for each of the maps.\nYour maps should look like mine below.\n\n\n# Copy map first time here!\n\n# Copy map second time here!\n\n# Copy map third time here!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nIn a comment below, explain the following: Why do the colors appear differently on each map? Which map best represents the distribution of values in X107..TOTAL.RELEASES?\n\n\n# Add comment here. \n\n\n\n\nPopups\nLeaflet popups display information about a point we click on it. They are configured via Hypertext Markup Language (HTML). HTML is the markup language used to create webpages. It is akin to RMarkdown; for both HTML and RMarkdown we use certain characters to signal how we want our text formatted and styled.\nThere are a few HTML tags that you need to know in order to style a popup:\n\n&lt;br&gt; creates a newline\n&lt;b&gt;…&lt;/b&gt; makes the enclosing text bold\n&lt;i&gt;…&lt;/i&gt; makes the enclosing text italicized\n\nSo let’s say I had my name stored in the variable name and my institution stored in the variable institution. If I wanted to create a popup with my name in bold and my institution beneath it italics, I would use the following HTML tags: paste(\"&lt;b&gt;\", name, \"&lt;/b&gt;&lt;br&gt;&lt;i&gt;\", institution, \"&lt;/i&gt;\"). Here I’ve pasted together my name (wrapped in the bold tags), a break, and my instituion (wrapped in the italic tags).\n\nQuestion\n\nNow, we are going to add popups on the map. Copy and paste the code from the previous step into the code chunk below. Then do the following:\n\nAdd the popup argument to the addCircleMarkers() function. Using the paste() function, create a popup that includes the facility’s name (in bold), the facility’s industry sector (in italics), and the chemicals released by the facility. (Note that the last the data point is stored in X..CHEMICAL.TEXT). Note that you will need to place a ~ in front of the paste() function to communicate that there are variables in your argument.\nAdd the popupOptions argument to the addCircleMarkers() function. Set the popupOptions to popupOptions(maxHeight = 200, closeOnClick = TRUE)). This will ensure that really tall popups will scroll vertically rather than taking up an entire page.\n\nYour popups should look like mine in the map below.\n\n\n# Copy previous map here!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nNotably, while the Emergency Planning and Community Right-to-Know Act mandates reporting of emissions, it does not mandate monitoring of emissions. While other environmental regulations do set certain monitoring standards for specific TRI chemicals and pollution activities, for all other chemicals and activities, facilities are required to report based on a “reasonable estimate” of releases and other waste management quantities. In the early 1990s the National Wildlife Federation uncovered that certain facilities showing tremendous reductions in emissions from one year to the next had not actually cut down on pollution, but instead developed different ways of estimating their emissions. The public doesn’t see how emissions are calculated; they only see the final reported emissions. Given this history of certain facilities reporting “phantom reductions”, what kinds of legal, practice-based, or social interventions might ensure that this data remains reliable and accountable to the communities that use it to assess risks? What might make these interventions challenging to implement; what would need to change in order for these interventions to be realized? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/problem-solving.html",
    "href": "labs/problem-solving.html",
    "title": "Problem Solving",
    "section": "",
    "text": "This lab will introduce you to resources and techniques for problem solving in R. You should reference this lab often throughout the semester for reminders on best practices for addressing errors and getting help.\n\n\n\nInterpret error messages in R\nRead R cheatsheets\nAccess R help pages\nReference Stack Overflow and other online resources for help"
  },
  {
    "objectID": "labs/problem-solving.html#introduction",
    "href": "labs/problem-solving.html#introduction",
    "title": "Problem Solving",
    "section": "",
    "text": "This lab will introduce you to resources and techniques for problem solving in R. You should reference this lab often throughout the semester for reminders on best practices for addressing errors and getting help.\n\n\n\nInterpret error messages in R\nRead R cheatsheets\nAccess R help pages\nReference Stack Overflow and other online resources for help"
  },
  {
    "objectID": "labs/problem-solving.html#interpreting-error-messages",
    "href": "labs/problem-solving.html#interpreting-error-messages",
    "title": "Problem Solving",
    "section": "Interpreting Error Messages",
    "text": "Interpreting Error Messages\nThroughout this week, we have taken a look at different error messages that R presents when it can’t evaluate our code. Today, we will consider these in more detail. First, it’s important to make some distinctions between the kinds of messages that R presents to us when attempting to run code:\n\nErrors\n\nTerminate a process that we are trying to run in R. They arise when it is not possible for R to continue evaluating a function.\n\nWarnings\n\nDon’t terminate a process but are meant to warn us that there may be an issue with our code and its output. They arise when R recognizes potential problems with the code we’ve supplied.\n\nMessages\n\nAlso don’t terminate a process and don’t necessarily indicate a problem but simply provide us with more potentially helpful information about the code we’ve supplied.\n\n\nCheck out the differences between an error and a warning in R by reviewing the output in the Console when you run the following code chunks.\n\nError in R\n\nsum(\"3\", \"4\")\n\nError in sum(\"3\", \"4\"): invalid 'type' (character) of argument\n\n\n\n\nWarning in R\n\nvector1 &lt;- 1:5\nvector2 &lt;- 3:6\nvector1 + vector2\n\nWarning in vector1 + vector2: longer object length is not a multiple of shorter\nobject length\n\n\n[1]  4  6  8 10  8\n\n\nSo what should you do when you get an error message? How should you interpret it? Luckily, there are some clues and standardized components of the message the indicate why R can’t execute the code. Consider the following error message that you received when running the code above:\nError in sum(“3”, “4”) : invalid ‘type’ (character) of argument\nThere are three things we should pay attention to in this message:\n\nThe word “Error” indicates that this code did not run.\nThe text immediately after the word “in” tells us which specific function did not run.\nThe text after the colon gives us clues as why the code did not run.\n\nReviewing the error above, I can guess that there was a problem with the argument that I supplied to the sum() function, and specifically that I supplied a function of the wrong type.\n\nQuestion\n\nRun the codes below and check out the error messages. Review the code to fix each of the errors. Note that each subsequent code chunk relies on the previous code chunk, so you will need to fix the errors in order and run the chunks in order.\n\n\n# Create three vectors\na &lt;- 1, 2, 3, 4, 5\nb &lt;- \"a\", \"b\", \"c\", \"d\", \"e\"\nc &lt;- TRUE, FALSE, TRUE, TRUE, FALSE\n\nError: &lt;text&gt;:2:7: unexpected ','\n1: # Create three vectors\n2: a &lt;- 1,\n         ^\n\n\n\n# Add the values in the vector a\na_added &lt;- add(a)\n\nError in add(a): could not find function \"add\"\n\n\n\n# Multiply the previous output by 3\nthree_times_a_added &lt;- added_a * 3\n\nError in eval(expr, envir, enclos): object 'added_a' not found\n\n\n\n# Create a dataframe with col1 and col2\ndf &lt;- data.frame(\n  col1 = c(1, 2, 3)\n  col2 = c(\"a\", \"b\", \"c\")\n\nError: &lt;text&gt;:4:3: unexpected symbol\n3:   col1 = c(1, 2, 3)\n4:   col2\n     ^\n\n\n\n# Add a new column to df\ndf$col3 &lt;- c(TRUE, FALSE)\n\nError in df$col3 &lt;- c(TRUE, FALSE): object of type 'closure' is not subsettable"
  },
  {
    "objectID": "labs/problem-solving.html#preparing-to-get-help",
    "href": "labs/problem-solving.html#preparing-to-get-help",
    "title": "Problem Solving",
    "section": "Preparing to Get Help",
    "text": "Preparing to Get Help\nWhen we do get errors in our code and need to ask for help in interpreting them, it’s important to provide collaborators with the information they need to help us. Sometimes when teaching R I will hear things like: “My code doesn’t work!” or “I’m stuck and don’t know what to do,” and it can be challenging to suss out the root of the issue without more information. Here are some strategies for describing issues you are having with your code:\n\nReference line numbers. Notice the left side of your template document has a series of numbers listed vertically next to each line? These are known as line numbers. Oftentimes, if you are having an issue with your code and ask me to review it, I will say something like: “Check out line 53.” By this I mean that you should scroll the document to the 53rd line. You can similarly tell me or your peers which line of your code you are struggling with.\nCompose good reproducible examples. A good reproducible example includes all of the lines of code that we need to reproduce an output on our own machines. This means that if you create a vector in a previous code snippet and then supply it as an argument in another code snippet, you are going to want to make sure that both of these lines of code appear in your reproducible example. Further, if the functions that you are using are from certain packages, you will want to make sure the library() call to load that package is in your reproducible example.\nUse the code and code block buttons in Slack to share example code. Certain characters like quotation marks and apostrophes are treated differently across RStudio and Slack. This is because these programs encode characters differently. For example, run the code chunk below and check out the output in your Console. The first line of code I typed directly into RStudio. The second I copied over from Slack.\n\n\n# typed directly into RStudio\ntoupper(\"apple\")\n# copied from Slack\ntoupper(“apple”)\n\nError: &lt;text&gt;:4:9: unexpected invalid token\n3: # copied from Slack\n4: toupper(“\n           ^\n\n\nNotice the slight differences in the shape of the quotation marks? R recognizes the first but doesn’t recognize the second, even though I used the same keyboard key to create both. This is due to the fact that these two systems use different character encodings.\nThe Code button (for a single line of code) and Code Block button (for multiple lines of code) in Slack are useful tools for composing code and avoiding character encoding issues. If you click these buttons when typing a Slack message, you can enter code in the red outlined box that appears, and this will easily copy to RStudio. I will ask you to always use these features when copying code this semester.\n\n\n\n\n\n\nTip\n\n\n\nIn Slack, you can also wrap text backticks (` `) to have it output in a single-line code block, and three backticks (``` ```) to have it output as a multi-line code block.\n\n\n\nQuestion\n\nCopy and paste one line of code from this lab as a threaded comment on the Slack message I posted on September 7, 2022. Be sure the text is formatted as code."
  },
  {
    "objectID": "labs/problem-solving.html#help-pages",
    "href": "labs/problem-solving.html#help-pages",
    "title": "Problem Solving",
    "section": "Help pages",
    "text": "Help pages\nOne resource we’ve already discussed are the R help pages. I tend to use the help pages when I know the function I need to use, but can’t remember how to apply it or what its parameters are. Help pages typically include a description of the function, its arguments, details about the function, the values it produces, a list of related functions, and examples of its use. We can access the help pages for a function by typing the name of the function with a question mark in front of it into our Console (e.g. ?log or ?sum). Some help pages are well-written and include helpful examples, while others are spotty and don’t include many examples.\n\nQuestion\n\nAccess the help pages for the function sort(). Write code below to sort the vector a (which you created in an earlier step) in decreasing order.\n\n\n# Write code here"
  },
  {
    "objectID": "labs/problem-solving.html#cheatsheets",
    "href": "labs/problem-solving.html#cheatsheets",
    "title": "Problem Solving",
    "section": "Cheatsheets",
    "text": "Cheatsheets\nThe R community has developed a series of cheatsheets that list the functions made available through a particular package and their arguments. I tend to use cheatsheets when I know what I need to do to a dataset in R, but I can’t recall the function that enables me to do it.\n\nQuestion\n\nNavigate to this cheatsheet for base R. Imagine we collected the temperature of our home each day for the past ten days (see code below).\n\n#Create a vector of temperatures\ntemps_to_factor &lt;- c(68, 70, 78, 75, 69, 80, 66, 66, 79)\n\nLet’s say we wanted to find how each day ranked from coolest to hottest. For instance, I wanted to know the ranking for day 1 vs. day 2 vs. day 3, and so on. Using the cheatsheet, find the function that will allow you to generate a ranking of each day’s temperature. Search the help pages for this function to determine how to randomly rank two days with the same temperature. Write a comment to yourself describing how this function is different than sorting the data.\n\n\n# Write code here\n\n# Write comment here"
  },
  {
    "objectID": "labs/problem-solving.html#searching-the-web",
    "href": "labs/problem-solving.html#searching-the-web",
    "title": "Problem Solving",
    "section": "Searching the Web",
    "text": "Searching the Web\nI encourage you to search the web when you get errors in your code. Others have likely experienced that error before and gotten help from communities of data analysts and programmers. However, you should never copy and paste code directly from Stack Overflow. This violates the course policies on Academic Honesty. Instead you should use these resources to take notes and learn how to improve and revise code. Any time you reference Stack Overflow or any other Web resource to help you figure out an answer to a problem, you should cite that resource in your code. Here is how you would cite that post in APA format:\n\nUsername. (Year, Month Date). Title of page (Question/Topic). Stack Overflow. URL\n\n\nQuestion\n\nAdd a comment to the code chunk above, citing this Stack Overflow post. Be sure to cite the post properly. You can read through the post to double check your answers."
  },
  {
    "objectID": "labs/problem-solving.html#submission",
    "href": "labs/problem-solving.html#submission",
    "title": "Problem Solving",
    "section": "Submission",
    "text": "Submission\n\nWhen you are done, save your .qmd file in RStudio.\nStage, commit, and push your file in the Git pane. Refer to step 12 in our Course Infrastructure setup for a reminder of how to do this.\nNavigate back to GitHub.com and click on the .qmd files to make sure you see your changes to the files there. If you don’t see the changes there, I won’t see them either!"
  },
  {
    "objectID": "problem-solving.html",
    "href": "problem-solving.html",
    "title": "Problem Solving with Data",
    "section": "",
    "text": "Read problem carefully.\nTake a deep breath!\nRead the problem again.\nDetermine the unit of observation in your available data and its relevance to the question.\nDetermine which variables in the available data are relevant to your question.\nDraw what your final data frame might look like.\nGet your data frame down to size! Compare the scope of what is requested in the question to what is requested in your data and subset accordingly.\nWrangle! (This is the most variable step, and you’ll need to have a handle on our data wrangling verbs.)\nSelect the most relevant variables for final presentation."
  },
  {
    "objectID": "assignments.html#syllabus-quiz-3",
    "href": "assignments.html#syllabus-quiz-3",
    "title": "Assignments",
    "section": "",
    "text": "Our course syllabus is a contract that identifies expectations that I have of you, expectations you can have of me, and how we can effectively use resources to create an impactful learning environment. It serves you to know the syllabus inside and out; knowing the syllabus means that you are aware of resources available to you if you are struggling and that you won’t be surprised by course policies that may negatively impact your grade. It also serves the whole class to have everyone know the course syllabus; knowing the syllabus sets standards for how we will learn as a community, and it also reduces the class time spent answering questions that have already been documented.\nAfter reading the syllabus, you should complete the 20 question syllabus quiz in Perusall. This syllabus quiz is graded on correctness. Please note that this is an open note quiz, and you may work with other students on this quiz."
  },
  {
    "objectID": "assignments.html#reading-quizzes-5",
    "href": "assignments.html#reading-quizzes-5",
    "title": "Assignments",
    "section": "Reading Quizzes (5%)",
    "text": "Reading Quizzes (5%)\nI’ve selected 10 readings that correspond to 10 different units we will engage in class. Reading ahead of class will help introduce you to the language you will see in class. Lectures and class activities will reinforce the concepts that you learn in the reading. All readings will be posted in Perusall with a very short reading quiz. Reading quizzes are graded based on completion, not on correctness. They are designed to let me know ahead of time which concepts students understand from the reading and which concepts they are struggling with. They also help you understand which concepts you may want to study/practice further."
  },
  {
    "objectID": "assignments.html#problem-solving-lab-2",
    "href": "assignments.html#problem-solving-lab-2",
    "title": "Assignments",
    "section": "Problem Solving Lab (2%)",
    "text": "Problem Solving Lab (2%)\nThis course’s problem solving lab introduces you to how to solve problems with code - from understanding error messages to getting help on Slack to referencing online resources. The lab will be completed in RStudio and submitted via GitHub classroom. The problem solving lab is graded on completion. You may lose points for incomplete sections."
  },
  {
    "objectID": "assignments.html#labs-30",
    "href": "assignments.html#labs-30",
    "title": "Assignments",
    "section": "Labs (30%)",
    "text": "Labs (30%)\nThere are ten units in this course, and ten labs associated with each unit. Labs in this course introduce a data science skill by walking you through exploratory analysis of a dataset documenting a socially-relevant issue (such as racial profiling in policing, affordable housing, and pollution). Labs will be started in class on Fridays and completed for homework by the following Wednesday. If you finish a lab early, you are encouraged to help your classmates. Labs will be graded on both completion and correctness. You will earn some credit for every exercise that you attempt, but points may be deducted for incorrect responses/code. Note that, at the end of each lab, there will be a prompt asking you to consider some of the ethical considerations of the data analysis you just completed. You must respond to this prompt in Slack to earn full credit on the lab."
  },
  {
    "objectID": "assignments.html#projects-30",
    "href": "assignments.html#projects-30",
    "title": "Assignments",
    "section": "Projects (30%)",
    "text": "Projects (30%)\nThere will be three group projects assigned over the course of the semester."
  },
  {
    "objectID": "assignments.html#mid-term-exam-15",
    "href": "assignments.html#mid-term-exam-15",
    "title": "Assignments",
    "section": "Mid-Term Exam (15%)",
    "text": "Mid-Term Exam (15%)\nThe mid-term exam will be graded on correctness. Note that after you receive your grade, you may choose up to two questions on the mid-term exam to have re-assessed in an in-person technical interview with me. In an in-person technical interview, I will ask you questions related to the exam questions you would like re-assessed, and you will have an opportunity to demonstrate your understanding of the concepts. Please note that an in-person technical interview must be scheduled before Thanksgiving Break."
  },
  {
    "objectID": "assignments.html#final-exam-15",
    "href": "assignments.html#final-exam-15",
    "title": "Assignments",
    "section": "Final Exam (15%)",
    "text": "Final Exam (15%)\nThe final exam will be a cumulative exam that assesses your understanding of concepts covered throughout the course. The final exam will be graded on correctness."
  },
  {
    "objectID": "assignments.html#final-exam-15-1",
    "href": "assignments.html#final-exam-15-1",
    "title": "Assignments",
    "section": "Final Exam (15%)",
    "text": "Final Exam (15%)"
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "Read the ggplot cheatsheets\nProduce and interpret univariate plots\nProduce and interpret multivariate plots\nSummarize and interpret variation and co-variation from observation of plots\nContextualize plots with descriptive labels and titles"
  },
  {
    "objectID": "labs/lab3.html#introduction",
    "href": "labs/lab3.html#introduction",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "",
    "text": "Read the ggplot cheatsheets\nProduce and interpret univariate plots\nProduce and interpret multivariate plots\nSummarize and interpret variation and co-variation from observation of plots\nContextualize plots with descriptive labels and titles"
  },
  {
    "objectID": "labs/lab3.html#review-of-key-terms",
    "href": "labs/lab3.html#review-of-key-terms",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nMultivariate Plots\n\nPlots that summarize and visualize the distribution and relationship between multiple variables\n\nUnivariate Plots\n\nPlots that summarize and visualize the distribution of a single variable\n\nVariation\n\nThe degree to which categorical or numeric values vary across data"
  },
  {
    "objectID": "labs/lab3.html#home-mortgage-disclosure-act-hmda",
    "href": "labs/lab3.html#home-mortgage-disclosure-act-hmda",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Home Mortgage Disclosure Act (HMDA)",
    "text": "Home Mortgage Disclosure Act (HMDA)\nIn the United States, homeownership can offer a pathway to promoting stability and social mobility. However, historically, opportunities for purchasing a home have been unequally distributed along racial and ethnic lines. In a practice known as redlining, some banks in the mid-twentieth century would make decisions about who would be approved for loans based on the perceived “riskiness” of the neighborhoods they lived in; risk was often determined by the racial makeup of the neighborhood. This discrimination in lending enforced the racial segregation of communities and made it much more difficult for Black and other minority individuals to secure access to a financial asset that could be passed down in their families. With limited access to this opportunity for securing generational wealth, we still feel the affects of redlining today - even though racially-motivated redlining was prohibited by the Fair Housing Act in 1968.\nIn order to monitor for fair lending practices, in 1975, the Home Mortgage Disclosure Act was passed, requiring lending institutions to publicly disclose data about their lending - reporting each loan application they receive, whether the loan was approved or denied, and demographic information on the borrowers. This data is currently published annually by the Consumer Federal Protection Bureau, and a number of organizations monitor it to determine if there are continued forms of racial discrimination in who has access to home loans.\nIn this lab, we are going to examine Home Mortgage Disclosure Act data from 2024 in Mississippi. Recently, a study by Lending Tree showed that Black homebuyers in Mississippi face some of the highest loan denial rates in the country:\n\n\nEach row in this dataset is one mortgage application, and variables describing the demographics of the applicant, details about the loan, and whether or not the loan was approved are included in the data. When denied, the reasons for denial are also included. For consistency in comparisons, I’ve filtered this dataset down to only include 30-year conventional, first-lien, single-family, non-commercial loans that are either for a home purchase or a home improvement. The data documentation for this dataset is quite thick, so I will provide you with a data dictionary for today.\n\n\n\n\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nderived_ethnicity\nSingle aggregated ethnicity categorization derived from applicant/borrower and co-applicant/co-borrower ethnicity fields\n\n\nderived_race\nSingle aggregated race categorization derived from applicant/borrower and co-applicant/co-borrower race fields\n\n\nderived_sex\nSingle aggregated sex categorization derived from applicant/borrower and co-applicant/co-borrower sex fields\n\n\naction_taken\nThe action taken on the covered loan or application\n\n\nloan_purpose\nThe purpose of covered loan or application\n\n\nloan_amount\nThe amount of the covered loan, or the amount applied for\n\n\nloan_to_value_ratio\nThe ratio of the total amount of debt secured by the property to the value of the property relied on in making the credit decision\n\n\nincome\nThe gross annual income, in thousands of dollars, relied on in making the credit decision, or if a credit decision was not made, the gross annual income relied on in processing the application\n\n\ndebt_to_income_ratio\nThe ratio, as a percentage, of the applicant’s or borrower’s total monthly debt to the total monthly income relied on in making the credit decision\n\n\ntract_minority_population_percent\nPercentage of minority population to total population for tract, rounded to two decimal places\n\n\ndenial_reason\nThe principal reason, or reasons, for denial"
  },
  {
    "objectID": "labs/lab3.html#setting-up-your-environment",
    "href": "labs/lab3.html#setting-up-your-environment",
    "title": "Lab 3: Plotting Frequencies and Distributions",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nRun the code below to the import the 2024 Home Mortgage Disclosure Act data for Mississippi into R. It may take a few moments to load. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\nhmda_ms_2024 &lt;- read.csv(\"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv?states=MS&years=2024\") |&gt;\n  filter(loan_type == 1 & \n           lien_status == 1 & \n           conforming_loan_limit == 'C' & \n           derived_dwelling_category %in% c(\"Single Family (1-4 Units):Site-Built\", \n                                            \"Single Family (1-4 Units):Manufactured\") & \n           business_or_commercial_purpose == 2 &\n           loan_purpose %in% c(1, 2)) |&gt;\n  select(lei,\n         county_code,\n         derived_ethnicity,\n         derived_race,\n         derived_sex,\n         action_taken,\n         loan_purpose,\n         loan_amount,\n         interest_rate,\n         property_value,\n         loan_to_value_ratio,\n         denial_reason.1,\n         denial_reason.2,\n         denial_reason.3,\n         denial_reason.4,\n         income,\n         debt_to_income_ratio) |&gt;\n  mutate(loan_purpose = recode(loan_purpose, \n                               \"1\" = \"Home Purchase\",\n                               \"2\" = \"Home improvement\",\n                               .default = NA_character_)) |&gt;\n  mutate(action_taken = recode(action_taken, \n                               \"1\" = \"Loan originated\", \n                               \"2\" = \"Application approved but not accepted\",\n                               \"3\" = \"Application denied\",\n                               \"4\" = \"Application withdrawn by applicant\",\n                               \"5\" = \"File closed for incompleteness\",\n                               \"6\" = \"Purchased loan\",\n                               \"7\" = \"Preapproval request denied\",\n                               \"8\" = \"Preapproval request approved but not accepted\",\n                               .default = NA_character_)) |&gt;\n  mutate(denial_reason.1 = recode(denial_reason.1, \n                                  \"1\" = \"Debt-to-income ratio\",\n                                  \"2\" = \"Employment history\",\n                                  \"3\" = \"Credit history\",\n                                  \"4\" = \"Collateral\",\n                                  \"5\" = \"Insufficient cash (downpayment, closing costs)\",\n                                  \"6\" = \"Unverifiable information\",\n                                  \"7\" = \"Credit application incomplete\",\n                                  \"8\" = \"Mortgage insurance denied\",\n                                  \"9\" = \"Other\",\n                                  \"10\" = \"Not applicable\",\n                               .default = NA_character_)) |&gt;\n  rowwise() |&gt;\n  mutate(denial_reason = case_when(any(!is.na(c_across(c(denial_reason.2,\n                                                         denial_reason.3,\n                                                         denial_reason.4)))) ~ \"Multiple\",\n      TRUE ~ denial_reason.1)) |&gt;\n  ungroup() |&gt;\n  mutate(loan_to_value_ratio = as.numeric(loan_to_value_ratio),\n         property_value = as.numeric(property_value)) |&gt;\n  mutate(debt_to_income_ratio = case_when(debt_to_income_ratio %in% c(\"36\", \"37\", \"38\",\"39\",\"40\",\"41\") ~ \"36%-41%\",\n                                          debt_to_income_ratio %in% c(\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\") ~ \"42%-49%\",\n                                          TRUE ~ debt_to_income_ratio)) |&gt;\n  mutate(debt_to_income_ratio = factor(debt_to_income_ratio, levels = c(\"Exempt\",\n                                                                        \"&lt;20%\",\n                                                                        \"20%-&lt;30%\",\n                                                                        \"30%-&lt;36%\",\n                                                                        \"36%-41%\",\n                                                                        \"42%-49%\",\n                                                                        \"50%-60%\",\n                                                                        \"&gt;60%\"))) |&gt;\n  select(-c(denial_reason.1, \n            denial_reason.2, \n            denial_reason.3, \n            denial_reason.4))\n\nWe’re going to start our analysis with univariate plotting. Specifically, we are going to produce data visualizations that count the number of observations in a dataset that fall into specific groupings. When grouping observations by a categorical variable, we will produce a bar plot. Remember when labeling that these plots visualize frequency. They use the height aesthetic to visualize us how many rows in a variable have a certain value.\nLet’s start by comparing how many home loans were for home purchases vs. the number for home improvements.\n\nQuestion\n\nIn the code below, determine the variable that should appear on the x-axis, along with the geom function that can be used to create a bar plot. Fill in the blanks to ensure your plot has the appropriate context.\n\n\nhmda_ms_2024 |&gt;\n  ggplot(aes(x = _____)) +\n  _____() +\n  labs(title = \"Home Mortgage Disclosure Act Applications, [FILL GEOGRAPHY], [FILL YEAR]\",\n       x = \"[FILL X-AXIS VARIABLE]\",\n       y = \"Count of [FILL WHAT's BEING COUNTED?]\") +\n  theme_minimal()\n\n\n\nNow let’s look at the number of applicants of each race.\n\n\n\nQuestion\n\nCreate a plot below that counts the number of applicants of each race. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. You can use the code you wrote above as a template to get you started. You’ll probably notice that the labels of your plot overlap. You can add + coord_flip() to the end of your code to flip the plot so that the bars are horizontal rather than vertical. This should help with legibility.\n\n\n# Create plot here\n\n\n\nIn the last plot, we visualized the frequency of a categorical variable. When grouping observations into intervals of a numeric variable, we will produce a histogram. For instance, let’s say we want to know the distribution of loan amounts that applicants requested in Mississippi in 2024. With a histogram, I can visualize how many loan applications requested $0-$100000, &gt;$100000-$200000, and so on. Histograms use the height aesthetic to visualize how many rows in a variable fall into each of these numeric bins.\n\n\n\nQuestion\n\nIn the code below, determine the variable that should appear on the x-axis, along with the geom function that can be used to create a histogram. Set the binwidth to create numeric intervals of 100000. Fill in the blanks to ensure your plot has the appropriate context.\n\n\nhmda_ms_2024  |&gt; \n  ggplot(aes(x = _____)) +\n  _____(binwidth = _____) +\n  labs(title = \"Home Mortgage Disclosure Act Applications, [FILL GEOGRAPHY], [FILL YEAR]\",\n       x = \"[FILL X-AXIS VARIABLE]\",\n       y = \"Count of [FILL WHAT's BEING COUNTED?]\",\n       caption = str_wrap(\"This histogram displays the distribution of home loan amounts requested by Mississippi applicants in 2024. The plot shows that the majority of applicants requested loans under $250,000. Since the average home loan size in the U.S. in 2024 was $252,505, this indicates that average requested home loan sizes in MS were smaller in comparison to the country as a whole.\", 120)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good plot caption does three things. First, it describes what the plot is and the variables displayed on it. Second, it articulates at least one fact from the plot. Third, it interprets the significance of that fact. The caption that I provided for the last plot follows this template:\n(describe) This histogram displays the distribution of home loan amounts requested by Mississippi applicants in 2024. (articulate fact) The plot shows that the majority of applicants requested loans under $250,000. (interpret significance) Since the average home loan size in the U.S. in 2024 was $252,505, this indicates that average requested home loan sizes in MS were smaller in comparison to the country as a whole.\n\n\nLoan to value ratio is a measure that indicates the percentage of a property’s value that a borrower requests of a lending institution. So let’s say I want to buy a property with a value of $100,000; I plan to put down $20,000, and I request a loan for $80,000. The loan to value ratio would be 80. Typically, when borrowers can put more down, it is considered a less risky loan.\n\n\n\nQuestion\n\nCreate a plot below that shows the distribution of loan to value ratios (in intervals of 10). Be sure to give it a descriptive title and labels covering all 5 essential components of data context. Also add a caption to describe the percentage of a loan’s property that borrowers most often request when applying for a loan.\nYou can use the code you wrote above as a template to get you started. Note that there were 20 outlier loans where borrowers requested more than double property’s value. There was even one loan where the applicant requested 14 times the property’s value. All of these loans were denied. Below I’ve given you some code to filter out these outliers in order to make the plot more legible.\n\n\nhmda_ms_2024 |&gt;\n  filter(loan_to_value_ratio &lt; 200) |&gt;\n# Fill remaining lines here\n\n\n\nLet’s move on to some multivariate plotting. Remember that we can add further variables to a plot via a number of different aesthetics (e.g. color: fill= or col=; size: size=; position: x= or y=, small multiples: + facet_wrap(vars(...)) ). Whenever we add further data to a plot, we should be on the lookout for overplotting.\n\n\n\nQuestion\n\nCreate a stacked barplot below that counts both the race of the applicants and the action taken in regards to the loan. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. I’ve given you some code below to move the legend to the bottom of the plot, to reduce the legend size, and to set the number of rows that appear in the legend.\n\n\nhmda_ms_2024 |&gt;\n# Fill remaining lines here\n  coord_flip() +\n  theme(legend.position = \"bottom\",\n        legend.text = element_text(size = 6), \n        legend.title = element_text(size = 8),\n        legend.key.size = unit(0.4, \"cm\")) +\n  guides(fill=guide_legend(nrow=4))\n\n\n\nWe learned earlier in this lab that there were more white applicants than applicants of other races applying for loans in Mississippi in 2024. If we only look at counts of approvals or denials then we don’t know if differences in approval numbers are due to disparities in who is getting loans or due to there just being more applicants of a particular race. For this reason, we are more interested in the percentage of approvals for each race and the percentage of denials for each race. This is more comparable across races than counts.\n\n\n\nQuestion\n\nCopy the plot that you created above into the code chunk below, and then adjust the position of the bars to “fill” in order to normalize their height. Add a caption that summarizes your findings.\n\n\n# Create plot here\n\n\n\nNow let’s move on to comparing how requested loan amounts differ across race. To do so, we are going to create a boxplot that compares summary statistics (i.e. minimum, maximum, median, 1st quartile, 3rd quartile, and outliers) of the loan amounts requested by applicants of each race.\n\n\n\nQuestion\n\nFill in the code below to create a boxplot that visualizes how the distributions of requested loan amounts differed across race. Add a caption that summarizes your findings.\n\n\nhmda_ms_2024 |&gt;\n  ggplot(aes(x = derived_race, __________)) +\n  __________ +\n  coord_flip() +\n  labs(title = \"Home Mortgage Disclosure Act Applications, Mississippi, 2024\",\n       x = \"Race of Applicant\",\n       y = _________,\n       caption = _________) +\n  theme_minimal() \n\n\n\nIn the past several exercises, we have been creating plots that count the number of rows in a variable have a certain value. …but what if our dataset presents aggregated data - where those counts have already been given to us? For instance, I wrote some code below to create a dataset where the counts of individuals of each race that were denied loans is already calculated, along with the percentage of individuals that were denied loans. I also created a dataset where the counts of individuals of each race and sex that were denied loans is already calculated, along with the percentage. You’ll learn more about these functions in a few weeks. Take a look at these datasets below, and consider how they compare them to the data that we started with.\n\nhmda_denial_by_race &lt;-\n  hmda_ms_2024 |&gt;\n  group_by(derived_race) |&gt;\n  summarize(total_denied = sum(action_taken == \"Application denied\"),\n            percent_denied = sum(action_taken == \"Application denied\")/n())\n\nhmda_denial_by_race_sex &lt;-\n  hmda_ms_2024 |&gt;\n  filter(derived_race != \"Free Form Text Only\") |&gt; #There were only 3 denials with free form text for race in MS in 2024, and they have been removed here for plot legibility\n  group_by(derived_race, derived_sex) |&gt;\n  summarize(total_denied = sum(action_taken == \"Application denied\"),\n            percent_denied = sum(action_taken == \"Application denied\")/n())\n\nhmda_denial_by_race\n\n\n  \n\n\nhmda_denial_by_race_sex\n\n\n  \n\n\n\n\n\n\nWhen counts have already been calculated for us, we need to use a different type of plot to visualize these counts. We don’t need the plot function to count the rows with each value; we just need it to set the height of the bar to a numeric value provided in our dataset. We will create column plots below to do this.\n\n\n\nQuestion\n\nIn the code below, fill in the blanks to set the height of each bar to the percent of individuals in a particular race whose mortgage was denied. Select a geom function to create a column chart. Be sure to fill in the correct labels.\n\n\nhmda_denial_by_race |&gt;\n  ggplot(aes(x = derived_race, _______)) +\n  _______ +\n  labs(title = \"Home Mortgage Disclosure Act Applications, Mississippi, 2024\",\n       x = \"Race of Applicant\",\n       y = \"___________\",\n       caption = str_wrap(\"This column plot visualizes the denial rates of home mortgage applications in MS in 2024. The plot shows that Black or African American applicants, American Indian or Alaska Native applicants, or applicants of 2 or more minority races were all denied mortgage at double the rate of White applicants. This may indicate disparities in lending practices.\", 120)) +\n  theme_minimal() +\n  coord_flip()\n\n\n\nWhile this gives us some insight into disparities in lending practices, we can’t see how multiple aspects of a person’s identity might factor in to these distinctions. The concept of intersectionality - introduced by civil rights scholar Kimberle Crenshaw in the late 1980s - considers how people of certain overlapping social identities may face unique forms of discrimination. Let’s take a look at how this data changes when we also consider sex in relation to denial rates.\n\n\n\nQuestion\n\nCopy your code above, replacing the hmda_denial_by_race data frame with hmda_denial_by_race_sex. Using the fill aesthetic, add the variable for describing the applicants’ sex to the plot. Set the position of the columns to \"dodge\", and be sure to add a label for the fill aesthetic. Adjust the caption to summarize your findings.\n\n\n# Create plot here\n\n\n\nIn reviewing data like this, some banks have explained the disparities by suggesting that certain groups of applicants are more likely to have lower credit scores, less cash for down payments, or more debt in relation to their income. Let’s take a look at the reasons that Mississippi banks denied mortgage loans in 2024.\n\n\n\nQuestion\n\nCreate a plot below that visualizes how many applicants of each race were denied mortgages for each denial reason. Be sure to give it a descriptive title and labels covering all 5 essential components of data context. You may wish to borrow some of your previous code to adjust the plot for legibility. I’ve given you some code below to filter the data frame to just those mortgage applications that were denied.\n\n\nhmda_ms_2024 |&gt;\n  filter(action_taken == \"Application denied\") |&gt;\n#Fill remaining lines here\n\n\n\nFrom this plot, you should learn that the top three reasons for denials were: 1) Multiple (meaning some combination of other denial reasons), 2) Credit history, and 3) Debt-to-income ratio. This dataset does not release information about applicants’ credit histories. However, we can take a look at the outcomes of applications from applicants with different debt-to-income ratios compared across races.\nDebt-to-income ratio refers to how much debt a person has in relation to the money they earn in income. A high debt-to-income ratio indicates that someone may not be pulling in enough income to pay off their debts, while a low debt-to-income ratio indicates that their income can reasonably help them pay off their debts. Banks are often hesitant to lend to individuals with high debt-to-income ratios.\n\n\n\nQuestion\n\nBelow I’ve given you some code to produce a plot that visualizes the percent of mortgage applications that resulted in each action (e.g. loan originated, denied, etc.), broken out by the debt-to-income ratio of the applicant. Referring to last week’s lab, add a line to this code to facet this plot by race. For which races was the denial rate higher for applicants with a lower than 50% debt-to-income ratio? Adjust the fact and interpretation presented in the caption with your response.\n\n\nhmda_ms_2024 |&gt;\n  ggplot(aes(x = debt_to_income_ratio, fill = action_taken)) +\n  geom_bar(position = \"fill\") +\n  coord_flip() +\n  #Add line to facet here\n  labs(title = \"Home Mortgage Disclosure Act Applications, Mississippi, 2024\",\n       x = \"Debt-to-Income Ratio\",\n       y = \"Percent of Mortgage Applications\",\n       fill = \"Action Taken\", \n       caption = str_wrap(\"This bar plot visualizes the percent of mortgage applications that resulted in each action, broken out by the debt-to-income ratio of the applicant. From this plot, we can see that most applicants with a debt-to-income ratio of 50% or higher were denied loans, and most applicants with a debt-to-income ratio of lower than 50% had their loans originated. This shows that banks are less likely to lend to applicants with higher debt-to-income ratios.\",width = 120)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.text = element_text(size = 8), \n        legend.title = element_text(size = 8),\n        legend.key.size = unit(0.4, \"cm\"),\n        plot.caption = element_text(size = 8)) +\n  guides(fill=guide_legend(nrow = 3))\n\n\n\n\n\n\n\n\n\n\nFinally, let’s take a look at differences in what borrowers owe when their mortgages are approved. A number of factors influence the interest rates that borrowers pay on their loans (including credit history, income, debt-to-income ratio, and loan term). Higher debt-to-income ratios often result in higher interest rates.\n\n\n\nQuestion\n\nBelow I’ve given you some code to produce a plot that visualizes how the distributions of interest rates differed across debt-to-income ratios when loans were approved. Referring to last week’s lab, add a line to this code to facet this plot by race. For which races were the interest rates higher even with lower debt-to-income ratios? Adjust the fact and interpretation presented in the caption with your response.\n\n\nhmda_ms_2024 |&gt;\n  filter(action_taken == \"Loan originated\") |&gt;\n  ggplot(aes(x = debt_to_income_ratio, y = interest_rate)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(title = \"Home Mortgage Disclosure Act Applications, Mississippi, 2024\",\n       x = \"Debt-to-Income Ratio\",\n       y = \"Interest Rate\",\n       caption = str_wrap(\"These boxplots visualize the distribution of interest rates for loans originated in MS in 2024, broken out by the debt-to-income ratio of the applicant. From this plot, we can see that the median interest rate for borrowers with a debt-to-income ratio of under 50% was around 7%. Median interest rates increased for borrowers with a debt-to-income ratio of over 50%. This shows that a high debt-to-income ratio may be one factor contributing to higher interest rates.\",width = 120)) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nThis dataset makes loan-level data about individual sensitive financial information available to the public. To reduce the risk that borrowers may be identified in the data, the CFPB removes certain variables before publication, such as the loan’s unique identifier, the property address, the applicant’s credit score, and any free-form text that the applicant writes on the application. Applicants do not have the option to opt-out of their data being disclosed in this dataset. If it were possible to re-identify individual applicants in this dataset, what would be the risks to the public disclosure of their information? Which applicants would be the most vulnerable? How should we think about the balance between the social value of this public data disclosure and the possible risks to those represented in it? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "infrastructure-setup.html",
    "href": "infrastructure-setup.html",
    "title": "Course Infrastructure Setup",
    "section": "",
    "text": "Tip\n\n\n\nIf at any point in this set-up you get stuck, there are many ways you can get help. I encourage you ask questions in Slack in the #sds-192-questions channel. If you use this resource, please be specific in regards to which step you are stuck on, and include screenshots in your question. You may also get help in office hours or at the Spinelli Center.\n\n\n\nSetting up your R Environment\n\nFollow the instructions in chapters 4-6 at https://happygitwithr.com/ to create a GitHub account, install R and RStudio if you haven’t already done so.\nOpen RStudio.\nCheck to ensure you have git installed by opening RStudio and clicking on Tools &gt; Global Options &gt; Git/SVN tab. Check the top of the pane for a field for the Git executable. It should say something like: “/usr/bin/git” or “C:/Program Files/Git/”. If it says “(Not Found)” git is not installed.\nIf git is not installed, follow instructions in chapter 7 at https://happygitwithr.com/\nEdit your git config by following these instructions:\n\n\nEdit gitconfig file from David Keyes.\n\n\nCreate a personal access token by following these instructions, and be absolutely, positively, undeniably sure to set the expiration to 150 days:\n\n\nCreate a Personal Access Token (PAT) on GitHub from David Keyes.\n\n\nStore your personal access token by following these instructions:\n\n\nStore Personal Access Token to Connect RStudio and GitHub from David Keyes.\n\n\nCopy the URL for your SDS 192 Lab GitHub repo. It should look like this: https://github.com/SDS-192-Intro/sds-192-lab-submissions-YOUR_USER_NAME\nCreate a project in RStudio from this GitHub repo by following the instructions starting at 26 seconds of this video:\n\n\nHow to Connect RStudio Projects with GitHub Repositories: GitHub First from David Keyes.\n\n\nInstall the rmarkdown package in RStudio by entering the following lines of code in the Console Pane of RStudio (lower left hand corer on initial install).\n\ninstall.packages(\"rmarkdown\")\nlibrary(rmarkdown)\n\n\n\nRStudio Panels\n\n\n\nOn the initial install, the Files tab will be in the lower right hand corner of RStudio. Open README.md, and then add your name to the body of the README file. Click Save.\nCommit your code by following the instructions in this video starting at 14 seconds:\n\n\nMake a Commit and View More History from David Keyes.\n\n\nPush your code to GitHub.com by following the instructions in this video starting at 27 seconds:\n\n\nGeneral Workflow: Push from David Keyes.\n\n\nNavigate to https://github.com/SDS-192-Intro/sds-192-lab-submissions-YOUR_USER_NAME and double check to make sure the README file has your name listed."
  },
  {
    "objectID": "assignments.html#course-infrastructure-set-up-2",
    "href": "assignments.html#course-infrastructure-set-up-2",
    "title": "Assignments",
    "section": "Course Infrastructure Set-up (2%)",
    "text": "Course Infrastructure Set-up (2%)\nIn this course, you will submit most assignments via GitHub. The course infrastructure set-up will help you configure RStudio to easily push assignments to GitHub. This assignment is graded on completion."
  },
  {
    "objectID": "labs/lab10a.html",
    "href": "labs/lab10a.html",
    "title": "Lab 10: APIs",
    "section": "",
    "text": "In this lab, we will write queries to access subsets of a very large dataset on the NYC Open Data Portal. We will practice all of the standards we have learned in the course so far in visualizing and wrangling the resulting data.\n\n\n\nWrite API Queries\nRecognize different HTTP Response Codes"
  },
  {
    "objectID": "labs/lab10a.html#introduction",
    "href": "labs/lab10a.html#introduction",
    "title": "Lab 10: APIs",
    "section": "",
    "text": "In this lab, we will write queries to access subsets of a very large dataset on the NYC Open Data Portal. We will practice all of the standards we have learned in the course so far in visualizing and wrangling the resulting data.\n\n\n\nWrite API Queries\nRecognize different HTTP Response Codes"
  },
  {
    "objectID": "labs/lab10a.html#review-of-key-terms",
    "href": "labs/lab10a.html#review-of-key-terms",
    "title": "Lab 10: APIs",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this reference guide when completing this lab.\n\n\n\nApplication Programming Interface (API)\n\nan mechanism that allows programmers or other systems (or users) to access resources from or post data to an online data service\n\nHTTP\n\na Web protocol for transmitting documents (e.g. Web pages, images, datasets, etc.) between web servers and browsers\n\nEndpoint\n\nAn exposed portion of an online data service that clients can assess via API queries"
  },
  {
    "objectID": "labs/lab10a.html#nyc-311-service-requests",
    "href": "labs/lab10a.html#nyc-311-service-requests",
    "title": "Lab 10: APIs",
    "section": "NYC 311 Service Requests",
    "text": "NYC 311 Service Requests\n311 is a dialing code set aside in many cities across the US to field calls about requests for municipal service – fixing potholes, reporting noise complaints, reporting parking infractions, etc. NYC has one of the most comprehensive 311 programs in the country, fielding calls to be routed to one of the city’s 77 agencies. Unlike in many other cities, New Yorkers can call 311 to report sexual harassment in taxi cabs or to report tenant rights issues (like a landlord failing to address a lack of heat or hot water). In October 2011, data documenting each request for service New Yorkers had made to 311 began to be published daily to the city’s public-facing open data portal. Since then, the dataset has become one of the largest and most complex archived and maintained on the portal, which includes datasets covering topics from restaurant health inspections, to crime statistics, to construction permits. Each row in the 311 dataset details one anonymized and geocoded request - the type of request that was made, when it was made, the location where the incident occurred, and how the relevant NYC agency responded. As of November 2022, the dataset has over 31 million rows, representing service requests made to 311 since 2010.\nWhen the 311 program was introduced, it was celebrated for its bottom-up approach to producing empirical evidence about problems facing New Yorkers; the program buttressed a persuasive techno-liberal imaginary, suggesting that fair and unbiased representation of quality of life concerns would emanate through the crowd-sourced data. When wrangled and visualized, the data can produce persuasive narratives about the state of equity and quality of life in communities across NYC, and thus a number of communities now leverage the data to legitimate claims. Community boards regularly cite 311 statistics about noise, infrastructure, and rodent issues when preparing district budget requests. Activists present the data to city legislators when advocating for or against new housing and transportation laws. Cyclists have produced apps that track complaints about the blocked bicycle lanes in New York. Journalists regularly reference the dataset to report which communities have received the most noise complaints, have had the most restaurants not following Covid-19 orders, or have had the most rat sightings. In 2018, eight pieces of legislation introduced by City Council required data about various issues – from rodent complaints, to noise complaints, to complaints about sexual harassment in taxis – be reported to the city agencies responsible for addressing those issues.\nYet, while representatives acknowledge 311 data to be a useful form of evidence for quality of life issues, they can be hesitant to rely on 311 statistics alone to measure urban problems. Communities that regularly leverage 311 open data warn that the data does not “represent” the saturation of problems in the city but that they represent “where people are complaining in the city.”\nMapping the data shows that, for almost all reported issues, 311 complaints are disproportionately high in gentrifying communities. The data under-represents problems in areas where individuals do not know to report, do not have the capacity to report, or fear harassment (from their landlords or bosses) for reporting. Complaints about NYC Housing Authority (NYCHA) are entirely excluded from the data. The data also over-represents problems when weaponized against minority communities and small businesses to consistently report minor and sometimes false noise issues or legal infractions. While this highlights how the dataset is being used to propagate inequity across the city, activists have also recognized that they can leverage 311 in liberatory ways. I’ve interviewed tenant rights groups that will run 311 calling campaigns in housing complexes, which can significantly increase the number of complaints made at a particular address - not necessarily because it is experiencing more issues than the building next to it, but because there has been advocacy around reporting the issues at this location."
  },
  {
    "objectID": "labs/lab10a.html#setting-up-your-environment",
    "href": "labs/lab10a.html#setting-up-your-environment",
    "title": "Lab 10: APIs",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the httr package.\nRun the code below to load the packages for today’s lab\n\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)\nlibrary(tigris)\n\n\nfood_atlas_url &lt;-\"https://ers.usda.gov/sites/default/files/_laserfiche/DataFiles/80591/FoodAccessResearchAtlasData2019.xlsx?v=95432\"\ntemp &lt;- tempfile()\ndownload.file(food_atlas_url, temp)\nfood_atlas &lt;- read_excel(temp, sheet=3)\nnm_tracts &lt;- tracts(state = \"NM\", year = 2019)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nfood_atlas_nm &lt;- nm_tracts |&gt; left_join(food_atlas, by = c(\"GEOID\" = \"CensusTract\"))\n\nfood_atlas_nm |&gt; st_crs()\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n#leaflet is 4326\n\n\n\nnm_map &lt;- leaflet(width = \"100%\") %&gt;%\n  setView(lat = 34, lng = -105.0324, zoom = 6) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron)\n\n\n\ncreate_chloro_map_num &lt;- function(df, var, breaks) {\n  pal_num &lt;- colorNumeric(palette=\"YlOrRd\", \n                        domain = df$var)\n  \n  nm_map |&gt;\n  addPolygons(data = df,\n              weight = 0.5,\n              color = \"black\",\n              fillColor = ~pal_bin(var), \n              fillOpacity = 0.5)\n\n  \n}\ncreate_chloro_map_bin &lt;- function(df, var, breaks) {\n  pal_bin &lt;- colorBin(palette=\"YlOrRd\", \n                    domain = df$var,\n                    bins = breaks)\n    nm_map |&gt;\n  addPolygons(data = df,\n              weight = 0.5,\n              color = \"black\",\n              fillColor = ~pal_bin(var), \n              fillOpacity = 0.5)\n}\ncreate_chloro_map_quant &lt;- function(df, vari, num) {\n    pal_quant &lt;- colorQuantile(palette=\"YlOrRd\", \n                               domain = df[[vari]],\n                               n = num)\n  nm_map |&gt;\n  addPolygons(data = df,\n              weight = 0.5,\n              color = \"black\",\n              fillColor = ~pal_quant(df[[vari]]), \n              fillOpacity = 0.5)\n\n}\n\ncreate_chloro_map_quant(food_atlas_nm, \"PovertyRate\", 4)"
  },
  {
    "objectID": "labs/lab9.html#toxic-release-inventory",
    "href": "labs/lab9.html#toxic-release-inventory",
    "title": "Lab 9: Mapping with Leaflet",
    "section": "Toxic Release Inventory",
    "text": "Toxic Release Inventory\nThe EPA’s Emergency Planning and Community Right to Know Act (EPCRA) of 1986 established the Toxic Release Inventory as a mechanism to monitor and inform the public of toxic emissions released in their communities. Every year, certain U.S. industrial facilities are required to report to the EPA the amounts of certain chemical on-site and off-site releases in pounds. Facilities required to report include those that employ more than 10 individuals, release more than a certain threshold of a TRI-regulated chemical, and are classified by a specified set of Standard Industrial Codes, including mining, utilities, manufacturing, publishing, and hazardous waste.\n\n\nToday, we are going to examine TRI data from 2023 in Louisiana’s Cancer Alley. Cancer Alley is an 85-mile stretch of land along the Mississippi River where more than 200 fossil fuel and petrochemical operate. It is an area of the country facing some of the highest cancer risks in the United States, and it is inhabited predominantly by Black communities, raising significant concerns about environmental injustices. Next week, we are going to look at the demographics of the community’s in cancer alley and consider its racial history. This week, we are going to focus on chemical emissions in this area."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Designing effective data visualizations involves reviewing available data and then determining how best to map variables in the data onto a variety of visual cues. When we refer to visual cues, we are referring to those visual components of the plot that help us discern differences across data points. For instance, a plot might use different shapes or colors to represent different categories of data. A plot might also place points at different positions or bars at different heights to represent different numeric values. This week we will practice mapping variables in a dataset onto different plot aesthetics in order to tell different stories with the data. We will only be creating one type of plot today - a scatterplot. However, we are going to show how we can use different visual cues to plot a number of different variables onto a scatterplot.\n\n\n\nRead the ggplot cheatsheets\nMap variables onto plot aeshetics\nAdjust the attributes of a plot\nAdjust the scales of aeshetics on plots\nDeal with overplotting\nFacet plots into small multiples"
  },
  {
    "objectID": "labs/lab2.html#introduction",
    "href": "labs/lab2.html#introduction",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "",
    "text": "Designing effective data visualizations involves reviewing available data and then determining how best to map variables in the data onto a variety of visual cues. When we refer to visual cues, we are referring to those visual components of the plot that help us discern differences across data points. For instance, a plot might use different shapes or colors to represent different categories of data. A plot might also place points at different positions or bars at different heights to represent different numeric values. This week we will practice mapping variables in a dataset onto different plot aesthetics in order to tell different stories with the data. We will only be creating one type of plot today - a scatterplot. However, we are going to show how we can use different visual cues to plot a number of different variables onto a scatterplot.\n\n\n\nRead the ggplot cheatsheets\nMap variables onto plot aeshetics\nAdjust the attributes of a plot\nAdjust the scales of aeshetics on plots\nDeal with overplotting\nFacet plots into small multiples"
  },
  {
    "objectID": "labs/lab2.html#review-of-key-terms",
    "href": "labs/lab2.html#review-of-key-terms",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\nAesthetics\n\nVisual cues that we map variables in a dataset onto\n\nCartesian grid\n\nA 2-dimensional grid with an intersecting x and y axis\n\nSequential color\n\nA uni-directional ordering of shades\n\nQualitative color\n\nA discrete set of colors\n\nDivergent color\n\nA diverging ordering of color shades\n\nOverplotting\n\nInstances when visual representations of individual data points overlap on a plot making aspects of the plot illegible"
  },
  {
    "objectID": "labs/lab2.html#national-bridge-inventory-dataset",
    "href": "labs/lab2.html#national-bridge-inventory-dataset",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "National Bridge Inventory Dataset",
    "text": "National Bridge Inventory Dataset\nEvery year the U.S. Federal Highway Administration publishes a dataset listing every federally-regulated bridge and tunnel in the U.S., along with its location, design features, operational conditions, and inspection ratings. The data is used to review the safety of these transportation infrastructures for the traveling public. As you can imagine, for politicians promising to the improve the state of transportation infrastructure, this dataset is integral to determining where to allocate improvement funds:\n\n\nToday, we are going to look at a subset of 2024 NBI data for Massachusetts and for Hampshire County, MA, and we are going to focus solely on highway bridges (excluding pedestrian and railroad bridges). We’re going to look at what kinds of variables might contribute to poor bridge conditions, where there are poor bridge conditions, and which entities are responsible for maintaining them. The data documentation for this dataset is quite thick, so I will provide you with a data dictionary for today.\n\n\n\n\n\n\n\nVARIABLE NAME\nDESCRIPTION\n\n\n\n\nSTRUCTURE_NUMBER_008\nUnique ID for the bridge\n\n\nCOUNTY_CODE_003_L\nName of the county where the bridge is located\n\n\nROUTE_PREFIX_005B_L\nRoute signing prefix for the inventory route\n\n\nMAINTENANCE_021_L\nThe actual name(s) of the agency(s) responsible for the maintenance of the structure\n\n\nYEAR_BUILT_027\nThe year of construction of the structure\n\n\nADT_029\nThe average daily traffic volume for the inventory route based on most recent available data\n\n\nSTRUCTURE_KIND_043A_L\nThe kind of material and/or design of the structure\n\n\nSTRUCTURAL_EVAL_067\nA rating of the structural evaluation of the bridge based on inspections of its main structures, substructures, and/or its load ratings\n\n\nBRIDGE_IMP_COST_094\nEstimated costs for bridge improvements"
  },
  {
    "objectID": "labs/lab2.html#setting-up-your-environment",
    "href": "labs/lab2.html#setting-up-your-environment",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the RColorBrewer package by entering the following into your R Console: install.packages(\"RColorBrewer\")\nRun the code below to the import the bridge inventory for Massachusetts and for Hampshire County into R. Call me or one of the data assistants over if you get an error.\n\n\nlibrary(tidyverse)\nlibrary(RColorBrewer)\ncounties &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_counties.csv\")\nroute_prefixes &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_route_pre.csv\")\nmaintenance &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_maintenance.csv\")\nkinds &lt;- read_csv(\"https://raw.githubusercontent.com/sds-192-intro-fall22/sds-192-public-website-quarto/a8b64e3070ca2543b904d4d92780b09e6062ced6/website/data/nbi_kind.csv\")\n\nnbi_ma &lt;- read.delim(\"https://www.fhwa.dot.gov/bridge/nbi/2024/delimited/MA24.txt\", sep = \",\") |&gt;\n  left_join(counties) |&gt;\n  left_join(route_prefixes) |&gt;\n  left_join(maintenance) |&gt;\n  left_join(kinds) |&gt;\n  filter(SERVICE_ON_042A == 1) |&gt;\n  select(STRUCTURE_NUMBER_008, COUNTY_CODE_003_L, ROUTE_PREFIX_005B_L, MAINTENANCE_021_L, YEAR_BUILT_027, ADT_029, STRUCTURE_KIND_043A_L, STRUCTURAL_EVAL_067, BRIDGE_IMP_COST_094) |&gt;\n  mutate(STRUCTURE_KIND_043A_L = \n           case_when(\n             STRUCTURE_KIND_043A_L == \"Concrete continuous\" ~ \"Concrete\",\n             STRUCTURE_KIND_043A_L == \"Steel continuous\" ~ \"Steel\",\n             STRUCTURE_KIND_043A_L == \"Prestressed concrete continuous\" ~ \"Prestressed concrete\",\n             TRUE ~ STRUCTURE_KIND_043A_L)) |&gt;\n  mutate(BRIDGE_IMP_COST_094 = BRIDGE_IMP_COST_094 * 1000)\n\nnbi_hampshire &lt;- nbi_ma |&gt; filter(COUNTY_CODE_003_L == \"Hampshire\")\n\nrm(counties, kinds, maintenance, route_prefixes)"
  },
  {
    "objectID": "labs/lab2.html#visualization-aesthetics",
    "href": "labs/lab2.html#visualization-aesthetics",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Visualization Aesthetics",
    "text": "Visualization Aesthetics\nThe visual cues that we select to display on a plot largely depend on the kind of data that we have available. Last week we discussed the differences between categorical variables and numeric variables. Certain types of visual cues are more suited for representing categorical variables, while other types of visual cues are more suited for representing numeric variables. For instance, we wouldn’t use different shapes to represent different numeric values because there is no obvious ordering to a group of shapes (e.g. a triangle isn’t necessarily larger or greater than a circle; they’re just different). Because of this, shapes are much more appropriate for representing nominal categorical variables. Size is a much more effective visual cue for numeric variables because size can increase as the values in the data increase.\n\n\n\n\n\n\n\n\nCue\nEffective for what kinds of variables?\nExample where applied?\n\n\n\n\nShape\nCategorical\nPoints on scatterplots\n\n\nSize\nNumeric\nPoints on scatterplots\n\n\nArea\nNumeric\nBars in bar plots\n\n\nColor\nCategorical (Qualitative palette)\nNumeric (Sequential/Divergent)\nPoints on scatterplots;\nBars in bar plots;\nLines in a line plot\n\n\nPosition\nCategorical; Numeric\nPoints on scatterplots\n\n\nAngle\nNumeric\nSlices in pie chart\n\n\n\nYou’ll remember from lecture that we map variables onto these visual cues via the aesthetic function (aes()) in ggplot().\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this ggplot cheatsheet when completing this lab. Note how this cheatsheet is organized. There are headings for things like:\n\nBasics\nGeom functions\nScales\nCoordinate systems\n\nThe first tip to find what you’re looking for is to consider what heading the graphic element will likely fall under. Also note that, for many functions on this cheatsheet:\n\nan image is provided for how the plot will transform,\nthe function and its arguments are referenced in bold, and\na text description of what will happen when you apply the function is provided\n\nFinally note that most of these functions listed on this cheatsheet are appended to the ggplot() object with a + sign.\n\n\n\nPosition\nSince today we will be working with scatterplots, let’s start by talking about position. When we refer to position, we refer to the location of a point on a Cartesian plane (i.e. its position on an x-axis and its position on a y-axis). We can create scatterplots by mapping a variable in our dataset onto the x-axis and another variable onto the y-axis (aes(x = VARIABLE_NAME, y = VARIABLE_NAME)). Let’s take a look at what happens when we map the year a bridge was built onto the x aesthetic and the bridge’s structural evaluation onto the y aesthetic for all Hampshire County Massachusetts highway bridges.\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nEach point on the plot corresponds to one observation (row) in the dataset. Since each row in this dataset is one Hampshire County, MA highway bridge, each point on this plot also represents one Hampshire County, MA highway bridge. The position of the point indicates to us the year that bridge was built and its structural evaluation. Zooming out to look at all of this data we can see that newer bridges tend to have higher structural evaluations.\n\nOverplotting\nYou’ll notice that parts of this plot can be challenging to read because some points overlap each other making it hard to distinguish one from the next. This is an example of overplotting, and there are a number of strategies we can take to addressing it. Most of these strategies involve revising attributes of the points on the plot (e.g. the size, transparency, and position of the points).\nNote that attributes are different than aesthetics. Recall that when we assign aesthetics in a plot, we are adjusting the visual cues on a plot according to the values in a variable. The visual cues will be different based on the values in that variable (e.g. size will be greater with greater values). On the other hand, when we adjust attributes, we are adjusting the visual cues on a plot in a fixed way (i.e. every point will be styled in the same way regardless of its value). Because of this attributes are assigned outside of the aes() (aesthetic) function. Let’s adjust the plot we created above by reducing the size of every point (size =), reducing the transparency of every point (alpha =), and adding \"jitter\" to the plot (position=). Recall that alpha is assigned between 0 and 1: 0 being transparent and 1 being opaque. Jitter means that we add a bit of random noise to the points on the plot in order to prevent overlapping points.\n\nQuestion\n\nIn the code below, adjust the size to 2 and the alpha to 0.5. Set the position to \"jitter\". Note how this changes the plot.\n\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 1, position = \"identity\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nColor\nColor can be used on plots to either distinguish between discrete values in a categorical variable or to represent the range of values in numeric variable. We use different kinds of color palettes for each of these scenarios. Palettes refer to a range of colors. We can have palettes with discrete colors (e.g. red, orange, and blue) or palettes with a gradient of colors (e.g. lightest red to darkest red).\n\n\n\n\n\n\n\n\nPalette\nEffective for what kinds of variables?\nExample\n\n\n\n\nQualitative\nCategorical\nRed, yellow, blue\n\n\nSequential\nNumeric\nLight blue to dark blue\n\n\nDivergent\nNumeric, extending in two directions (e.g. &gt;1 and &lt;1)\nBlue to purple to red\n\n\n\n\nQuestion\n\nIn the plot below, color the points on the plot with a qualitative palette by setting col= to the categorical variable ROUTE_PREFIX_005B_L. Remember that, in this case, color is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting col= in the labs() function to a phrase that describes the color variable.\n\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nCopy the completed plot from the last exercise below but swap out the variable you mapped to the color aesthetic with a numeric variable from the dataset. Be sure to also adjust the legend label. What kind of palette gets created? What new information do you gain about Hampshire County bridges from the plot? Note your interpretations in your quarto file.\n\n\n\n\nShape\nA different way to differentiate data on a plot is to map the shape aesthetic onto the plot. In this case, rather than all observations in the dataset appearing as points on a plot, observations will appear as different shapes based on their associated values in a categorical variable.\n\nQuestion\n\nIn the plot below, assign the shape of the data on the plot by setting shape= to the categorical variable STRUCTURE_KIND_043A_L. Remember that, in this case, shape is an aesthetic, so this must be added inside of the aes() function. Add a label for the legend by setting shape= in the labs() function to a phrase that describes the shape variable.\n\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"ADD LEGED LABEL HERE\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nSize\nWhile we might map a categorical variable onto the the shape aesthetic, we can alternatively map a numeric variable onto the size aesthetic. For instance, note what we learn when we map the variable for average daily traffic onto the size aesthetic below.\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how I moved my legend position to the bottom using + theme(legend.position = \"bottom\") in the code above.\n\n\n\nQuestion\n\nCopy one of your plots from above, and inside the aes() function, set size= to a numeric variable not already represented on the plot. Keep in mind that you’ll need to remove the size= attribute from geom_point() for the points to size correctly. Be sure to also adjust the legend labels. What new information do you gain about Hampshire County bridges from the plot?\n\n\n# Copy and adjust plot here"
  },
  {
    "objectID": "labs/lab2.html#scales",
    "href": "labs/lab2.html#scales",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Scales",
    "text": "Scales\nWhen we map a variable onto an aesthetic, we are only indicating that the variable should be mapped. We are not indicating how the variable should be mapped. In order to indicate how we want a variable mapped to an aesthetic, we can adjust its scales. Scales are adjusted by tacking the following onto a ggplot() object: + scale_&lt;aesthetic&gt;_&lt;type&gt;(). For instance, let’s say that I wanted to adjust the scale of my x-axis to a log scale. I would attached + scale_x_log10() to my ggplot() object.\n\n\n\n\n\n\n\n\nScale\nDescription\nExample\n\n\n\n\nContinuous\nNumeric values are mapped along a continuum\n+ scale_y_continuous()\n\n\nDiscrete\nCategorical values are mapped into discrete buckets\n+ scale_color_discrete()\n\n\nBinned\nNumeric values are mapped into discrete bins\n+ scale_size_binned()\n\n\nLog\nNumeric values are mapped logarthmically\n+ scale_y_log10()\n\n\nDate-Time\nNumeric values are mapped along a timeline\n+ scale_x_datetime()\n\n\n\nLet’s talk about how we would adjust the scales for each of the aesthetics we’ve covered so far.\n\nPosition\nWe can adjust the scale of our x and y-axes by adding + scale_x_&lt;type&gt;() or + scale_y_&lt;type&gt;() to our plots. Note what happens when we attempt to create a scatterplot that shows the relationship between the year a bridge was built and the bridge improvement costs for all MA highway bridges.\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nDue to huge disparities in costs for bridge improvements, this plot is difficult to interpret. Most bridge improvement costs are under $10,000,000, but with at least one bridge with costs just under a $1,000,000,000, the vast majority of the points on the plot appear at the very bottom of the y-axis scale and are largely indiscernible from one another. This is a case when it makes sense to apply a log scale to the y-axis.\n\nQuestion\n\nIn the plot below, change the y-axis scale from continuous to log10. You might reference the formula specified above or reference the ggplot cheatsheet for help. Note that your y-axis will appear in scientific notation. Convert it to comma notation by adding: labels = scales::comma as an argument in the scale function call.\n\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = BRIDGE_IMP_COST_094)) +\n  geom_point(size = 1, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Bridge Improvement Costs\") +\n  theme_minimal() + \n  scale_y_continuous(labels = scales::comma)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nSometimes I might wish to group certain numeric values into bins on a plot. For instance, let’s say I just want to see how many bridges are structurally deficient in comparison to bridges that are operationally sound. Typically a bridge is considered structurally deficient when it scores 4 or lower. So I want to group the numeric values in STRUCTURAL_EVAL_067 into two bins: 0-4 and 4-9. To do this, I will set the scale to: + scale_y_binned() and add an argument to establish the bin breaks: breaks = c(4, 9) as well as an argument to label the bin breaks: labels = c(\"Structurally Deficent\", \"No Deficiencies\"). Check out what happens to the y-axis scale when I do this below.\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() + \n  scale_y_binned(breaks = c(4, 9), labels = c(\"Structurally Deficent\", \"No Deficiencies\"))\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nThe Interstate Highway Act of 1956 (signed under Dwight D. Eisenhower) established the U.S.’s interstate highway system and is considered the country’s largest public works project. Adjust the x-axis scale of the plot below to help tell this story. Specifically, bin years into before 1954 (when Eisenhower was elected) and 1954-2021. Set the label at 1954 to “Eisenhower Elected” and the label at 2021 to “Today.”\n\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 1, alpha = 0.2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nColor\nWe’ve already talked about how both categorical and numeric variables can be mapped to the color aesthetic. However, sometimes, we want to be able to further customize which colors should appear on the plot and how they appear on the plot. The RColorBrewer package, which you installed earlier in the lab includes a number of palettes for coloring points on a plot. Check them out below:\n\nRColorBrewer::display.brewer.all() \n\n\n\n\n\n\n\n\nNote how the first set of palettes is sequential, the second set is categorical, and the third set is divergent. We can assign these palettes to our plots using one of two functions: + scale_color_brewer() for categorical data and + scale_color_distiller() for numeric data. Within this function, we can set the argument palette equal to one of the palettes specified in the image above.\n\nQuestion\n\nAdd `+ scale_color_brewer()` to the ggplot() object below and assign a categorical palette. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ROUTE_PREFIX_005B_L)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Massachusetts, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Route Prefix\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nQuestion\n\nAdd `+ scale_color_distiller()` to the ggplot() object below and assign a sequential palette. By default the colors will be ordered from darkest to lightest. Add an argument to the function call to reverse the direction of the colors. Refer to the ggplot cheatsheet or the help pages for help with formatting the function call.\n\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067,\n             col = ADT_029)) +\n  geom_point(size = 2, alpha = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"Average Daily Traffic\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nSize and Shape\nThe following represents the values associated with ggplot() point shapes.\n\nWe can manually assign shapes using + scale_shape_manual(values = c(&lt;shape_values&gt;)).\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             shape = MAINTENANCE_021_L)) +\n  geom_point(alpha = 0.5, size = 2, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       shape = \"Maintainer\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        legend.box=\"vertical\") +\n  guides(shape = guide_legend(nrow = 3, byrow = TRUE)) +\n  scale_shape_manual(values = c(15:17))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the guides() function above allows us to wrap the legend into three rows!\n\n\nAlso note how we bin point sizes just like we binned values on the x and y axis.\n\nnbi_hampshire |&gt;\n  ggplot(aes(x = ROUTE_PREFIX_005B_L, \n             y = STRUCTURAL_EVAL_067, \n             size = ADT_029)) +\n  geom_point(alpha = 0.5, position = \"jitter\") +\n  coord_flip() +\n  labs(title = \"Highway Bridge Conditions, Hampshire County, MA, 2024\",\n       x = \"Route Prefix\", \n       y = \"Structural Evaluation\",\n       size = \"Average Daily Traffic\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_size_binned(breaks = c(0, 500, 5000, 100000))"
  },
  {
    "objectID": "labs/lab2.html#faceting",
    "href": "labs/lab2.html#faceting",
    "title": "Lab 2: Visualization Aesthetics",
    "section": "Faceting",
    "text": "Faceting\nFaceting involves breaking out a single plot into multiple smaller plots based on the value in a variable. Faceting is very helpful when we have a categorical variable with many distinct values. If we tried to color by that variable, the colors would likely be indistinguishable from one another. For instance, check out what happens when we try to color by the COUNTY_CODE_003_L variable below.\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067, \n             col = COUNTY_CODE_003_L)) +\n  geom_point(alpha = 0.5, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\",\n       col = \"County\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are so many counties that it’s extremely challenging to distinguish between colors on this plot. In this case, instead of using a color aesthetic, we facet the plot by adding + facet_wrap(vars(COUNTY_CODE_003_L)) to the ggplot() object. Check out what happens when we do that below.\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA, 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal() +\n  facet_wrap(vars(COUNTY_CODE_003_L))\n\n\n\n\n\n\n\n\n\nQuestion\n\nFacet the plot below by STRUCTURE_KIND_043A_L to see the degree of structural deficiencies for different kinds of highway bridges of different ages in Massachusetts.\n\n\nnbi_ma |&gt;\n  ggplot(aes(x = YEAR_BUILT_027, \n             y = STRUCTURAL_EVAL_067)) +\n  geom_point(alpha = 0.1, size = 0.5, position = \"jitter\") +\n  labs(title = \"Highway Bridge Conditions, MA 2024\",\n       x = \"Year Built\", \n       y = \"Structural Evaluation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nFederal regulations require that highway bridges and public road bridges in the United States must be inspected every two years, and bridge inspectors must follow a strict set of guidelines when performing a bridge inspection. If a bridge is discovered to have certain structural deficiencies, it may be shut down or may require more frequent inspections. Despite these widespread safety standards, we still sometimes see catastrophic bridge failures or discover critical bridge conditions that could potentially put lives at risk. This could result from human error, low vigilance or attention to detail during inspections, lack of understanding/adherence to inspection standards, or inspection equipment malfunctions. What kinds of interventions do you think should be put in place to ensure the accountability of bridge inspection data? What kinds of policies, practices, or programs could help to minimize mistakes or encourage that mistakes be fixed promptly? Share your ideas on our sds-192-discussions Slack channel."
  },
  {
    "objectID": "labs/lab10.html",
    "href": "labs/lab10.html",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "",
    "text": "This lab is an extension of last week’s lab on Point Mapping in Leaflet. In this lab, we will continue our research into Louisiana’s Cancer Alley, layering in consideration of the demographics of the communities surrounding polluting facilities. This will allow us to assess racial disparities in exposure to environmental toxics. In this lab, we will also consider how choices that we make in mapping can shape the stories that the maps tell.\n\n\n\nTransform polygon data to an appropriate CRS\nMap polygon data in Leaflet\nCreate palettes for polygons on maps\nAdjust layer controls on polygon maps\nRecognize some of the ways that maps can “lie”"
  },
  {
    "objectID": "labs/lab10.html#introduction",
    "href": "labs/lab10.html#introduction",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "",
    "text": "This lab is an extension of last week’s lab on Point Mapping in Leaflet. In this lab, we will continue our research into Louisiana’s Cancer Alley, layering in consideration of the demographics of the communities surrounding polluting facilities. This will allow us to assess racial disparities in exposure to environmental toxics. In this lab, we will also consider how choices that we make in mapping can shape the stories that the maps tell.\n\n\n\nTransform polygon data to an appropriate CRS\nMap polygon data in Leaflet\nCreate palettes for polygons on maps\nAdjust layer controls on polygon maps\nRecognize some of the ways that maps can “lie”"
  },
  {
    "objectID": "labs/lab10.html#review-of-key-terms",
    "href": "labs/lab10.html#review-of-key-terms",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this reference guide when completing this lab.\n\n\n\nEcological Fallacy\n\nA reasoning error that emerges when we assume that conclusions that we draw about a group also apply to individual members of that group\n\nModifiable Aerial Unit Problem\n\nA problem that occurs when representing spatial data; how we choose to draw administrative boundaries can bias our understanding of the relationships amongst spatial data points"
  },
  {
    "objectID": "labs/lab10.html#u.s.-census-data",
    "href": "labs/lab10.html#u.s.-census-data",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "U.S. Census Data",
    "text": "U.S. Census Data\nThe United States Census is survey of the population of the United States, taken every ten years. This data collection is mandated by the U.S. Constitution. There are countless stories that can be documented about the census. For example: - The census is one of the first examples of “big data” in the U.S.; too large to be computed by hand, special machines had to be developed to tabulate census data in the late 1800s. - The categories available for documenting race and ethnicity have shifted considerably over the past two centuries, mirroring changing perceptions of race in the U.S.\n- For more stories, you might consider reading (Bouk 2022) .\nThe census has a number of data tables available for public analysis. Today, we are just going to look at data documenting the population of Black residents in Louisiana, collected during the 2020 census."
  },
  {
    "objectID": "labs/lab10.html#setting-up-your-environment",
    "href": "labs/lab10.html#setting-up-your-environment",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the following packages in your Environment:\n\n\ninstall.packages(\"devtools\")\ninstall.packages(\"remotes\")\nremotes::install_github(\"walkerke/tidycensus\")\n\n\nCreate an API Key for accessing census data here. The key will be emailed to you, and you must activate it with the link you receive in your email. NOTE: It may take several minutes to activate. If you click on the link in your email, and it doesn’t work, try again in a bit.\nRun the following code in your terminal to store the census API key in your environment.\n\nlibrary(tidycensus)\ncensus_api_key(\"[ADD KEY HERE]\", overwrite = FALSE, install = TRUE)\n\nRestart R. Don’t skip this step!!\nRun the code below to load the packages for today’s lab.\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(RColorBrewer)"
  },
  {
    "objectID": "labs/lab10.html#projections",
    "href": "labs/lab10.html#projections",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Projections",
    "text": "Projections\nWhenever we are mapping data, we need to consider the map’s projection. Recall that there are infinite ways to flatten our spherical globe into a 2-dimensional map. The projection standard that we choose indicates how 3-dimensional geometric shapes are configured on our 2-dimensional map. By default, leaflet expects all latitude, longitude, and shape data to be input as EPSG:4326, and it projects all of that data according to EPSG:3857. All of the basetiles are also projected as EPSG:3857.\n…but what if a different standard were used to project data on the map. To demonstrate how a map of Louisiana counties might look different with a different projection, below, I’ve created a custom leaflet projection that projects data according to the US National Atlas Equal Area (EPSG:9311) system.\n\nepsg9311 &lt;- leafletCRS(\n  crsClass = \"L.Proj.CRS\",\n  code = \"EPSG:9311\",\n  proj4def = \"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\",\n  resolutions = 2 ^ (16:7)\n)\n\nHere’s how the map looks with leaflet’s default projection.\n\nplot_la_map &lt;- function(opts = leafletOptions()) {\n  leaflet(census_counties, options = opts) %&gt;%\n  setView(lat = 30.5191, lng = -91.5209, zoom = 6) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron, group = \"Minimal\") %&gt;%\n    addPolygons(\n      weight = 1,\n      color = \"black\",\n    )\n}\n\nplot_la_map()\n\n\n\n\n\nHere’s how the map looks when data is projected according to my custom projection. Be sure to Zoom out so you can see where the polygons appear on the map.\n\nplot_la_map(opts = leafletOptions(crs = epsg9311))\n\n\n\n\n\nHere I’ve told leaflet to project my data according to a projection standard that does not match the projection of my base tiles, which is why the data appears in the wrong location in relation to the base tiles. Additionally, you will notice that the orientation of the polygons on the map look a bit different than our default map. Keep in mind that each of these projection standards has different purposes. There is not a particular way to project data that is more “correct” than another way. However, when working in leaflet, it is important to have our data projected in the same way that our base tiles are projected so that we know where shapes are located on the map."
  },
  {
    "objectID": "labs/lab10.html#symbolization",
    "href": "labs/lab10.html#symbolization",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Symbolization",
    "text": "Symbolization\nHow we symbolize geographic features on a map can tell a different story about what is going on at a particular location. So far, we have been using a rather minimalist basemap that only shows major streets and major land features in Louisiana. Proximity to major highways and bodies of water can tell important story when it comes to pollution. However, geographic features that don’t appear on this minimalist map also tell an important story. For example, valleys nestled between mountain ranges can trap pollution, leading to poorer air quality in those areas. In the code below, we are going to create a basemap that let’s us toggle between two different ways of symbolizing geography.\nTo create the option to toggle between these maps, we will need to assign each version of the map to a “group.” We can then use addLayersControl() function to indicate that we want to allow users to toggle between these two groups.\n\nQuestion\n\nI’ve initialized a map of Louisiana for you below with our basic base tiles. Set the group for these tiles to “Minimal”. Note that we will call addProviderTiles() to add a second set of base tiles. Set these base tiles to providers$Esri.WorldImagery and the group to “Satellite”. Add the addLayersControl() function and set the base groups to the names of the two groups you just established.\nYour map should match mine below.\n\n\nla_map &lt;- leaflet(width = \"100%\") %&gt;%\n  setView(lat = 30.5191, lng = -91.5209, zoom = 6) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron, group = ______) %&gt;%\n  addProviderTiles(______, group = ______) %&gt;%\n  _____(\n    baseGroups = c(\"_____\",\"_____\"),\n    options = layersControlOptions(collapsed = FALSE)\n  )\n\nla_map\n\n\n\n\n\n\n\nZoom in to the region along the Mississippi River between Baton Rouge and New Orleans and then toggle to the Satellite map. What do you notice about the geographic features of that area that you didn’t notice with the minimalist basemap? Both show that Cancer Alley is situated along the Mississippi River. However, with the satellite map, you may notice that there are several long and skinny plots of land along the Mississippi River. This is giving us a view into the legacies of environmental racism in this space.\nPrior to this land being referred to as Cancer Alley, it was often referred to as “Plantation Country” as it was the location of several large sugarcane plantations. Plantations were set up in these long and skinny plots of land along the Mississippi River, making it possible to bring sugar to market. While you might find stately plantation houses facing the river, on the opposite side of these stretches of land, you would find the houses of the enslaved workers that were forced to work on these plantations. In the 1900s, following emancipation and the rising costs of sugarcane production, these plots became attractive to oil and gas companies looking to set up petrochemical refineries with access to a water source near the Gulf of Mexico. Descendants of the Black enslaved population that worked on the sugarcane plantations still live in these communities along the Mississippi River, making this a site of considerable environmental injustices. For more information see (Knowles and Rogers 2020)."
  },
  {
    "objectID": "labs/lab10.html#standardization",
    "href": "labs/lab10.html#standardization",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Standardization",
    "text": "Standardization\nNow that we have our basemaps, we can begin to visualize the data we have pulled regarding Louisiana’s Black population from the 2020 census. Below I’ve created the first version of that map for you. First I created a color palette spanning from light blue to dark blue, and binning Black population values into 5 buckets. Then I added polygons of Louisiana counties to the map and colored the counties according the palette that I created. Finally, I added a legend to explain the colors appearing on the map.\n\npal_bin_census_county_pop &lt;- colorBin(palette=\"Blues\", \n                    domain = census_counties$value,\n                    bins = 5)\n\nla_map %&gt;%\n  addPolygons(data = census_counties, \n              weight = 1,\n              color = \"black\",\n              fillColor = ~pal_bin_census_county_pop(value),\n              fillOpacity = 0.5,) %&gt;%\n  addLegend(data = census_counties,\n            title = \"Population Black Residents\", \n            pal = pal_bin_census_county_pop, \n            values = ~value)\n\n\n\n\n\nYou may notice that the colors appear a darker shade of blue (indicating a higher Black population) right around Louisiana’s major cities. The problem with using this map to identify predominantly Black communities is this: when we compare the total population of a certain demographic across regions, we don’t know if a larger population is a result of that demographic being more represented in that region, alternatively if it is a result of the total population being higher in that region. To be able to compare how Black communities are represented across Louisiana, we need to standardize our data to account for the total population. In other words, we need to calculate the percentage of the total population that identifies as Black in each region.\n\nUsing a data wrangling verb, add a column to each census data frame that calculates the percentage of the population in each census county/tract that identifies as Black. To do so, you will need to divide the Black population stored in value by the total population summary_value and multiply by 100.\n\n\ncensus_counties &lt;- #Fill code here\n\ncensus_tracts &lt;- #Fill code here\n\n\n\n\nCopy the palette and map that I created above, which colored counties by the total Black population. Adjust the code to color the counties by the variable you created to record the percent Black population.\n\n\npal_bin_census_county_perc &lt;- #Create county palette here\n\n#Create county map here"
  },
  {
    "objectID": "labs/lab10.html#aggregation",
    "href": "labs/lab10.html#aggregation",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Aggregation",
    "text": "Aggregation\nExamining our map of Louisiana counties, we can see that counties along the Mississippi River from Baton Rouge to New Orleans do appear to have a higher percentage of Black residents. However, counties are quite large and can have some considerable racial disparities within them. This map is not giving us much insight into the racial make-up of communities specifically along the Mississippi River. In other words, our data here is aggregated at too broad a spatial scale to see the racial make-up of Cancer Alley. We are going to add another layer to map you created above that allows us to toggle between visualizing the racial make-up of counties and the racial make-up of census tracts, which are smaller units of geography.\n\nCopy the palette and map that you created above. Create a second palette to color polygons by the percent Black population in census tracts.\nPipe a second addPolygons() function call and a second addLegend() function call onto your map. Copy arguments from the previous function calls and adjust them to color the polygons in this second call by the percent Black population in census tracts.\nFinally, in change the weight of the county polygons to 2 so that we can differentiate the borders of our census counties from our census tracts.\n\n\n# Copy the palette you created above for census counties here\n\n# Create palette for census tracts here\n\n# Copy the map you created above here and adjust according to the prompt\n\n\n\n\n\n\n\n\n\nWe have two layers on our map now, and we want to be able to toggle between them. In other words, I want to be able to turn the county layer off and the tract layer on and vice versa. To do this, we need to assign each layer (which includes our polygons and the corresponding legend) to a group and then add layer controls to the map. We also need to indicate which layers should be turned on and which should be hidden by default when we load the map.\n\nCopy the map that you created above into the code chunk below. Add a group = argument to each addPolygons() and addLegend() call; set the group for your county layers as “County” and the group for your tract layers as “Tract”.\nThen pipe the addLayersControl() function onto your map. Within this function, set the baseGroup = argument as we did above to c(\"Minimal\",\"Satellite\") so that we can toggle between basemaps. Then set the overlayGroups = argument to c(\"County\", \"Tract\") so that we can toggle between overlay layers. Add options = layersControlOptions(collapsed = FALSE) so that the layer controls appear when we load the map. Add position = \"bottomleft\" so that the layer controls don’t overlap with the legends.\nFinally, pipe the function hideGroup(\"Tract\") onto the end of your map. This will hide the “Tract” group when we load the map.\n\n\n# Copy map here and adjust according to the prompt\n\n\n\n\n\n\n\n\n\n\n\nZoom in to the region along the Mississippi River between Baton Rouge and New Orleans. Then toggle the Tract layer on. Notice how there is a higher representation of Black residents residing along the Mississippi River compared to the rest of the counties. Below, explain how this is an example of the ecological fallacy."
  },
  {
    "objectID": "labs/review.html",
    "href": "labs/review.html",
    "title": "Lab 10: APIs",
    "section": "",
    "text": "In this lab, we will write queries to access subsets of a very large dataset on the NYC Open Data Portal. We will practice all of the standards we have learned in the course so far in visualizing and wrangling the resulting data.\n\n\n\nWrite API Queries\nRecognize different HTTP Response Codes"
  },
  {
    "objectID": "labs/review.html#introduction",
    "href": "labs/review.html#introduction",
    "title": "Lab 10: APIs",
    "section": "",
    "text": "In this lab, we will write queries to access subsets of a very large dataset on the NYC Open Data Portal. We will practice all of the standards we have learned in the course so far in visualizing and wrangling the resulting data.\n\n\n\nWrite API Queries\nRecognize different HTTP Response Codes"
  },
  {
    "objectID": "labs/review.html#review-of-key-terms",
    "href": "labs/review.html#review-of-key-terms",
    "title": "Lab 10: APIs",
    "section": "Review of Key Terms",
    "text": "Review of Key Terms\n\n\n\n\n\n\nTip\n\n\n\nYou may wish to reference this reference guide when completing this lab.\n\n\n\nApplication Programming Interface (API)\n\nan mechanism that allows programmers or other systems (or users) to access resources from or post data to an online data service\n\nHTTP\n\na Web protocol for transmitting documents (e.g. Web pages, images, datasets, etc.) between web servers and browsers\n\nEndpoint\n\nAn exposed portion of an online data service that clients can assess via API queries"
  },
  {
    "objectID": "labs/review.html#nyc-311-service-requests",
    "href": "labs/review.html#nyc-311-service-requests",
    "title": "Lab 10: APIs",
    "section": "NYC 311 Service Requests",
    "text": "NYC 311 Service Requests\n311 is a dialing code set aside in many cities across the US to field calls about requests for municipal service – fixing potholes, reporting noise complaints, reporting parking infractions, etc. NYC has one of the most comprehensive 311 programs in the country, fielding calls to be routed to one of the city’s 77 agencies. Unlike in many other cities, New Yorkers can call 311 to report sexual harassment in taxi cabs or to report tenant rights issues (like a landlord failing to address a lack of heat or hot water). In October 2011, data documenting each request for service New Yorkers had made to 311 began to be published daily to the city’s public-facing open data portal. Since then, the dataset has become one of the largest and most complex archived and maintained on the portal, which includes datasets covering topics from restaurant health inspections, to crime statistics, to construction permits. Each row in the 311 dataset details one anonymized and geocoded request - the type of request that was made, when it was made, the location where the incident occurred, and how the relevant NYC agency responded. As of November 2022, the dataset has over 31 million rows, representing service requests made to 311 since 2010.\nWhen the 311 program was introduced, it was celebrated for its bottom-up approach to producing empirical evidence about problems facing New Yorkers; the program buttressed a persuasive techno-liberal imaginary, suggesting that fair and unbiased representation of quality of life concerns would emanate through the crowd-sourced data. When wrangled and visualized, the data can produce persuasive narratives about the state of equity and quality of life in communities across NYC, and thus a number of communities now leverage the data to legitimate claims. Community boards regularly cite 311 statistics about noise, infrastructure, and rodent issues when preparing district budget requests. Activists present the data to city legislators when advocating for or against new housing and transportation laws. Cyclists have produced apps that track complaints about the blocked bicycle lanes in New York. Journalists regularly reference the dataset to report which communities have received the most noise complaints, have had the most restaurants not following Covid-19 orders, or have had the most rat sightings. In 2018, eight pieces of legislation introduced by City Council required data about various issues – from rodent complaints, to noise complaints, to complaints about sexual harassment in taxis – be reported to the city agencies responsible for addressing those issues.\nYet, while representatives acknowledge 311 data to be a useful form of evidence for quality of life issues, they can be hesitant to rely on 311 statistics alone to measure urban problems. Communities that regularly leverage 311 open data warn that the data does not “represent” the saturation of problems in the city but that they represent “where people are complaining in the city.”\nMapping the data shows that, for almost all reported issues, 311 complaints are disproportionately high in gentrifying communities. The data under-represents problems in areas where individuals do not know to report, do not have the capacity to report, or fear harassment (from their landlords or bosses) for reporting. Complaints about NYC Housing Authority (NYCHA) are entirely excluded from the data. The data also over-represents problems when weaponized against minority communities and small businesses to consistently report minor and sometimes false noise issues or legal infractions. While this highlights how the dataset is being used to propagate inequity across the city, activists have also recognized that they can leverage 311 in liberatory ways. I’ve interviewed tenant rights groups that will run 311 calling campaigns in housing complexes, which can significantly increase the number of complaints made at a particular address - not necessarily because it is experiencing more issues than the building next to it, but because there has been advocacy around reporting the issues at this location."
  },
  {
    "objectID": "labs/review.html#setting-up-your-environment",
    "href": "labs/review.html#setting-up-your-environment",
    "title": "Lab 10: APIs",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nInstall the httr package.\nRun the code below to load the packages for today’s lab\n\n\nlibrary(httr)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)"
  },
  {
    "objectID": "labs/review.html#how-do-apis-work",
    "href": "labs/review.html#how-do-apis-work",
    "title": "Lab 10: APIs",
    "section": "How do APIs work?",
    "text": "How do APIs work?\nAPIs enable us to access data from an online service via an API query. Here are the steps that take place behind the scenes when we issue a Web-based API query:\n\nWe provide a line of code that indicates the location of a database online (or the data’s endpoint) and what subsets and aggregates of data want. This line of code is called an API query. API queries look very much like URLs that you enter into a Web browser to access a Web page.\nWe issue that request by entering the API query into a Web browser, or by referencing it via an import function in R.\nA request to GET the data resource is issued via the Hypertext Transfer Protocol (HTTP) - a protocol that manages the transfer of data between clients and servers on the Web. HTTP sends a request to the location indicated by the endpoint to retrieve the data.\nA response code gets issued from the server to the client. There are many codes associated with different issues, but here are the most common:\n\n\n200 indicates success (or that the data can be accessed).\n\n\nexample &lt;- GET(\"http://smith.edu/\")\nexample[[\"status_code\"]]\n\n[1] 200\n\n\n\n403 indicates that the client does not have permission to access the data.\n404 indicates that the resource could not be found at the endpoint the client specified. For example, below the directory this-url-does-not-exist does not exist at http://smith.edu/.\n\n\nexample &lt;- GET(\"http://smith.edu/this-url-does-not-exist\")\nexample[[\"status_code\"]]\n\n[1] 404\n\n\nIf successful, the server also sends the requested resource to the client.\nThis all happens very quickly, and most of it we don’t see!"
  },
  {
    "objectID": "labs/review.html#writing-api-queries",
    "href": "labs/review.html#writing-api-queries",
    "title": "Lab 10: APIs",
    "section": "Writing API Queries",
    "text": "Writing API Queries\nAn API query can be parsed into component parts:\n\nAn endpoint to the data resource\nThe format we want the data returned in (e.g. CSV, JSON, XML)\nParameters indicating what subsets/aggregates of the data that we want\nA reference to an API key that grants us access to the data\n\nDifferent APIs have different syntax rules for formatting API queries, so it is important to reference the API documentation to learn how these component parts come together.\nToday we are going to use the API that is made available from Socarata. Socrata is a Web hosting platform that is very commonly used by state and municipal governments, along with federal agencies, to host open government datasets. For example, the City of New York, the City of Chicago, the US Center for Disease Control, and the state of Texas all use the Socrata platform to host datasets they wish to make available to the public. Because Socrata is managing the data, Socrata also defines the API to access the data. A Socrata API query will follow this basic format:\n&lt;endpoint&gt;.&lt;format&gt;?&lt;optional-parameters&gt;\n\nNote that most calls to Socrata do not require an API key.\n\n\n\n\n\n\n\nTip\n\n\n\nNYC 311 is updated very often, and APIs allow us to access the most recent data available at a service. This means that your results for this lab will probably look different from mine. My results are based on running these codes on the evening of November 29, 2022.\n\n\n\nQuestion\n\nWrite an API query to access data at the endpoint https://data.cityofnewyork.us/resource/erm2-nwe9 in a CSV format. Let’s not worry about adding parameters yet. The resulting data frame should look something like my data frame below.\n\n\n# Code below!\n\n#nyc_311 &lt;- read_csv(&lt;URL HERE!&gt;)\n#head(nyc_311)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n$limit\nNote that this will only return the first 1000 rows of data.\n…but we know from examining the “311 Service Requests from 2010 to Present” metadata that this dataset has over 30 million rows. By default, the API limits the amount of data that gets sent to us so as not to overwhelm our systems.\nWe can manually adjust how much data gets sent by setting the limit parameter to a specific number of rows we wish to have returned. To add a parameter to a Socrata query, first, we put a ? at the end of the query. This indicates that we’re about to list a series of parameters. We then reference parameter names we wish to set behind a $ (e.g. ...?$limit). Finally, we set values to those parameters by adding =&lt;some-value&gt; after the parameter name. (e.g. ...?$limit=10).\n\nQuestion\n\nLet’s say I wanted to access data at the endpoint https://data.cityofnewyork.us/resource/erm2-nwe9 in a CSV format. Let’s not worry about adding parameters yet. Adjust your call above to limit the results to 20 rows of 311 data. The resulting data frame should look something like my data frame below.\n\n\n# Code below!\n\n#nyc_311 &lt;- read_csv(&lt;URL HERE!&gt;)\n#head(nyc_311)\n\n\n\n\n  \n\n\n\n\n\nAdditional parameters are available to subset and aggregate our data in various ways. Socrata has it’s own query language called SoQL, which codifies these parameters. SoQL is modeled after SQL - a language for querying data stored in large relational databases. Many SoQL parameters have a direct translation to SQL parameters. …and many of the parameters also have a direct translation to our dplyr data wrangling verbs.\n\n\n\ndplyr\nSoQL\nSQL\n\n\n\n\nhead()\n$limit\nLIMIT\n\n\nselect()\n$select\nSELECT\n\n\nfilter()\n$where\nWHERE\n\n\ngroup_by()\n$group\nGROUP BY\n\n\narrange()\n$order\nORDER BY\n\n\n\n\n\n\n\n$select\nThis dataset has 41 columns, and many of the variables in this dataset will not be immediately relevant to a data science task at hand. By pulling all of the columns, we are requesting much more data (and space on our computers!) than we actually need. We can request that only specific columns be returned via the $select= parameter. We assign the names of the fields we wish to have returned to this parameter. We can find the names of the fields at our data documentation here.\n\n\n\n\n\n\nTip\n\n\n\nNote that we use %&gt;% or |&gt; to string together multiple data wrangling verbs in dplyr. When writing SoQL, we use & to string together multiple parameters (e.g. ...?$limit=&lt;value&gt;&$select=&lt;value&gt;.\n\n\n\nQuestion\n\nCreate an API call to return the unique keys and complaint types for the latest 30 entries in NYC’s 311 open dataset. In other words, translate the following dplyr call into a SoSQL query:\nnyc_311 %&gt;% \n  select(unique_key, complaint_type) %&gt;% \n  head(30)\nPlot the counts of complaint types as a barplot. It should look something like my plot below.\n\n\n# Code below!\n# nyc_311_unique_keys &lt;- read_csv(&lt;URL HERE!&gt;)\n# Plot here!\n\n\n\n\n\n\n\n\n\n\n\n\n\n$where\nIt’s most likely that I don’t want just 30 random rows of data, but instead that I want specific subsets of the data. We can use the $where= parameter to filter the data to relevant rows. Just like with filter() in dplyr, we can return cases where the values in a row are equal to (=), not equal to (!=), greater than (&gt;), or less than (&lt;) a value we supply (e.g. $where=&lt;field&gt;=&lt;value&gt; or $where=&lt;field&gt;&gt;&lt;value&gt;)\n\nPercent Encoding\nWhen determining the value that we are going to supply to the $where parameter, it’s important to keep in mind that not all characters are safe to include in URLs. Some of our keyboard characters serve special purposes in URLs. For instance, the / is used to indicate a sub-directory, and (as we’ve just learned) ? are used to indicate that we are about to string a series of parameters to the end of the URL. Because these characters serve special purposes in a URL, we can’t use them when writing out our value in the ?where parameter… So what do we do when we want to supply a value that includes one of these special characters?\nThis is where percent encoding comes in. Whenever we would normally use the special characters listed below on the left in a URL, we would replace it with the percent encoding listed on the right.\n\nspace : %20\n!: %21\n\": %22\n%: %25\n': %27\n-: %2D\n\nSo for instance, the following: 'Noise - Commercial' includes five reserved characters:\n\nan apostrophe\na space\na dash\nanother space\nanother apostrophe.\n\nI would encode that value as follows: %27Noise%20%2D%20Commercial%27.\n\nCreate an API call to return the unique keys, created_dates and complaint_types for the rows where the agency is listed as ‘FDNY’. In other words, translate the following dplyr call into a SoSQL query:\nnyc_311 %&gt;%\n  select(unique_key, created_date, complaint_type) %&gt;%\n  filter(agency == 'FDNY')\n  \nPlot the data as a point point to match my plot below.\n\n\n# Code below!\n# nyc_311_fdny &lt;- read_csv(&lt;URL HERE!&gt;)\n# Plot here!\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also string together multiple filter conditions with operators like AND, OR, NOT, IS NULL, etc. $where=&lt;field&gt;=&lt;value&gt; AND &lt;field&gt;=&lt;value&gt;. Note that there are spaces between the first and second condition above, and spaces are reserved characters. We need to replace those spaces with the percent encodings for spaces: $where=&lt;field&gt;=&lt;value&gt;%20AND%20&lt;field&gt;=&lt;value&gt;.\n\n\nQuestion\n\nCreate an API call to return the unique keys, created dates, incident addresses, and BBLs (this is a unique ID for the tax lot of a building in NYC) for rows with complaint type “Food Poisoning” in Manhattan’s community district 5 (which hosts Times Square and other major NYC tourist attractions). Limit the results to 3000 entries in NYC’s 311 open dataset. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %&gt;% \n      select(unique_key, created_date, incident_address, bbl) %&gt;% \n      filter(complaint_type == \"Food Poisoning\" & \n              community_board == \"05 MANHATTAN\") %&gt;% \n      head(3000)\n      \nWrangle the resulting data frame to determine the 5 addresses with the most food poisoning complaints in this community district. It will look something like my data frame below.\n\n\n# Code below!\n# nyc_311_food_poisoning &lt;- read_csv(&lt;URL HERE!&gt;)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n$order\nThe $order parameter will re-arrange the rows, according to the values in a column that we supply, such that the smallest value will appear at the top and the largest value will appear at the bottom (e.g. $order=&lt;field&gt;). We can tack DESC to end of this parameter in order to reverse the sorting order (e.g. $order=&lt;field&gt;%20DESC).\n\nQuestion\n\nOrder the previous result in descending order by created date, and the subset to the 500 most recent complaints. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %&gt;% \n      select(unique_key, created_date, incident_address, bbl) %&gt;% \n      filter(complaint_type == \"Food Poisoning\" & \n              community_board == \"05 MANHATTAN\") %&gt;% \n      arrange(desc(created_date)) %&gt;%\n      head(500)\nWrangle the resulting data frame to determine the count of complaints at each BBL, and order the results by the count. Store the data frame in nyc_311_food_poisoning_ordered_counts.\nI’ve written an API query to return most recent restaurant grades for restaurants in Manhattan’s community district 5. Join the two data frames by the bbl.\n\nNote that there can be more than one restaurant on a particular tax lot. This means that, if there were x number of Food Poisoning complaints at a particular lot, those complaints may have been associated with any number of restaurants at that lot. BBL is the most specific field we have available for identifying the location associated with a complaint in 311. This means that we are most likely going to have a one-to-many join, and we need to be careful when making assumptions about associations across the datasets.\n\nYour resulting data frame should look something like mine below.\n\n\n# Code below!\n# nyc_311_food_poisoning_ordered &lt;- read_csv(&lt;URL HERE!&gt;)\n# nyc_311_food_poisoning_ordered_counts &lt;- &lt;WRANGLE HERE!&gt;\n\nrestaurant_inspections &lt;- read_csv(\"https://data.cityofnewyork.us/resource/43nn-pn8j.csv?$where=community_board=%27105%27%20AND%20grade%20IS%20NOT%20NULL&$select=camis,dba,bbl,grade,max(grade_date)&$group=camis,dba,bbl,grade&$limit=3000\")\n\n# Join datasets here!\n\n\n\n\n  \n\n\n\n\n\nDates can be tricky to work within APIs because we need to know exactly how they are formatted in order to do meaningful things with them. Typically we want to look up how dates are formatted in the API documentation. In this dataset, the dates are formatted as follows: yyyy-mm-ddThh:mm:ss.0000 (e.g. 2022-11-01T00:00:00.000).\nWhen we know how dates are formatted, there’s often many ways we can subset the data to certain dates. For instance, I can access all of the rows where a date is between('date 1' AND 'date 2'). I can extract the year as an integer from a column using date_extract_y(&lt;date-field&gt;) and then filter to the rows in that year. A full list of data transformations that can be applied is Socrata is available here.\n\n\n\nQuestion\n\nCreate an API call to return the unique keys, incident addresses, and created dates for rows with complaint type “Construction Lead Dust” that were created in October 2022. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %&gt;% \n      select(unique_key, created_date, incident_address)\n      filter(complaint_type == 'Construction Lead Dust' & \n              created_date &gt; '2022-10-01' &     \n              created_date &lt; '2022-10-30')\nOnce you get this working, write a function called get_construction_lead_dust_complaints. This function will take a borough as an argument, and should construct a string URL that will further filter the data to a given borough (hint: you will need the function paste0 to construct this string). In your function, read the data frame using read_csv and return the data frame.\nI’ve created a vector of three NYC boroughs for you. Iterate your function over this vector, returning the results as a data frame with rows bound. Your resulting data frame should look something like mine below.\n\n\n# Code below!\n# nyc_311_construction_lead_dust &lt;- read_csv(&lt;URL HERE!&gt;) \n# get_construction_lead_dust_complaints &lt;- &lt;FUNCTION HERE!&gt;\n\nboroughs &lt;- c(\"MANHATTAN\", \"BROOKLYN\", \"BRONX\")\n\n# Write code to iterate function here!\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n$group\nAggregating in SoQL calls looks and feels a bit different than it does in dplyr. The $group= parameter indicates that we wish to return a result grouped by a particular field. However, instead of using a verb like summarize to indicate that we want to perform a calculation in each group, we are going to specify what aggregated columns we want returned to us via the $select parameter.\nSo let’s say I want to return counts of the number of rows assigned to each agency in this dataset. In other words, I want a dataframe with a column listing the agency and a column listing the counts of rows with that agency listed.\nFirst, I would set $select=agency,count(*). Here * refers to everything (i.e. all rows), and count() is a function indicating that we should count all rows. After indicating what columns I want back, then I would set $group=agency.\n\nnyc_311_agency &lt;-\n  read_csv(\"https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$select=agency,count(*)&$group=agency\")\n\nhead(nyc_311_agency)\n\n\n  \n\n\n\n\nQuestion\n\nCreate an API call to return the counts per descriptor and borough for the complaint type “Consumer Complaint.” In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %&gt;% \n      filter(complaint_type == \"Consumer Complaint\") %&gt;% \n      group_by(descriptor,borough) %&gt;% \n      summarize(count = n())\n\nPivot the data so that there are separate columns indicating counts for each borough in the dataset. Your data frame will look something like mine below.\n\n\n# Code below!\n# nyc_311_consumer_complaint &lt;- read_csv(&lt;URL HERE!&gt;)\n# Pivot here!\n\n\n\n\n  \n\n\n\n\n\ncount(*) is function SoQL function. A full list of similar summary functions in SoQL is available here. Another useful SoQL function is distinct, which can be used to return the distinct values in a column. When we use distinct in conjunction with count, we can count the distinct values in a column (e.g. count(distinct%20&lt;field&gt;)).\n\n\n\nQuestion\n\nCreate an API call to return the counts of complaints with the descriptor “Gender Pricing” in each borough, along with the count of distinct incident addresses for these complaints in each borough. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %&gt;% \n      filter(complaint_type == \"Gender Pricing\") %&gt;% \n      group_by(borough) %&gt;% \n      summarize(count = n(),\n                count_distinct_incident_address = length(unique(incident_address)) )\n\nPivot and plot the data to match my plot below.\n\n\n# Code below!\n# nyc_311_gender_pricing &lt;- read_csv(&lt;URL HERE!&gt;)\n# Plot here!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$having\nOnce we’ve aggregated data in a SoQL query, we no longer use $where= to filter it. Instead, we use $having= to filter aggregate data. For example, in the above call, I could tack on ...&having=count&gt;10 to return the rows from my aggregated data where the count was greater than 10.\n\nQuestion\n\nCreate an API call to return the latitude, longitude, and counts per BBL for the complaint type “HEAT/HOT WATER.” Filter out rows where the BBL is ‘0000000000’, and sort the counts in descending order. Filter the results to the rows where the count of heat and hot water complaints is greater than 1000. In other words, translate the following dplyr call into a SoSQL query:\n    nyc_311 %&gt;%\n      filter(complaint_type == \"HEAT/HOT WATER\" & \n              bbl != \"0000000000\") %&gt;% \n      group_by(bbl, latitude, longitude) %&gt;% \n      summarize(count = n()) %&gt;% \n      arrange(desc(count)) %&gt;%\n      ungroup() %&gt;%\n      filter(count &gt; 1000)\nConvert the result into a geom object, create a palette using the counts variable, and map the points via leaflet to match my map below.\n\n\n# Code below!\n# nyc_311_heat_counts &lt;- read_csv(&lt;URL HERE!&gt;) |&gt; st_as_sf()\n# Palette here!\n# Map here!\n\n\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nAs noted in the introductory paragraphs, 311 can both over-count and under-count certain quality of life concerns in New York City. It can be a valuable tool for activists trying to draw attention to certain issues, and it can also be weaponized against certain communities when complaints are issued against an establishment in a discriminatory way. Despite this, the data is often used by city decision-makers to determine where to prioritize resources and services. What is the value of 311 data? When and wow should it be used?"
  },
  {
    "objectID": "labs/lab10.html#layering-toxics-data",
    "href": "labs/lab10.html#layering-toxics-data",
    "title": "Lab 10: Polygon Mapping in Leaflet",
    "section": "Layering Toxics Data",
    "text": "Layering Toxics Data\nAs a last step, we are going to add a third layer to our map, displaying the toxic facilities data that we mapped last week. Run the code below to load that data, transform the CRS, create a palette to color the points by total releases, and map the data.\n\ntri_la_2023 &lt;- read_csv(\"https://raw.githubusercontent.com/SDS-192-Intro/public-website-fall-25/refs/heads/main/data/2023_la.csv\", name_repair = make.names) |&gt;\n  filter(X50..UNIT.OF.MEASURE == \"Pounds\") |&gt;\n  arrange(desc(X107..TOTAL.RELEASES)) |&gt;\n  mutate(chemical_total_text = paste(X37..CHEMICAL, X107..TOTAL.RELEASES, sep = \": \")) |&gt;\n  group_by(X2..TRIFD) |&gt;\n  summarize(X4..FACILITY.NAME = first(X4..FACILITY.NAME),\n            X12..LATITUDE = first(X12..LATITUDE),\n            X13..LONGITUDE = first(X13..LONGITUDE),\n            X23..INDUSTRY.SECTOR = first(X23..INDUSTRY.SECTOR),\n            X107..TOTAL.RELEASES = sum(X107..TOTAL.RELEASES),\n            X..CHEMICAL.TEXT = paste(chemical_total_text, collapse = \" &lt;br&gt; \"))\n\ntri_la_2023 &lt;- tri_la_2023 %&gt;%\n  st_as_sf(coords = c(\"X13..LONGITUDE\", \"X12..LATITUDE\"), crs = 4269) %&gt;%\n  st_transform(4326)\n\npal_bin &lt;- colorBin(palette=\"YlOrRd\", \n                    domain = tri_la_2023$X107..TOTAL.RELEASES,\n                    bins = 6)\n\nla_map %&gt;%\n  addCircleMarkers(data = tri_la_2023, \n                   weight = 0.5,\n                   radius = 3,\n                   color = \"black\",\n                   fillColor = ~pal_bin(X107..TOTAL.RELEASES), \n                   fillOpacity = 0.8,\n                   popup = ~paste(\"&lt;b&gt;\",\n                                  X4..FACILITY.NAME,\n                                  \"&lt;/b&gt;&lt;br&gt;&lt;i&gt;\",\n                                  X23..INDUSTRY.SECTOR,\n                                  \"&lt;/i&gt;&lt;br&gt;&lt;br&gt;\",\n                                  X..CHEMICAL.TEXT),\n                   popupOptions = popupOptions(maxHeight = 200, \n                                               closeOnClick = TRUE)) %&gt;%\n  addLegend(data = tri_la_2023,\n            title = \"TRI Releases in Pounds\", \n            pal = pal_bin, \n            values = ~X107..TOTAL.RELEASES)\n\n\n\n\n\n\nCopy the last map that you created above visualizing the percent Black population in LA counties and tracts. Then copy my code above to addCircleMarkers() for TRI facilities and addLegend() for TRI facilities. Pipe these functions onto your map after your addPolygons() function calls but before the addLayersControl() function call.\nSet the group = argument for the TRI circle markers and the TRI legend to “TRI”. Then add “TRI” as an additional group in the overlayGroups = argument in your addLayersControl() function.\n\n\n# Copy map functions and adjust here\n\n\n\n\n\n\n\n\n\nAs you play with this map, you might notice one problem. As soon as you turn the tract layer on, it covers the TRI layer, and you can no longer click on points to learn more about each facility. We need to communicate to leaflet how we want our layers ordered. To do that we, are going to set a zIndex, which indicates the order by which layers should be stacked on top of each other. A higher zIndex indicates that the layer will be stacked higher. In leaflet zIndex values range from 400 to 500.\nTo set the zIndex for each layer, we need to add separate “panes” to our map - one for “circles” and one for “polygons”. We want the facility markers to appear above the polygons (so they remain clickable), so we will want our circle pane to be stacked higher than our polygons pane.\n\nCopy the last map that you created above. At the beginning of your code right after calling la_map, pipe a addMapPane() function twice. In the first function call, set name = argument to “circles” and the zIndex = argument to 420. In the second function call, set name = argument to “polygons” and the zIndex = argument to 410.\nAdd the argument options = pathOptions(pane = \"polygons\") to both of your addPolygons() function calls, and add the argument options = pathOptions(pane = \"circles\") to your addCircleMarkers() function call.\nAdjust your code to hide the “County” map by default.\n\n\n# Copy map here and adjust according to the prompt\n\n\n\n\n\n\n\n\nZoom in to the region along the Mississippi River between Baton Rouge and New Orleans. Notice how there are at least a few census tracts along the Mississippi River where high-polluting industrial facilities are located, but there is a lower percentage Black population. Explain how the modifiable aerial unit problem may be at play here and why we should consider it in relation to pollution data.\n\n\n\n\n\n\n\nEthical Considerations\n\n\n\nFor several years, the U.S. Environmental Protection Agency published a map called EJScreen. The map included layers for socioeconomic indicators like race, income, and age, along with layers for environmental indicators such as TRI releases, proximity to superfund sites, proximity to major traffic sources, and more. Users could also map layers for health disparities, such as cancer rates and asthma rates. It helped document how certain communities faced disproportionate environmental burdens across the country. On February 5, 2025, EJScreen was removed from the EPA’s website, at the same time that the EPA was overhauling several diversity, equity, and inclusion (DEI) initiatives under the Trump administration. Luckily, the advocacy organization Public Environmental Data Partners (PEDP) archived a reconstructed version of the map and made it available here. This one example of several government data sources that have been suppressed for political reasons. Why do you think these data removals are happening? What are some of the consequences of removing or defunding public government data like this? What role should non-governmental organizations play in securing access to this data? Are there ever good political reasons to suppress public data, and who should be involved those decisions? Share your ideas on our sds-192-discussions Slack channel."
  }
]